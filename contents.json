{
  "paths": [
    {
      "type": "file",
      "value": "index.md"
    },
    {
      "type": "dir",
      "name": "Scrapy框架高级与动态页面的爬取",
      "children": [
        {
          "type": "file",
          "value": "Scrapy框架高级与动态页面的爬取/Scrapy 框架 - CrawlSpider.md"
        },
        {
          "type": "file",
          "value": "Scrapy框架高级与动态页面的爬取/Scrapy 框架 - 下载中间件Middleware.md"
        },
        {
          "type": "file",
          "value": "Scrapy框架高级与动态页面的爬取/Scrapy 框架 - 图片管道使用.md"
        },
        {
          "type": "file",
          "value": "Scrapy框架高级与动态页面的爬取/Scrapy 框架 - 模拟登录——Request、Response.md"
        },
        {
          "type": "file",
          "value": "Scrapy框架高级与动态页面的爬取/Scrapy 框架 - 爬取JS生成的动态页面.md"
        },
        {
          "type": "file",
          "value": "Scrapy框架高级与动态页面的爬取/Splash 的使用.md"
        }
      ]
    },
    {
      "type": "dir",
      "name": "数据提取与验证码的识别（上）",
      "children": [
        {
          "type": "file",
          "value": "数据提取与验证码的识别（上）/Beautiful Soup.md"
        },
        {
          "type": "file",
          "value": "数据提取与验证码的识别（上）/JsonPath.md"
        },
        {
          "type": "file",
          "value": "数据提取与验证码的识别（上）/PyQuery.md"
        },
        {
          "type": "file",
          "value": "数据提取与验证码的识别（上）/Re.md"
        },
        {
          "type": "file",
          "value": "数据提取与验证码的识别（上）/XPath.md"
        }
      ]
    },
    {
      "type": "dir",
      "name": "数据提取与验证码的识别（下）",
      "children": [
        {
          "type": "file",
          "value": "数据提取与验证码的识别（下）/Python下Tesseract Ocr引擎及安装介绍.md"
        },
        {
          "type": "file",
          "value": "数据提取与验证码的识别（下）/Selenium 处理滚动条.md"
        },
        {
          "type": "file",
          "value": "数据提取与验证码的识别（下）/Selenium与PhantomJS.md"
        },
        {
          "type": "file",
          "value": "数据提取与验证码的识别（下）/爬虫之多线程.md"
        }
      ]
    },
    {
      "type": "dir",
      "name": "类级别的写法与Scrapy框架",
      "children": [
        {
          "type": "file",
          "value": "类级别的写法与Scrapy框架/Scrapy 数据的保存.md"
        },
        {
          "type": "file",
          "value": "类级别的写法与Scrapy框架/Scrapy 数据的提取.md"
        },
        {
          "type": "file",
          "value": "类级别的写法与Scrapy框架/Scrapy 框架介绍与安装.md"
        },
        {
          "type": "file",
          "value": "类级别的写法与Scrapy框架/Scrapy 框架使用.md"
        }
      ]
    }
  ],
  "contents": [
    {
      "path": "index.md",
      "url": "index.html",
      "content": "# 2021PythonWebCrawler\n\nLecture notes for 2021 Python Web Crawler\n\nOrash is the lecturer and Misaka is the vice lecturer.\n\nLecture video: [【小小图灵社】2021Python冬季网络爬虫课程](https://www.bilibili.com/video/BV1iv4y1o7t5)\n\nSource code available on GitHub: [littleturings/2021PythonWebCrawler](https://github.com/littleturings/2021PythonWebCrawler)\n",
      "html": "<h1 id=\"2021pythonwebcrawler\">2021PythonWebCrawler <a class=\"heading-anchor-permalink\" href=\"#2021pythonwebcrawler\">#</a></h1>\n<p>Lecture notes for 2021 Python Web Crawler</p>\n<p>Orash is the lecturer and Misaka is the vice lecturer.</p>\n<p>Lecture video: <a href=\"https://www.bilibili.com/video/BV1iv4y1o7t5\">【小小图灵社】2021Python冬季网络爬虫课程</a></p>\n<p>Source code available on GitHub: <a href=\"https://github.com/littleturings/2021PythonWebCrawler\">littleturings/2021PythonWebCrawler</a></p>\n",
      "id": 0
    },
    {
      "path": "Scrapy框架高级与动态页面的爬取/Scrapy 框架 - CrawlSpider.md",
      "url": "Scrapy框架高级与动态页面的爬取/Scrapy 框架 - CrawlSpider.html",
      "content": "### 1. CrawlSpiders\n\n#### 原理图\n\n```text\nsequenceDiagram\nstart_urls ->>调度器: 初始化url\n调度器->>下载器: request\n下载器->>rules: response\nrules->>数据提取: response\nrules->>调度器: 新的url\n```\n\n通过下面的命令可以快速创建 `CrawlSpiders` 模板 的代码\n\n```sh\nscrapy genspider -t crawl 文件名 (allowed_url)\n```\n\n首先再说下 Spider，它是所有爬虫的基类，而 `CrawSpiders` 就是 `Spider` 的派生类。对于设计原则是只爬取 `start_url` 列表中的网页，而从爬取的网页中获取 link 并继续爬取的工作 `CrawlSpider` 类更适合\n\n### 2. Rule 对象\n\n`Rule` 类与 `CrawlSpider` 类都位于 `scrapy.contrib.spiders` 模块中\n\n```python\nclass scrapy.contrib.spiders.Rule (\nlink_extractor, callback=None,cb_kwargs=None,follow=None,process_links=None,process_request=None )\n```\n\n参数含义：\n\n- `link_extractor` 为 `LinkExtractor` ，用于定义需要提取的链接\n- `callback`：当 `link_extractor` 获取到链接时参数所指定的值作为回调函数\n\n  - `callback` 参数使用注意：\n    当编写爬虫规则时，请避免使用 `parse` 作为回调函数。于 `CrawlSpider` 使用 `parse` 方法来实现其逻辑，如果您覆盖了 `parse` 方法，`CrawlSpider` 将会运行失败\n\n- `follow`：指定了根据该规则从 `response` 提取的链接是否需要跟进。当 `callback` 为 `None`,默认值为 `True`\n- `process_links`：主要用来过滤由 `link_extractor` 获取到的链接\n- `process_request`：主要用来过滤在 `rule` 中提取到的 `request`\n\n### 3.LinkExtractors\n\n#### 3.1 概念\n\n> 顾名思义，链接提取器\n\n#### 3.2 作用\n\n`Response` 对象中获取链接，并且该链接会被接下来爬取\n每个 `LinkExtractor` 有唯一的公共方法是 `extract_links()` ，它接收一个 `Response` 对象，并返回一个 `scrapy.link.Link` 对象\n\n#### 3.3 使用\n\n```python\nclass scrapy.linkextractors.LinkExtractor(\n    allow = (),\n    deny = (),\n    allow_domains = (),\n    deny_domains = (),\n    deny_extensions = None,\n    restrict_xpaths = (),\n    tags = ('a','area'),\n    attrs = ('href'),\n    canonicalize = True,\n    unique = True,\n    process_value = None\n)\n```\n\n| 主要参数        | 说明                                                                 |\n| --------------- | -------------------------------------------------------------------- |\n| allow           | 满足括号中“正则表达式”的值会被提取，如果为空，则全部匹配。           |\n| deny            | 与这个正则表达式(或正则表达式列表)不匹配的 URL 一定不提取。          |\n| allow_domains   | 会被提取的链接的 domains。                                           |\n| deny_domains    | 一定不会被提取链接的 domains。                                       |\n| restrict_xpaths | 使用 xpath 表达式，和 allow 共同作用过滤链接(只选到节点，不选到属性) |\n\n##### 3.3.1 查看效果（shell 中验证)\n\n首先运行\n\n```sh\nscrapy shell http://www.fhxiaoshuo.com/read/33/33539/17829387.shtml\n```\n\n继续 import 相关模块\n\n```python\nfrom scrapy.linkextractors import LinkExtractor\n```\n\n提取当前网页中获得的链接\n\n```python\nlink = LinkExtractor(restrict_xpaths=(r'//div[@class=\"bottem\"]/a[4]')\n```\n\n调用 LinkExtractor 实例的 extract_links()方法查询匹配结果\n\n```python\nlink.extract_links(response)\n```\n\n##### 3.3.2 查看效果 CrawlSpider 版本\n\n```python\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom xiaoshuo.items import XiaoshuoItem\n\n\nclass XiaoshuoSpiderSpider(CrawlSpider):\n    name = 'xiaoshuo_spider'\n    allowed_domains = ['fhxiaoshuo.com']\n    start_urls = ['http://www.fhxiaoshuo.com/read/33/33539/17829387.shtml']\n\n    rules = [\n        Rule(LinkExtractor(restrict_xpaths=(r'//div[@class=\"bottem\"]/a[4]')), callback='parse_item'),]\n\n    def parse_item(self, response):\n        info = response.xpath(\"//div[@id='TXT']/text()\").extract()\n        it = XiaoshuoItem()\n        it['info'] = info\n        yield it\n\n```\n\n注意\n\n```python\nrules = [\n    Rule(LinkExtractor(restrict_xpaths=(r'//div[@class=\"bottem\"]/a[4]')), callback='parse_item'),\n]\n```\n\n- `callback` 后面函数名用引号引起\n- 函数名不能是 `parse`\n- 格式问题\n",
      "html": "<h3 id=\"1.-crawlspiders\">1. CrawlSpiders <a class=\"heading-anchor-permalink\" href=\"#1.-crawlspiders\">#</a></h3>\n<h4 id=\"%E5%8E%9F%E7%90%86%E5%9B%BE\">原理图 <a class=\"heading-anchor-permalink\" href=\"#%E5%8E%9F%E7%90%86%E5%9B%BE\">#</a></h4>\n<pre><code class=\"language-text\">sequenceDiagram\nstart_urls -&gt;&gt;调度器: 初始化url\n调度器-&gt;&gt;下载器: request\n下载器-&gt;&gt;rules: response\nrules-&gt;&gt;数据提取: response\nrules-&gt;&gt;调度器: 新的url\n</code></pre>\n<p>通过下面的命令可以快速创建 <code>CrawlSpiders</code> 模板 的代码</p>\n<pre><code class=\"language-sh\">scrapy genspider -t crawl 文件名 (allowed_url)\n</code></pre>\n<p>首先再说下 Spider，它是所有爬虫的基类，而 <code>CrawSpiders</code> 就是 <code>Spider</code> 的派生类。对于设计原则是只爬取 <code>start_url</code> 列表中的网页，而从爬取的网页中获取 link 并继续爬取的工作 <code>CrawlSpider</code> 类更适合</p>\n<h3 id=\"2.-rule-%E5%AF%B9%E8%B1%A1\">2. Rule 对象 <a class=\"heading-anchor-permalink\" href=\"#2.-rule-%E5%AF%B9%E8%B1%A1\">#</a></h3>\n<p><code>Rule</code> 类与 <code>CrawlSpider</code> 类都位于 <code>scrapy.contrib.spiders</code> 模块中</p>\n<pre><code class=\"language-python\">class scrapy.contrib.spiders.Rule (\nlink_extractor, callback=None,cb_kwargs=None,follow=None,process_links=None,process_request=None )\n</code></pre>\n<p>参数含义：</p>\n<ul>\n<li>\n<p><code>link_extractor</code> 为 <code>LinkExtractor</code> ，用于定义需要提取的链接</p>\n</li>\n<li>\n<p><code>callback</code>：当 <code>link_extractor</code> 获取到链接时参数所指定的值作为回调函数</p>\n<ul>\n<li><code>callback</code> 参数使用注意：\n当编写爬虫规则时，请避免使用 <code>parse</code> 作为回调函数。于 <code>CrawlSpider</code> 使用 <code>parse</code> 方法来实现其逻辑，如果您覆盖了 <code>parse</code> 方法，<code>CrawlSpider</code> 将会运行失败</li>\n</ul>\n</li>\n<li>\n<p><code>follow</code>：指定了根据该规则从 <code>response</code> 提取的链接是否需要跟进。当 <code>callback</code> 为 <code>None</code>,默认值为 <code>True</code></p>\n</li>\n<li>\n<p><code>process_links</code>：主要用来过滤由 <code>link_extractor</code> 获取到的链接</p>\n</li>\n<li>\n<p><code>process_request</code>：主要用来过滤在 <code>rule</code> 中提取到的 <code>request</code></p>\n</li>\n</ul>\n<h3 id=\"3.linkextractors\">3.LinkExtractors <a class=\"heading-anchor-permalink\" href=\"#3.linkextractors\">#</a></h3>\n<h4 id=\"3.1-%E6%A6%82%E5%BF%B5\">3.1 概念 <a class=\"heading-anchor-permalink\" href=\"#3.1-%E6%A6%82%E5%BF%B5\">#</a></h4>\n<blockquote>\n<p>顾名思义，链接提取器</p>\n</blockquote>\n<h4 id=\"3.2-%E4%BD%9C%E7%94%A8\">3.2 作用 <a class=\"heading-anchor-permalink\" href=\"#3.2-%E4%BD%9C%E7%94%A8\">#</a></h4>\n<p><code>Response</code> 对象中获取链接，并且该链接会被接下来爬取\n每个 <code>LinkExtractor</code> 有唯一的公共方法是 <code>extract_links()</code> ，它接收一个 <code>Response</code> 对象，并返回一个 <code>scrapy.link.Link</code> 对象</p>\n<h4 id=\"3.3-%E4%BD%BF%E7%94%A8\">3.3 使用 <a class=\"heading-anchor-permalink\" href=\"#3.3-%E4%BD%BF%E7%94%A8\">#</a></h4>\n<pre><code class=\"language-python\">class scrapy.linkextractors.LinkExtractor(\n    allow = (),\n    deny = (),\n    allow_domains = (),\n    deny_domains = (),\n    deny_extensions = None,\n    restrict_xpaths = (),\n    tags = ('a','area'),\n    attrs = ('href'),\n    canonicalize = True,\n    unique = True,\n    process_value = None\n)\n</code></pre>\n<table>\n<thead>\n<tr>\n<th>主要参数</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>allow</td>\n<td>满足括号中“正则表达式”的值会被提取，如果为空，则全部匹配。</td>\n</tr>\n<tr>\n<td>deny</td>\n<td>与这个正则表达式(或正则表达式列表)不匹配的 URL 一定不提取。</td>\n</tr>\n<tr>\n<td>allow_domains</td>\n<td>会被提取的链接的 domains。</td>\n</tr>\n<tr>\n<td>deny_domains</td>\n<td>一定不会被提取链接的 domains。</td>\n</tr>\n<tr>\n<td>restrict_xpaths</td>\n<td>使用 xpath 表达式，和 allow 共同作用过滤链接(只选到节点，不选到属性)</td>\n</tr>\n</tbody>\n</table>\n<h5 id=\"3.3.1-%E6%9F%A5%E7%9C%8B%E6%95%88%E6%9E%9C%EF%BC%88shell-%E4%B8%AD%E9%AA%8C%E8%AF%81)\">3.3.1 查看效果（shell 中验证) <a class=\"heading-anchor-permalink\" href=\"#3.3.1-%E6%9F%A5%E7%9C%8B%E6%95%88%E6%9E%9C%EF%BC%88shell-%E4%B8%AD%E9%AA%8C%E8%AF%81)\">#</a></h5>\n<p>首先运行</p>\n<pre><code class=\"language-sh\">scrapy shell http://www.fhxiaoshuo.com/read/33/33539/17829387.shtml\n</code></pre>\n<p>继续 import 相关模块</p>\n<pre><code class=\"language-python\">from scrapy.linkextractors import LinkExtractor\n</code></pre>\n<p>提取当前网页中获得的链接</p>\n<pre><code class=\"language-python\">link = LinkExtractor(restrict_xpaths=(r'//div[@class=&quot;bottem&quot;]/a[4]')\n</code></pre>\n<p>调用 LinkExtractor 实例的 extract_links()方法查询匹配结果</p>\n<pre><code class=\"language-python\">link.extract_links(response)\n</code></pre>\n<h5 id=\"3.3.2-%E6%9F%A5%E7%9C%8B%E6%95%88%E6%9E%9C-crawlspider-%E7%89%88%E6%9C%AC\">3.3.2 查看效果 CrawlSpider 版本 <a class=\"heading-anchor-permalink\" href=\"#3.3.2-%E6%9F%A5%E7%9C%8B%E6%95%88%E6%9E%9C-crawlspider-%E7%89%88%E6%9C%AC\">#</a></h5>\n<pre><code class=\"language-python\">from scrapy.linkextractors import LinkExtractor\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom xiaoshuo.items import XiaoshuoItem\n\n\nclass XiaoshuoSpiderSpider(CrawlSpider):\n    name = 'xiaoshuo_spider'\n    allowed_domains = ['fhxiaoshuo.com']\n    start_urls = ['http://www.fhxiaoshuo.com/read/33/33539/17829387.shtml']\n\n    rules = [\n        Rule(LinkExtractor(restrict_xpaths=(r'//div[@class=&quot;bottem&quot;]/a[4]')), callback='parse_item'),]\n\n    def parse_item(self, response):\n        info = response.xpath(&quot;//div[@id='TXT']/text()&quot;).extract()\n        it = XiaoshuoItem()\n        it['info'] = info\n        yield it\n\n</code></pre>\n<p>注意</p>\n<pre><code class=\"language-python\">rules = [\n    Rule(LinkExtractor(restrict_xpaths=(r'//div[@class=&quot;bottem&quot;]/a[4]')), callback='parse_item'),\n]\n</code></pre>\n<ul>\n<li><code>callback</code> 后面函数名用引号引起</li>\n<li>函数名不能是 <code>parse</code></li>\n<li>格式问题</li>\n</ul>\n",
      "id": 1
    },
    {
      "path": "Scrapy框架高级与动态页面的爬取/Scrapy 框架 - 下载中间件Middleware.md",
      "url": "Scrapy框架高级与动态页面的爬取/Scrapy 框架 - 下载中间件Middleware.html",
      "content": "### 1. Spider 下载中间件(Middleware)\n\nSpider 中间件(Middleware) 下载器中间件是介入到 Scrapy 的 spider 处理机制的钩子框架，您可以添加代码来处理发送给 Spiders 的 `response` 及 spider 产生的 `item` 和 `request`\n\n### 2. 激活一个下载 DOWNLOADER_MIDDLEWARES\n\n要激活一个下载器中间件组件，将其添加到 `DOWNLOADER_MIDDLEWARES`设置中，该设置是一个字典，其键是中间件类路径，它们的值是中间件命令\n\n```python\nDOWNLOADER_MIDDLEWARES  =  {\n    'myproject.middlewares.CustomDownloaderMiddleware' ： 543 ，\n}\n```\n\n该`DOWNLOADER_MIDDLEWARES`设置与`DOWNLOADER_MIDDLEWARES_BASEScrapy`中定义的设置（并不意味着被覆盖）合并， 然后按顺序排序，以获得最终的已启用中间件的排序列表：第一个中间件是靠近引擎的第一个中间件，最后一个是靠近引擎的中间件到下载器。换句话说，`process_request()` 每个中间件的方法将以增加中间件的顺序（100,200,300，...）`process_response()`被调用，并且每个中间件的方法将以降序调用\n\n要决定分配给中间件的顺序，请参阅 `DOWNLOADER_MIDDLEWARES_BASE`设置并根据要插入中间件的位置选择一个值。顺序很重要，因为每个中间件都执行不同的操作，而您的中间件可能依赖于之前（或后续）正在使用的中间件\n\n如果要禁用内置中间件（`DOWNLOADER_MIDDLEWARES_BASE`默认情况下已定义和启用的中间件 ），则必须在项目`DOWNLOADER_MIDDLEWARES`设置中定义它，并将“ 无” 作为其值。例如，如果您要禁用用户代理中间件\n\n```python\nDOWNLOADER_MIDDLEWARES  =  {\n    'myproject.middlewares.CustomDownloaderMiddleware' ： 543 ，\n    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware' ： None ，\n}\n```\n\n最后，请记住，某些中间件可能需要通过特定设置启用\n\n### 3. 编写你自己的下载中间件\n\n每个中间件组件都是一个 Python 类，它定义了一个或多个以下方法\n\n```python\nclass scrapy.downloadermiddlewares.DownloaderMiddleware\n```\n\n> 任何下载器中间件方法也可能返回一个延迟\n\n#### 3.1 process_request(self, request, spider)\n\n> 当每个 request 通过下载中间件时，该方法被调用\n\n`process_request()`必须返回其中之一\n\n- 返回 None\n  - Scrapy 将继续处理该 request，执行其他的中间件的相应方法，直到合适的下载器处理函数(download handler)被调用，该 request 被执行(其 response 被下载)\n- 返回一个 Response 对象\n  - Scrapy 将不会调用 任何 其他的 process_request()或 process_exception()方法，或相应地下载函数； 其将返回该 response。已安装的中间件的 process_response()方法则会在每个 response 返回时被调用\n- 返回一个 Request 对象\n  - Scrapy 则停止调用 process_request 方法并重新调度返回的 request。当新返回的 request 被执行后， 相应地中间件链将会根据下载的 response 被调用\n- raise IgnoreRequest\n  - 如果抛出 一个 IgnoreRequest 异常，则安装的下载中间件的 process_exception() 方法会被调用。如果没有任何一个方法处理该异常， 则 request 的 errback(Request.errback)方法会被调用。如果没有代码处理抛出的异常， 则该异常被忽略且不记录(不同于其他异常那样)\n\n参数:\n\n- request (Request 对象) – 处理的 request\n- spider (Spider 对象) – 该 request 对应的 spider\n\n#### 3.2 process_response(self, request, response, spider)\n\n> 当下载器完成 http 请求，传递响应给引擎的时候调用\n\n- process_request() 必须返回以下其中之一:\n\n  - 返回一个 Response (可以与传入的 response 相同，也可以是全新的对象)， 该 response 会被在链中的其他中间件的 process_response() 方法处理。\n\n  - 返回一个 Request 对象，则中间件链停止， 返回的 request 会被重新调度下载。处理类似于 process_request() 返回 request 所做的那样。\n\n  - 抛出一个 IgnoreRequest 异常，则调用 request 的 errback(Request.errback)。 如果没有代码处理抛出的异常，则该异常被忽略且不记录(不同于其他异常那样)。\n\n- 参数:\n  - request (Request 对象) – response 所对应的 request\n  - response (Response 对象) – 被处理的 response\n  - spider (Spider 对象) – response 所对应的 spider\n\n### 4 使用代理\n\n`settings.py`\n\n```python\nPROXIES=[\n    {\"ip\":\"122.236.158.78:8118\"},\n    {\"ip\":\"112.245.78.90:8118\"}\n]\nDOWNLOADER_MIDDLEWARES = {\n    #'xiaoshuo.middlewares.XiaoshuoDownloaderMiddleware': 543,\n    'xiaoshuo.proxyMidde.ProxyMidde':100\n}\n```\n\n创建一个 `midderwares`\n\n```python\nfrom xiaoshuo.settings import PROXIES\nimport random\nclass ProxyMidde(object):\n    def process_request(self, request, spider):\n            proxy = random.choice(PROXIES)\n            request.meta['proxy']='http://'+proxy['ip']\n```\n\n写一个 spider 测试\n\n```python\nfrom scrapy import Spider\n\n\nclass ProxyIp(Spider):\n    name = 'ip'\n    #http://www.882667.com/\n    start_urls = ['http://ip.cn']\n\n    def parse(self, response):\n        print(response.text)\n\n```\n\n### 5 使用动态 UA\n\n```python\n# 随机的User-Agent\nclass RandomUserAgent(object):\n    def process_request(self, request, spider):\n        useragent = random.choice(USER_AGENTS)\n\n        request.headers.setdefault(\"User-Agent\", useragent)\n```\n",
      "html": "<h3 id=\"1.-spider-%E4%B8%8B%E8%BD%BD%E4%B8%AD%E9%97%B4%E4%BB%B6(middleware)\">1. Spider 下载中间件(Middleware) <a class=\"heading-anchor-permalink\" href=\"#1.-spider-%E4%B8%8B%E8%BD%BD%E4%B8%AD%E9%97%B4%E4%BB%B6(middleware)\">#</a></h3>\n<p>Spider 中间件(Middleware) 下载器中间件是介入到 Scrapy 的 spider 处理机制的钩子框架，您可以添加代码来处理发送给 Spiders 的 <code>response</code> 及 spider 产生的 <code>item</code> 和 <code>request</code></p>\n<h3 id=\"2.-%E6%BF%80%E6%B4%BB%E4%B8%80%E4%B8%AA%E4%B8%8B%E8%BD%BD-downloader_middlewares\">2. 激活一个下载 DOWNLOADER_MIDDLEWARES <a class=\"heading-anchor-permalink\" href=\"#2.-%E6%BF%80%E6%B4%BB%E4%B8%80%E4%B8%AA%E4%B8%8B%E8%BD%BD-downloader_middlewares\">#</a></h3>\n<p>要激活一个下载器中间件组件，将其添加到 <code>DOWNLOADER_MIDDLEWARES</code>设置中，该设置是一个字典，其键是中间件类路径，它们的值是中间件命令</p>\n<pre><code class=\"language-python\">DOWNLOADER_MIDDLEWARES  =  {\n    'myproject.middlewares.CustomDownloaderMiddleware' ： 543 ，\n}\n</code></pre>\n<p>该<code>DOWNLOADER_MIDDLEWARES</code>设置与<code>DOWNLOADER_MIDDLEWARES_BASEScrapy</code>中定义的设置（并不意味着被覆盖）合并， 然后按顺序排序，以获得最终的已启用中间件的排序列表：第一个中间件是靠近引擎的第一个中间件，最后一个是靠近引擎的中间件到下载器。换句话说，<code>process_request()</code> 每个中间件的方法将以增加中间件的顺序（100,200,300，…）<code>process_response()</code>被调用，并且每个中间件的方法将以降序调用</p>\n<p>要决定分配给中间件的顺序，请参阅 <code>DOWNLOADER_MIDDLEWARES_BASE</code>设置并根据要插入中间件的位置选择一个值。顺序很重要，因为每个中间件都执行不同的操作，而您的中间件可能依赖于之前（或后续）正在使用的中间件</p>\n<p>如果要禁用内置中间件（<code>DOWNLOADER_MIDDLEWARES_BASE</code>默认情况下已定义和启用的中间件 ），则必须在项目<code>DOWNLOADER_MIDDLEWARES</code>设置中定义它，并将“ 无” 作为其值。例如，如果您要禁用用户代理中间件</p>\n<pre><code class=\"language-python\">DOWNLOADER_MIDDLEWARES  =  {\n    'myproject.middlewares.CustomDownloaderMiddleware' ： 543 ，\n    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware' ： None ，\n}\n</code></pre>\n<p>最后，请记住，某些中间件可能需要通过特定设置启用</p>\n<h3 id=\"3.-%E7%BC%96%E5%86%99%E4%BD%A0%E8%87%AA%E5%B7%B1%E7%9A%84%E4%B8%8B%E8%BD%BD%E4%B8%AD%E9%97%B4%E4%BB%B6\">3. 编写你自己的下载中间件 <a class=\"heading-anchor-permalink\" href=\"#3.-%E7%BC%96%E5%86%99%E4%BD%A0%E8%87%AA%E5%B7%B1%E7%9A%84%E4%B8%8B%E8%BD%BD%E4%B8%AD%E9%97%B4%E4%BB%B6\">#</a></h3>\n<p>每个中间件组件都是一个 Python 类，它定义了一个或多个以下方法</p>\n<pre><code class=\"language-python\">class scrapy.downloadermiddlewares.DownloaderMiddleware\n</code></pre>\n<blockquote>\n<p>任何下载器中间件方法也可能返回一个延迟</p>\n</blockquote>\n<h4 id=\"3.1-process_request(self%2C-request%2C-spider)\">3.1 process_request(self, request, spider) <a class=\"heading-anchor-permalink\" href=\"#3.1-process_request(self%2C-request%2C-spider)\">#</a></h4>\n<blockquote>\n<p>当每个 request 通过下载中间件时，该方法被调用</p>\n</blockquote>\n<p><code>process_request()</code>必须返回其中之一</p>\n<ul>\n<li>返回 None\n<ul>\n<li>Scrapy 将继续处理该 request，执行其他的中间件的相应方法，直到合适的下载器处理函数(download handler)被调用，该 request 被执行(其 response 被下载)</li>\n</ul>\n</li>\n<li>返回一个 Response 对象\n<ul>\n<li>Scrapy 将不会调用 任何 其他的 process_request()或 process_exception()方法，或相应地下载函数； 其将返回该 response。已安装的中间件的 process_response()方法则会在每个 response 返回时被调用</li>\n</ul>\n</li>\n<li>返回一个 Request 对象\n<ul>\n<li>Scrapy 则停止调用 process_request 方法并重新调度返回的 request。当新返回的 request 被执行后， 相应地中间件链将会根据下载的 response 被调用</li>\n</ul>\n</li>\n<li>raise IgnoreRequest\n<ul>\n<li>如果抛出 一个 IgnoreRequest 异常，则安装的下载中间件的 process_exception() 方法会被调用。如果没有任何一个方法处理该异常， 则 request 的 errback(Request.errback)方法会被调用。如果没有代码处理抛出的异常， 则该异常被忽略且不记录(不同于其他异常那样)</li>\n</ul>\n</li>\n</ul>\n<p>参数:</p>\n<ul>\n<li>request (Request 对象) – 处理的 request</li>\n<li>spider (Spider 对象) – 该 request 对应的 spider</li>\n</ul>\n<h4 id=\"3.2-process_response(self%2C-request%2C-response%2C-spider)\">3.2 process_response(self, request, response, spider) <a class=\"heading-anchor-permalink\" href=\"#3.2-process_response(self%2C-request%2C-response%2C-spider)\">#</a></h4>\n<blockquote>\n<p>当下载器完成 http 请求，传递响应给引擎的时候调用</p>\n</blockquote>\n<ul>\n<li>\n<p>process_request() 必须返回以下其中之一:</p>\n<ul>\n<li>\n<p>返回一个 Response (可以与传入的 response 相同，也可以是全新的对象)， 该 response 会被在链中的其他中间件的 process_response() 方法处理。</p>\n</li>\n<li>\n<p>返回一个 Request 对象，则中间件链停止， 返回的 request 会被重新调度下载。处理类似于 process_request() 返回 request 所做的那样。</p>\n</li>\n<li>\n<p>抛出一个 IgnoreRequest 异常，则调用 request 的 errback(Request.errback)。 如果没有代码处理抛出的异常，则该异常被忽略且不记录(不同于其他异常那样)。</p>\n</li>\n</ul>\n</li>\n<li>\n<p>参数:</p>\n<ul>\n<li>request (Request 对象) – response 所对应的 request</li>\n<li>response (Response 对象) – 被处理的 response</li>\n<li>spider (Spider 对象) – response 所对应的 spider</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"4-%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%90%86\">4 使用代理 <a class=\"heading-anchor-permalink\" href=\"#4-%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%90%86\">#</a></h3>\n<p><code>settings.py</code></p>\n<pre><code class=\"language-python\">PROXIES=[\n    {&quot;ip&quot;:&quot;122.236.158.78:8118&quot;},\n    {&quot;ip&quot;:&quot;112.245.78.90:8118&quot;}\n]\nDOWNLOADER_MIDDLEWARES = {\n    #'xiaoshuo.middlewares.XiaoshuoDownloaderMiddleware': 543,\n    'xiaoshuo.proxyMidde.ProxyMidde':100\n}\n</code></pre>\n<p>创建一个 <code>midderwares</code></p>\n<pre><code class=\"language-python\">from xiaoshuo.settings import PROXIES\nimport random\nclass ProxyMidde(object):\n    def process_request(self, request, spider):\n            proxy = random.choice(PROXIES)\n            request.meta['proxy']='http://'+proxy['ip']\n</code></pre>\n<p>写一个 spider 测试</p>\n<pre><code class=\"language-python\">from scrapy import Spider\n\n\nclass ProxyIp(Spider):\n    name = 'ip'\n    #http://www.882667.com/\n    start_urls = ['http://ip.cn']\n\n    def parse(self, response):\n        print(response.text)\n\n</code></pre>\n<h3 id=\"5-%E4%BD%BF%E7%94%A8%E5%8A%A8%E6%80%81-ua\">5 使用动态 UA <a class=\"heading-anchor-permalink\" href=\"#5-%E4%BD%BF%E7%94%A8%E5%8A%A8%E6%80%81-ua\">#</a></h3>\n<pre><code class=\"language-python\"># 随机的User-Agent\nclass RandomUserAgent(object):\n    def process_request(self, request, spider):\n        useragent = random.choice(USER_AGENTS)\n\n        request.headers.setdefault(&quot;User-Agent&quot;, useragent)\n</code></pre>\n",
      "id": 2
    },
    {
      "path": "Scrapy框架高级与动态页面的爬取/Scrapy 框架 - 图片管道使用.md",
      "url": "Scrapy框架高级与动态页面的爬取/Scrapy 框架 - 图片管道使用.html",
      "content": "### 1. 介绍\n\nScrapy 提供了一个 item pipeline ，来下载属于某个特定项目的图片，比如，当你抓取产品时，也想把它们的图片下载到本地。\n\n这条管道，被称作图片管道，在 `ImagesPipeline` 类中实现，提供了一个方便并具有额外特性的方法，来下载并本地存储图片:\n\n- 将所有下载的图片转换成通用的格式（JPG）和模式（RGB）\n- 避免重新下载最近已经下载过的图片\n- 缩略图生成\n- 检测图像的宽/高，确保它们满足最小限制\n\n这个管道也会为那些当前安排好要下载的图片保留一个内部队列，并将那些到达的包含相同图片的项目连接到那个队列中。 这可以避免多次下载几个项目共享的同一个图片\n\n### 2. 使用图片管道\n\n当使用 `ImagesPipeline` ，典型的工作流程如下所示:\n\n1. 在一个爬虫里，你抓取一个项目，把其中图片的 URL 放入 `image_urls` 组内\n2. 项目从爬虫内返回，进入项目管道\n3. 当项目进入 `ImagesPipeline` ，`image_urls` 组内的 URLs 将被 Scrapy 的调度器和下载器（这意味着调度器和下载器的中间件可以复用）安排下载，当优先级更高，会在其他页面被抓取前处理。项目会在这个特定的管道阶段保持`locker`的状态，直到完成图片的下载（或者由于某些原因未完成下载）。\n4. 当图片下载完，另一个组`images`将被更新到结构中。这个组将包含一个字典列表，其中包括下载图片的信息，比如下载路径、源抓取地址（从 `image_urls` 组获得）和图片的校验码。 `images` 列表中的图片顺序将和源 `image_urls` 组保持一致。如果某个图片下载失败，将会记录下错误信息，图片也不会出现在 `images` 组中\n\n### 3. 具体流程(此处以 zol 网站为例)\n\n1. 定义 item\n\n    ```python\n    import scrapy\n\n\n    class ImagedownloadItem(scrapy.Item):\n        # define the fields for your item here like:\n        img_name = scrapy.Field()\n        img_urls =scrapy.Field()\n    ```\n\n2. 编写 spider\n\n    > 思路：获取文件地址-->获取图片名称-->推送地址\n\n    此处是一张一张的推送\n\n    ```python\n    class ZolSpiderSpider(scrapy.Spider):\n        name = 'zol'\n        allowed_domains = ['zol.com.cn']\n        url ='http://desk.zol.com.cn'\n        start_urls = [url+'/bizhi/7106_88025_2.html']\n\n        def parse(self, response):\n            image_url = response.xpath('//img[@id=\"bigImg\"]/@src').extract_first()\n            image_name = response.xpath('//h3')[0].xpath('string(.)').extract_first().strip().replace('\\r\\n\\t\\t', '')\n            next_image = response.xpath('//a[@id=\"pageNext\"]/@href').extract_first()\n            item = ImagedownloadItem()\n            item[\"img_name\"] = image_name\n            item[\"img_urls\"] = image_url\n            yield item\n\n            yield scrapy.Request(self.url+next_image,callback=self.parse,)\n    ```\n\n3. 编写 pipline\n\n    以下如果不想改文件名，`meta` 属性可以忽略不写\n\n    ```python\n        def get_media_requests(self, item, info):\n            '''\n            #如果item[urls]里里面是列表，用下面\n            urls= item['urls']\n            for url in urls:\n                yield scrapy.Request(url,meta={\"item\",item})\n            '''\n            # 如果item[urls]里里面是一个图片地址，用这下面的\n            yield scrapy.Request(item['img_urls'], meta={\"item\": item})\n    ```\n\n    因为 scrapy 里是使用它们 URL 的 `SHA1 hash` 作为文件名，所以如果想重命名：\n\n    ```python\n    def file_path(self, request, response=None, info=None):\n            item = request.meta[\"item\"]\n            #去掉文件里的/,避免创建图片文件时出错\n            filename = item[\"img_name\"].replace(\"/\",\"-\")+\".jpg\"\n\n            return filename\n    ```\n\n4. 定义图片保存在哪？\n    在 `settings` 中增加一句\n\n    ```python\n    IMAGES_STORE = \"e:/pics\"\n    ```\n",
      "html": "<h3 id=\"1.-%E4%BB%8B%E7%BB%8D\">1. 介绍 <a class=\"heading-anchor-permalink\" href=\"#1.-%E4%BB%8B%E7%BB%8D\">#</a></h3>\n<p>Scrapy 提供了一个 item pipeline ，来下载属于某个特定项目的图片，比如，当你抓取产品时，也想把它们的图片下载到本地。</p>\n<p>这条管道，被称作图片管道，在 <code>ImagesPipeline</code> 类中实现，提供了一个方便并具有额外特性的方法，来下载并本地存储图片:</p>\n<ul>\n<li>将所有下载的图片转换成通用的格式（JPG）和模式（RGB）</li>\n<li>避免重新下载最近已经下载过的图片</li>\n<li>缩略图生成</li>\n<li>检测图像的宽/高，确保它们满足最小限制</li>\n</ul>\n<p>这个管道也会为那些当前安排好要下载的图片保留一个内部队列，并将那些到达的包含相同图片的项目连接到那个队列中。 这可以避免多次下载几个项目共享的同一个图片</p>\n<h3 id=\"2.-%E4%BD%BF%E7%94%A8%E5%9B%BE%E7%89%87%E7%AE%A1%E9%81%93\">2. 使用图片管道 <a class=\"heading-anchor-permalink\" href=\"#2.-%E4%BD%BF%E7%94%A8%E5%9B%BE%E7%89%87%E7%AE%A1%E9%81%93\">#</a></h3>\n<p>当使用 <code>ImagesPipeline</code> ，典型的工作流程如下所示:</p>\n<ol>\n<li>在一个爬虫里，你抓取一个项目，把其中图片的 URL 放入 <code>image_urls</code> 组内</li>\n<li>项目从爬虫内返回，进入项目管道</li>\n<li>当项目进入 <code>ImagesPipeline</code> ，<code>image_urls</code> 组内的 URLs 将被 Scrapy 的调度器和下载器（这意味着调度器和下载器的中间件可以复用）安排下载，当优先级更高，会在其他页面被抓取前处理。项目会在这个特定的管道阶段保持<code>locker</code>的状态，直到完成图片的下载（或者由于某些原因未完成下载）。</li>\n<li>当图片下载完，另一个组<code>images</code>将被更新到结构中。这个组将包含一个字典列表，其中包括下载图片的信息，比如下载路径、源抓取地址（从 <code>image_urls</code> 组获得）和图片的校验码。 <code>images</code> 列表中的图片顺序将和源 <code>image_urls</code> 组保持一致。如果某个图片下载失败，将会记录下错误信息，图片也不会出现在 <code>images</code> 组中</li>\n</ol>\n<h3 id=\"3.-%E5%85%B7%E4%BD%93%E6%B5%81%E7%A8%8B(%E6%AD%A4%E5%A4%84%E4%BB%A5-zol-%E7%BD%91%E7%AB%99%E4%B8%BA%E4%BE%8B)\">3. 具体流程(此处以 zol 网站为例) <a class=\"heading-anchor-permalink\" href=\"#3.-%E5%85%B7%E4%BD%93%E6%B5%81%E7%A8%8B(%E6%AD%A4%E5%A4%84%E4%BB%A5-zol-%E7%BD%91%E7%AB%99%E4%B8%BA%E4%BE%8B)\">#</a></h3>\n<ol>\n<li>\n<p>定义 item</p>\n<pre><code class=\"language-python\">import scrapy\n\n\nclass ImagedownloadItem(scrapy.Item):\n    # define the fields for your item here like:\n    img_name = scrapy.Field()\n    img_urls =scrapy.Field()\n</code></pre>\n</li>\n<li>\n<p>编写 spider</p>\n<blockquote>\n<p>思路：获取文件地址–&gt;获取图片名称–&gt;推送地址</p>\n</blockquote>\n<p>此处是一张一张的推送</p>\n<pre><code class=\"language-python\">class ZolSpiderSpider(scrapy.Spider):\n    name = 'zol'\n    allowed_domains = ['zol.com.cn']\n    url ='http://desk.zol.com.cn'\n    start_urls = [url+'/bizhi/7106_88025_2.html']\n\n    def parse(self, response):\n        image_url = response.xpath('//img[@id=&quot;bigImg&quot;]/@src').extract_first()\n        image_name = response.xpath('//h3')[0].xpath('string(.)').extract_first().strip().replace('\\r\\n\\t\\t', '')\n        next_image = response.xpath('//a[@id=&quot;pageNext&quot;]/@href').extract_first()\n        item = ImagedownloadItem()\n        item[&quot;img_name&quot;] = image_name\n        item[&quot;img_urls&quot;] = image_url\n        yield item\n\n        yield scrapy.Request(self.url+next_image,callback=self.parse,)\n</code></pre>\n</li>\n<li>\n<p>编写 pipline</p>\n<p>以下如果不想改文件名，<code>meta</code> 属性可以忽略不写</p>\n<pre><code class=\"language-python\">    def get_media_requests(self, item, info):\n        '''\n        #如果item[urls]里里面是列表，用下面\n        urls= item['urls']\n        for url in urls:\n            yield scrapy.Request(url,meta={&quot;item&quot;,item})\n        '''\n        # 如果item[urls]里里面是一个图片地址，用这下面的\n        yield scrapy.Request(item['img_urls'], meta={&quot;item&quot;: item})\n</code></pre>\n<p>因为 scrapy 里是使用它们 URL 的 <code>SHA1 hash</code> 作为文件名，所以如果想重命名：</p>\n<pre><code class=\"language-python\">def file_path(self, request, response=None, info=None):\n        item = request.meta[&quot;item&quot;]\n        #去掉文件里的/,避免创建图片文件时出错\n        filename = item[&quot;img_name&quot;].replace(&quot;/&quot;,&quot;-&quot;)+&quot;.jpg&quot;\n\n        return filename\n</code></pre>\n</li>\n<li>\n<p>定义图片保存在哪？\n在 <code>settings</code> 中增加一句</p>\n<pre><code class=\"language-python\">IMAGES_STORE = &quot;e:/pics&quot;\n</code></pre>\n</li>\n</ol>\n",
      "id": 3
    },
    {
      "path": "Scrapy框架高级与动态页面的爬取/Scrapy 框架 - 模拟登录——Request、Response.md",
      "url": "Scrapy框架高级与动态页面的爬取/Scrapy 框架 - 模拟登录——Request、Response.html",
      "content": "### 1. Scrapy - Request 和 Response（请求和响应）\n\nScrapy 的 `Request` 和 `Response` 对象用于爬网网站。\n\n通常，`Request` 对象在爬虫程序中生成并传递到系统，直到它们到达下载程序，后者执行请求并返回一个 `Response` 对象，该对象返回到发出请求的爬虫程序。\n\n```text\nsequenceDiagram\n爬虫->>Request: 创建\nRequest->>Response:获取下载数据\nResponse->>爬虫:数据\n```\n\n### 2. Request 对象\n\n```python\nclass scrapy.http.Request(url[, callback, method='GET', headers, body, cookies, meta, encoding='utf-8', priority=0, dont_filter=False, errback])\n```\n\n一个 `Request` 对象表示一个 HTTP 请求，它通常是在爬虫生成，并由下载执行，从而生成 `Response`\n\n| 参数                   | 说明                                                                                                                                                                                                                 |\n| ---------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| url（string）          | 此请求的网址                                                                                                                                                                                                         |\n| callback（callable）   | 将使用此请求的响应（一旦下载）作为其第一个参数调用的函数。有关更多信息，请参阅下面的将附加数据传递给回调函数。如果请求没有指定回调，parse()将使用 spider 的 方法。请注意，如果在处理期间引发异常，则会调用 errback。 |\n| method（string）       | 此请求的 HTTP 方法。默认为'GET'。可设置为\"GET\", \"POST\", \"PUT\"等，且保证字符串大写                                                                                                                                    |\n| meta（dict）           | 属性的初始值 Request.meta,在不同的请求之间传递数据使用                                                                                                                                                               |\n| body（str 或 unicode） | 请求体。如果 unicode 传递了 a，那么它被编码为 str 使用传递的编码（默认为 utf-8）。如果 body 没有给出，则存储一个空字符串。不管这个参数的类型，存储的最终值将是一个 str（不会是 unicode 或 None）。                   |\n| headers（dict）        | 这个请求的头。dict 值可以是字符串（对于单值标头）或列表（对于多值标头）。如果 None 作为值传递，则不会发送 HTTP 头.一般不需要                                                                                         |\n| encoding               | 使用默认的 'utf-8' 就行。                                                                                                                                                                                            |\n| cookie（dict 或 list） | 请求 cookie。这些可以以两种形式发送。                                                                                                                                                                                |\n\n- 使用 dict：\n\n  ```python\n  request_with_cookies = Request(url=\"http://www.sxt.cn/index/login/login.html\",)\n  ```\n\n- 使用列表：\n\n  ```python\n  request_with_cookies = Request(url=\"http://www.example.com\",\n                              cookies=[{'name': 'currency',\n                                      'value': 'USD',\n                                      'domain': 'example.com',\n                                      'path': '/currency'}])\n  ```\n\n  后一种形式允许定制 cookie 的 `domain` 属性和 `path` 属性。这只有在保存 Cookie 用于以后的请求时才有用\n\n  ```python\n  request_with_cookies = Request(url=\"http://www.example.com\",\n                              cookies={'currency': 'USD', 'country': 'UY'},\n                              meta={'dont_merge_cookies': True})\n  ```\n\n#### 将附加数据传递给回调函数\n\n请求的回调是当下载该请求的响应时将被调用的函数。将使用下载的 Response 对象作为其第一个参数来调用回调函数\n\n```python\ndef parse_page1(self, response):\n    item = MyItem()\n    item['main_url'] = response.url\n    request = scrapy.Request(\"http://www.example.com/some_page.html\",\n                             callback=self.parse_page2)\n    request.meta['item'] = item\n    return request\n\ndef parse_page2(self, response):\n    item = response.meta['item']\n    item['other_url'] = response.url\n    return item\n```\n\n### 3. 请求子类 FormRequest 对象\n\n`FormRequest` 类扩展了 Request 具有处理 HTML 表单的功能的基础。它使用 `lxml.html` 表单 从 `Response` 对象的表单数据预填充表单字段\n\n```python\nclass scrapy.http.FormRequest(url[, formdata, ...])\n```\n\n本 `FormRequest` 类增加了新的构造函数的参数。其余的参数与 `Request` 类相同，这里没有记录\n\n- 参数：`formdata`（元组的 dict 或 iterable） - 是一个包含 HTML Form 数据的字典（或（key，value）元组的迭代），它将被 url 编码并分配给请求的主体。\n\n该 `FormRequest` 对象支持除标准以下类方法 Request 的方法：\n\n```python\nclassmethod from_response(response[, formname=None, formid=None, formnumber=0, formdata=None, formxpath=None, formcss=None, clickdata=None, dont_click=False, ...])\n```\n\n返回一个新 `FormRequest` 对象，其中的表单字段值已预先 `<form>` 填充在给定响应中包含的 HTML 元素中.\n\n| 参数                       | 说明                                                                                                                                                                         |\n| -------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| response（Responseobject） | 包含将用于预填充表单字段的 HTML 表单的响应                                                                                                                                   |\n| formname（string）         | 如果给定，将使用 name 属性设置为此值的形式                                                                                                                                   |\n| formid（string）           | 如果给定，将使用 id 属性设置为此值的形式                                                                                                                                     |\n| formxpath（string）        | 如果给定，将使用匹配 xpath 的第一个表单                                                                                                                                      |\n| formcss（string）          | 如果给定，将使用匹配 css 选择器的第一个形式                                                                                                                                  |\n| formnumber（integer）      | 当响应包含多个表单时要使用的表单的数量。第一个（也是默认）是 0                                                                                                               |\n| formdata（dict）           | 要在表单数据中覆盖的字段。如果响应元素中已存在字段，则其值将被在此参数中传递的值覆盖                                                                                         |\n| clickdata（dict）          | 查找控件被点击的属性。如果没有提供，表单数据将被提交，模拟第一个可点击元素的点击。除了 html 属性，控件可以通过其相对于表单中其他提交表输入的基于零的索引，通过 nr 属性来标识 |\n| dont_click（boolean）      | 如果为 True，表单数据将在不点击任何元素的情况下提交                                                                                                                          |\n\n#### 3.1 请求使用示例\n\n使用 `FormRequest` 通过 HTTP POST 发送数据\n\n如果你想在你的爬虫中模拟 HTML 表单 POST 并发送几个键值字段，你可以返回一个 `FormRequest` 对象（从你的爬虫）像这样：\n\n```python\nreturn [FormRequest(url=\"http://www.example.com/post/action\",\n                    formdata={'name': 'John Doe', 'age': '27'},\n                    callback=self.after_post)]\n```\n\n使用 `FormRequest.from_response()` 来模拟用户登录\n\n网站通常通过元素（例如会话相关数据或认证令牌（用于登录页面））提供预填充的表单字段。进行剪贴时，您需要自动预填充这些字段，并且只覆盖其中的一些，例如用户名和密码。您可以使用 此作业的方法。这里有一个使用它的爬虫示例：\n\n```html\n<input type=\"hidden\" /> FormRequest.from_response()\n```\n\n```python\nimport scrapy\n\nclass LoginSpider(scrapy.Spider):\n    name = 'example.com'\n    start_urls = ['http://www.example.com/users/login.php']\n\n    def parse(self, response):\n        return scrapy.FormRequest.from_response(\n            response,\n            formdata={'username': 'john', 'password': 'secret'},\n            callback=self.after_login\n        )\n\n    def after_login(self, response):\n        # check login succeed before going on\n        if \"authentication failed\" in response.body:\n            self.logger.error(\"Login failed\")\n            return\n\n        # continue scraping with authenticated session...\n```\n\n### 4 响应对象\n\n```python\nclass scrapy.http.Response(url[, status=200, headers=None, body=b'', flags=None, request=None])\n```\n\n一个 `Response` 对象表示的 HTTP 响应，这通常是下载（由下载），并供给到爬虫进行处理\n\n| 参数                     | 说明                                                                                  |\n| ------------------------ | ------------------------------------------------------------------------------------- |\n| url（string）            | 此响应的 URL                                                                          |\n| status（integer）        | 响应的 HTTP 状态。默认为 200                                                          |\n| headers（dict）          | 这个响应的头。dict 值可以是字符串（对于单值标头）或列表（对于多值标头）               |\n| body（str）              | 响应体。它必须是 str，而不是 unicode，除非你使用一个编码感知响应子类，如 TextResponse |\n| flags（list）            | 是一个包含属性初始值的 Response.flags 列表。如果给定，列表将被浅复制                  |\n| request（Requestobject） | 属性的初始值 Response.request。这代表 Request 生成此响应                              |\n\n### 5. 模拟登录\n\n使用的函数\n\n| 函数                        | 说明                                                                                                                    |\n| --------------------------- | ----------------------------------------------------------------------------------------------------------------------- |\n| start_requests()            | 可以返回一个请求给爬虫的起始网站，这个返回的请求相当于 start_urls，start_requests()返回的请求会替代 start_urls 里的请求 |\n| Request()                   | get 请求，可以设置，url、cookie、回调函数                                                                               |\n| FormRequest.from_response() | 表单 post 提交，第一个必须参数，上一次响应 cookie 的 response 对象，其他参数，cookie、url、表单内容等                   |\n| yield Request()             | 可以将一个新的请求返回给爬虫执行                                                                                        |\n\n在发送请求时 cookie 的操作\n\n| 操作                                          | 说明                                                                             |\n| --------------------------------------------- | -------------------------------------------------------------------------------- |\n| meta={'cookiejar':1}                          | 表示开启 cookie 记录，首次请求时写在 Request() 里                                |\n| meta={'cookiejar':response.meta['cookiejar']} | 表示使用上一次 response 的 cookie，写在 FormRequest.from_response() 里 post 授权 |\n| meta={'cookiejar':True}                       | 表示使用授权后的 cookie 访问需要登录查看的页面                                   |\n\n#### 获取 Scrapy 框架 Cookies\n\n样例代码\n\n`start_requests()`方法，可以返回一个请求给爬虫的起始网站，这个返回的请求相当于 `start_urls`, `start_requests()` 返回的请求会替代 `start_urls` 里的请求\n\n在发送请求时 cookie 的操作\n\n`meta={'cookiejar':1}`表示开启 cookie 记录，首次请求时写在 `Request()` 里\n\n`meta={'cookiejar':response.meta['cookiejar']}`表示使用上一次 Response 的 cookie，写在 `Request` 里 POST 授权\n\n```python\nimport scrapy\nfrom scrapy import Request\nfrom scrapy import FormRequest\n\n\nclass SxtSpiderSpider(scrapy.Spider):\n    name = 'sxt1'\n    allowed_domains = ['sxt.cn']\n\n    def start_requests(self):\n        return [Request('http://www.sxt.cn/index/login/login.html', meta={'cookiejar': 1}, callback=self.parse)]\n\n    def parse(self, response):\n        formdata = {\n            \"user\": \"17703181473\", \"password\": \"123456\"\n        }\n        return FormRequest(                                        formdata=formdata,\n                                        url='http://www.sxt.cn/index/login/login.html',\n                                        meta={'cookiejar': response.meta['cookiejar']},\n                                        callback=self.login_after)\n\n    def login_after(self, response):\n        yield scrapy.Request('http://www.sxt.cn/index/user.html',\n                             meta={\"cookiejar\": response.meta['cookiejar']},\n                             callback=self.next)\n    def next(self,response):\n        print(response.text)\n```\n",
      "html": "<h3 id=\"1.-scrapy---request-%E5%92%8C-response%EF%BC%88%E8%AF%B7%E6%B1%82%E5%92%8C%E5%93%8D%E5%BA%94%EF%BC%89\">1. Scrapy - Request 和 Response（请求和响应） <a class=\"heading-anchor-permalink\" href=\"#1.-scrapy---request-%E5%92%8C-response%EF%BC%88%E8%AF%B7%E6%B1%82%E5%92%8C%E5%93%8D%E5%BA%94%EF%BC%89\">#</a></h3>\n<p>Scrapy 的 <code>Request</code> 和 <code>Response</code> 对象用于爬网网站。</p>\n<p>通常，<code>Request</code> 对象在爬虫程序中生成并传递到系统，直到它们到达下载程序，后者执行请求并返回一个 <code>Response</code> 对象，该对象返回到发出请求的爬虫程序。</p>\n<pre><code class=\"language-text\">sequenceDiagram\n爬虫-&gt;&gt;Request: 创建\nRequest-&gt;&gt;Response:获取下载数据\nResponse-&gt;&gt;爬虫:数据\n</code></pre>\n<h3 id=\"2.-request-%E5%AF%B9%E8%B1%A1\">2. Request 对象 <a class=\"heading-anchor-permalink\" href=\"#2.-request-%E5%AF%B9%E8%B1%A1\">#</a></h3>\n<pre><code class=\"language-python\">class scrapy.http.Request(url[, callback, method='GET', headers, body, cookies, meta, encoding='utf-8', priority=0, dont_filter=False, errback])\n</code></pre>\n<p>一个 <code>Request</code> 对象表示一个 HTTP 请求，它通常是在爬虫生成，并由下载执行，从而生成 <code>Response</code></p>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>url（string）</td>\n<td>此请求的网址</td>\n</tr>\n<tr>\n<td>callback（callable）</td>\n<td>将使用此请求的响应（一旦下载）作为其第一个参数调用的函数。有关更多信息，请参阅下面的将附加数据传递给回调函数。如果请求没有指定回调，parse()将使用 spider 的 方法。请注意，如果在处理期间引发异常，则会调用 errback。</td>\n</tr>\n<tr>\n<td>method（string）</td>\n<td>此请求的 HTTP 方法。默认为’GET’。可设置为&quot;GET&quot;, “POST”, &quot;PUT&quot;等，且保证字符串大写</td>\n</tr>\n<tr>\n<td>meta（dict）</td>\n<td>属性的初始值 Request.meta,在不同的请求之间传递数据使用</td>\n</tr>\n<tr>\n<td>body（str 或 unicode）</td>\n<td>请求体。如果 unicode 传递了 a，那么它被编码为 str 使用传递的编码（默认为 utf-8）。如果 body 没有给出，则存储一个空字符串。不管这个参数的类型，存储的最终值将是一个 str（不会是 unicode 或 None）。</td>\n</tr>\n<tr>\n<td>headers（dict）</td>\n<td>这个请求的头。dict 值可以是字符串（对于单值标头）或列表（对于多值标头）。如果 None 作为值传递，则不会发送 HTTP 头.一般不需要</td>\n</tr>\n<tr>\n<td>encoding</td>\n<td>使用默认的 ‘utf-8’ 就行。</td>\n</tr>\n<tr>\n<td>cookie（dict 或 list）</td>\n<td>请求 cookie。这些可以以两种形式发送。</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>\n<p>使用 dict：</p>\n<pre><code class=\"language-python\">request_with_cookies = Request(url=&quot;http://www.sxt.cn/index/login/login.html&quot;,)\n</code></pre>\n</li>\n<li>\n<p>使用列表：</p>\n<pre><code class=\"language-python\">request_with_cookies = Request(url=&quot;http://www.example.com&quot;,\n                            cookies=[{'name': 'currency',\n                                    'value': 'USD',\n                                    'domain': 'example.com',\n                                    'path': '/currency'}])\n</code></pre>\n<p>后一种形式允许定制 cookie 的 <code>domain</code> 属性和 <code>path</code> 属性。这只有在保存 Cookie 用于以后的请求时才有用</p>\n<pre><code class=\"language-python\">request_with_cookies = Request(url=&quot;http://www.example.com&quot;,\n                            cookies={'currency': 'USD', 'country': 'UY'},\n                            meta={'dont_merge_cookies': True})\n</code></pre>\n</li>\n</ul>\n<h4 id=\"%E5%B0%86%E9%99%84%E5%8A%A0%E6%95%B0%E6%8D%AE%E4%BC%A0%E9%80%92%E7%BB%99%E5%9B%9E%E8%B0%83%E5%87%BD%E6%95%B0\">将附加数据传递给回调函数 <a class=\"heading-anchor-permalink\" href=\"#%E5%B0%86%E9%99%84%E5%8A%A0%E6%95%B0%E6%8D%AE%E4%BC%A0%E9%80%92%E7%BB%99%E5%9B%9E%E8%B0%83%E5%87%BD%E6%95%B0\">#</a></h4>\n<p>请求的回调是当下载该请求的响应时将被调用的函数。将使用下载的 Response 对象作为其第一个参数来调用回调函数</p>\n<pre><code class=\"language-python\">def parse_page1(self, response):\n    item = MyItem()\n    item['main_url'] = response.url\n    request = scrapy.Request(&quot;http://www.example.com/some_page.html&quot;,\n                             callback=self.parse_page2)\n    request.meta['item'] = item\n    return request\n\ndef parse_page2(self, response):\n    item = response.meta['item']\n    item['other_url'] = response.url\n    return item\n</code></pre>\n<h3 id=\"3.-%E8%AF%B7%E6%B1%82%E5%AD%90%E7%B1%BB-formrequest-%E5%AF%B9%E8%B1%A1\">3. 请求子类 FormRequest 对象 <a class=\"heading-anchor-permalink\" href=\"#3.-%E8%AF%B7%E6%B1%82%E5%AD%90%E7%B1%BB-formrequest-%E5%AF%B9%E8%B1%A1\">#</a></h3>\n<p><code>FormRequest</code> 类扩展了 Request 具有处理 HTML 表单的功能的基础。它使用 <code>lxml.html</code> 表单 从 <code>Response</code> 对象的表单数据预填充表单字段</p>\n<pre><code class=\"language-python\">class scrapy.http.FormRequest(url[, formdata, ...])\n</code></pre>\n<p>本 <code>FormRequest</code> 类增加了新的构造函数的参数。其余的参数与 <code>Request</code> 类相同，这里没有记录</p>\n<ul>\n<li>参数：<code>formdata</code>（元组的 dict 或 iterable） - 是一个包含 HTML Form 数据的字典（或（key，value）元组的迭代），它将被 url 编码并分配给请求的主体。</li>\n</ul>\n<p>该 <code>FormRequest</code> 对象支持除标准以下类方法 Request 的方法：</p>\n<pre><code class=\"language-python\">classmethod from_response(response[, formname=None, formid=None, formnumber=0, formdata=None, formxpath=None, formcss=None, clickdata=None, dont_click=False, ...])\n</code></pre>\n<p>返回一个新 <code>FormRequest</code> 对象，其中的表单字段值已预先 <code>&lt;form&gt;</code> 填充在给定响应中包含的 HTML 元素中.</p>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>response（Responseobject）</td>\n<td>包含将用于预填充表单字段的 HTML 表单的响应</td>\n</tr>\n<tr>\n<td>formname（string）</td>\n<td>如果给定，将使用 name 属性设置为此值的形式</td>\n</tr>\n<tr>\n<td>formid（string）</td>\n<td>如果给定，将使用 id 属性设置为此值的形式</td>\n</tr>\n<tr>\n<td>formxpath（string）</td>\n<td>如果给定，将使用匹配 xpath 的第一个表单</td>\n</tr>\n<tr>\n<td>formcss（string）</td>\n<td>如果给定，将使用匹配 css 选择器的第一个形式</td>\n</tr>\n<tr>\n<td>formnumber（integer）</td>\n<td>当响应包含多个表单时要使用的表单的数量。第一个（也是默认）是 0</td>\n</tr>\n<tr>\n<td>formdata（dict）</td>\n<td>要在表单数据中覆盖的字段。如果响应元素中已存在字段，则其值将被在此参数中传递的值覆盖</td>\n</tr>\n<tr>\n<td>clickdata（dict）</td>\n<td>查找控件被点击的属性。如果没有提供，表单数据将被提交，模拟第一个可点击元素的点击。除了 html 属性，控件可以通过其相对于表单中其他提交表输入的基于零的索引，通过 nr 属性来标识</td>\n</tr>\n<tr>\n<td>dont_click（boolean）</td>\n<td>如果为 True，表单数据将在不点击任何元素的情况下提交</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"3.1-%E8%AF%B7%E6%B1%82%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B\">3.1 请求使用示例 <a class=\"heading-anchor-permalink\" href=\"#3.1-%E8%AF%B7%E6%B1%82%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B\">#</a></h4>\n<p>使用 <code>FormRequest</code> 通过 HTTP POST 发送数据</p>\n<p>如果你想在你的爬虫中模拟 HTML 表单 POST 并发送几个键值字段，你可以返回一个 <code>FormRequest</code> 对象（从你的爬虫）像这样：</p>\n<pre><code class=\"language-python\">return [FormRequest(url=&quot;http://www.example.com/post/action&quot;,\n                    formdata={'name': 'John Doe', 'age': '27'},\n                    callback=self.after_post)]\n</code></pre>\n<p>使用 <code>FormRequest.from_response()</code> 来模拟用户登录</p>\n<p>网站通常通过元素（例如会话相关数据或认证令牌（用于登录页面））提供预填充的表单字段。进行剪贴时，您需要自动预填充这些字段，并且只覆盖其中的一些，例如用户名和密码。您可以使用 此作业的方法。这里有一个使用它的爬虫示例：</p>\n<pre><code class=\"language-html\">&lt;input type=&quot;hidden&quot; /&gt; FormRequest.from_response()\n</code></pre>\n<pre><code class=\"language-python\">import scrapy\n\nclass LoginSpider(scrapy.Spider):\n    name = 'example.com'\n    start_urls = ['http://www.example.com/users/login.php']\n\n    def parse(self, response):\n        return scrapy.FormRequest.from_response(\n            response,\n            formdata={'username': 'john', 'password': 'secret'},\n            callback=self.after_login\n        )\n\n    def after_login(self, response):\n        # check login succeed before going on\n        if &quot;authentication failed&quot; in response.body:\n            self.logger.error(&quot;Login failed&quot;)\n            return\n\n        # continue scraping with authenticated session...\n</code></pre>\n<h3 id=\"4-%E5%93%8D%E5%BA%94%E5%AF%B9%E8%B1%A1\">4 响应对象 <a class=\"heading-anchor-permalink\" href=\"#4-%E5%93%8D%E5%BA%94%E5%AF%B9%E8%B1%A1\">#</a></h3>\n<pre><code class=\"language-python\">class scrapy.http.Response(url[, status=200, headers=None, body=b'', flags=None, request=None])\n</code></pre>\n<p>一个 <code>Response</code> 对象表示的 HTTP 响应，这通常是下载（由下载），并供给到爬虫进行处理</p>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>url（string）</td>\n<td>此响应的 URL</td>\n</tr>\n<tr>\n<td>status（integer）</td>\n<td>响应的 HTTP 状态。默认为 200</td>\n</tr>\n<tr>\n<td>headers（dict）</td>\n<td>这个响应的头。dict 值可以是字符串（对于单值标头）或列表（对于多值标头）</td>\n</tr>\n<tr>\n<td>body（str）</td>\n<td>响应体。它必须是 str，而不是 unicode，除非你使用一个编码感知响应子类，如 TextResponse</td>\n</tr>\n<tr>\n<td>flags（list）</td>\n<td>是一个包含属性初始值的 Response.flags 列表。如果给定，列表将被浅复制</td>\n</tr>\n<tr>\n<td>request（Requestobject）</td>\n<td>属性的初始值 Response.request。这代表 Request 生成此响应</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"5.-%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95\">5. 模拟登录 <a class=\"heading-anchor-permalink\" href=\"#5.-%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95\">#</a></h3>\n<p>使用的函数</p>\n<table>\n<thead>\n<tr>\n<th>函数</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>start_requests()</td>\n<td>可以返回一个请求给爬虫的起始网站，这个返回的请求相当于 start_urls，start_requests()返回的请求会替代 start_urls 里的请求</td>\n</tr>\n<tr>\n<td>Request()</td>\n<td>get 请求，可以设置，url、cookie、回调函数</td>\n</tr>\n<tr>\n<td>FormRequest.from_response()</td>\n<td>表单 post 提交，第一个必须参数，上一次响应 cookie 的 response 对象，其他参数，cookie、url、表单内容等</td>\n</tr>\n<tr>\n<td>yield Request()</td>\n<td>可以将一个新的请求返回给爬虫执行</td>\n</tr>\n</tbody>\n</table>\n<p>在发送请求时 cookie 的操作</p>\n<table>\n<thead>\n<tr>\n<th>操作</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>meta={‘cookiejar’:1}</td>\n<td>表示开启 cookie 记录，首次请求时写在 Request() 里</td>\n</tr>\n<tr>\n<td>meta={‘cookiejar’:response.meta[‘cookiejar’]}</td>\n<td>表示使用上一次 response 的 cookie，写在 FormRequest.from_response() 里 post 授权</td>\n</tr>\n<tr>\n<td>meta={‘cookiejar’:True}</td>\n<td>表示使用授权后的 cookie 访问需要登录查看的页面</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"%E8%8E%B7%E5%8F%96-scrapy-%E6%A1%86%E6%9E%B6-cookies\">获取 Scrapy 框架 Cookies <a class=\"heading-anchor-permalink\" href=\"#%E8%8E%B7%E5%8F%96-scrapy-%E6%A1%86%E6%9E%B6-cookies\">#</a></h4>\n<p>样例代码</p>\n<p><code>start_requests()</code>方法，可以返回一个请求给爬虫的起始网站，这个返回的请求相当于 <code>start_urls</code>, <code>start_requests()</code> 返回的请求会替代 <code>start_urls</code> 里的请求</p>\n<p>在发送请求时 cookie 的操作</p>\n<p><code>meta={'cookiejar':1}</code>表示开启 cookie 记录，首次请求时写在 <code>Request()</code> 里</p>\n<p><code>meta={'cookiejar':response.meta['cookiejar']}</code>表示使用上一次 Response 的 cookie，写在 <code>Request</code> 里 POST 授权</p>\n<pre><code class=\"language-python\">import scrapy\nfrom scrapy import Request\nfrom scrapy import FormRequest\n\n\nclass SxtSpiderSpider(scrapy.Spider):\n    name = 'sxt1'\n    allowed_domains = ['sxt.cn']\n\n    def start_requests(self):\n        return [Request('http://www.sxt.cn/index/login/login.html', meta={'cookiejar': 1}, callback=self.parse)]\n\n    def parse(self, response):\n        formdata = {\n            &quot;user&quot;: &quot;17703181473&quot;, &quot;password&quot;: &quot;123456&quot;\n        }\n        return FormRequest(                                        formdata=formdata,\n                                        url='http://www.sxt.cn/index/login/login.html',\n                                        meta={'cookiejar': response.meta['cookiejar']},\n                                        callback=self.login_after)\n\n    def login_after(self, response):\n        yield scrapy.Request('http://www.sxt.cn/index/user.html',\n                             meta={&quot;cookiejar&quot;: response.meta['cookiejar']},\n                             callback=self.next)\n    def next(self,response):\n        print(response.text)\n</code></pre>\n",
      "id": 4
    },
    {
      "path": "Scrapy框架高级与动态页面的爬取/Scrapy 框架 - 爬取JS生成的动态页面.md",
      "url": "Scrapy框架高级与动态页面的爬取/Scrapy 框架 - 爬取JS生成的动态页面.html",
      "content": "### 问题\n\n有的页面的很多部分都是用 JS 生成的，而对于用 scrapy 爬虫来说就是一个很大的问题，因为 scrapy 没有 JS engine，所以爬取的都是静态页面，对于 JS 生成的动态页面都无法获得\n\n[官网](http://splash.readthedocs.io/en/stable/)\n\n### 解决方案\n\n- 利用第三方中间件来提供 JS 渲染服务： `scrapy-splash` 等\n- 利用 `webkit` 或者基于 `webkit` 库\n\n> Splash 是一个 Javascript 渲染服务。它是一个实现了 HTTP API 的轻量级浏览器，Splash 是用 Python 实现的，同时使用 `Twisted` 和 `QT`。`Twisted` 和 `QT` 用来让服务具有异步处理能力，以发挥 `webkit` 的并发能力\n\n### 安装\n\n1. pip 安装 `scrapy-splash` 库\n\n   ```sh\n   pip install scrapy-splash\n   ```\n\n2. `scrapy-splash` 使用的是 Splash HTTP API， 所以需要一个 `splash instance` ，一般采用 `docker` 运行 splash，所以需要安装 `docker`\n\n3. 安装并运行 `docker`\n\n4. 拉取镜像\n\n   ```sh\n   docker pull scrapinghub/splash\n   ```\n\n5. 用 `docker` 运行 `scrapinghub/splash`\n\n   ```sh\n   docker run -p 8050:8050 scrapinghub/splash\n   ```\n\n6. 配置 splash 服务（以下操作全部在 settings.py）:\n\n   1. 使用 splash 解析，要在配置文件中设置 splash 服务器地址：\n\n      ```python\n      SPLASH_URL = 'http://localhost:8050/'\n      ```\n\n   2. 将 splash middleware 添加到 `DOWNLOADER_MIDDLEWARE` 中\n\n      ```python\n      DOWNLOADER_MIDDLEWARES = {\n          'scrapy_splash.SplashCookiesMiddleware': 723,\n          'scrapy_splash.SplashMiddleware': 725,\n          'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,\n      }\n      ```\n\n   3. Enable `SplashDeduplicateArgsMiddleware`\n\n      ```python\n      SPIDER_MIDDLEWARES = {\n          'scrapy_splash.SplashDeduplicateArgsMiddleware': 100\n      }\n      ```\n\n      这个中间件需要支持 `cache_args` 功能; 它允许通过不在磁盘请求队列中多次存储重复的 Splash 参数来节省磁盘空间。如果使用 Splash 2.1+，则中间件也可以通过不将这些重复的参数多次发送到 Splash 服务器来节省网络流量\n\n   4. 配置消息队列所使用的过滤类\n\n      ```python\n      DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'\n      ```\n\n   5. 配置消息队列需要使用的类\n\n      ```python\n      HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'\n      ```\n\n### 样例\n\n```python\nimport scrapy\nfrom scrapy_splash import SplashRequest\n\n\nclass DoubanSpider(scrapy.Spider):\n    name = 'douban'\n\n    allowed_domains = ['douban.com']\n\n\ndef start_requests(self):\n    yield SplashRequest('https://movie.douban.com/typerank?type_name=剧情&type=11&interval_id=100:90', args={'wait': 0.5})\n\n\ndef parse(self, response):\n    print(response.text)\n\n```\n",
      "html": "<h3 id=\"%E9%97%AE%E9%A2%98\">问题 <a class=\"heading-anchor-permalink\" href=\"#%E9%97%AE%E9%A2%98\">#</a></h3>\n<p>有的页面的很多部分都是用 JS 生成的，而对于用 scrapy 爬虫来说就是一个很大的问题，因为 scrapy 没有 JS engine，所以爬取的都是静态页面，对于 JS 生成的动态页面都无法获得</p>\n<p><a href=\"http://splash.readthedocs.io/en/stable/\">官网</a></p>\n<h3 id=\"%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88\">解决方案 <a class=\"heading-anchor-permalink\" href=\"#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88\">#</a></h3>\n<ul>\n<li>利用第三方中间件来提供 JS 渲染服务： <code>scrapy-splash</code> 等</li>\n<li>利用 <code>webkit</code> 或者基于 <code>webkit</code> 库</li>\n</ul>\n<blockquote>\n<p>Splash 是一个 Javascript 渲染服务。它是一个实现了 HTTP API 的轻量级浏览器，Splash 是用 Python 实现的，同时使用 <code>Twisted</code> 和 <code>QT</code>。<code>Twisted</code> 和 <code>QT</code> 用来让服务具有异步处理能力，以发挥 <code>webkit</code> 的并发能力</p>\n</blockquote>\n<h3 id=\"%E5%AE%89%E8%A3%85\">安装 <a class=\"heading-anchor-permalink\" href=\"#%E5%AE%89%E8%A3%85\">#</a></h3>\n<ol>\n<li>\n<p>pip 安装 <code>scrapy-splash</code> 库</p>\n<pre><code class=\"language-sh\">pip install scrapy-splash\n</code></pre>\n</li>\n<li>\n<p><code>scrapy-splash</code> 使用的是 Splash HTTP API， 所以需要一个 <code>splash instance</code> ，一般采用 <code>docker</code> 运行 splash，所以需要安装 <code>docker</code></p>\n</li>\n<li>\n<p>安装并运行 <code>docker</code></p>\n</li>\n<li>\n<p>拉取镜像</p>\n<pre><code class=\"language-sh\">docker pull scrapinghub/splash\n</code></pre>\n</li>\n<li>\n<p>用 <code>docker</code> 运行 <code>scrapinghub/splash</code></p>\n<pre><code class=\"language-sh\">docker run -p 8050:8050 scrapinghub/splash\n</code></pre>\n</li>\n<li>\n<p>配置 splash 服务（以下操作全部在 <a href=\"http://settings.py\">settings.py</a>）:</p>\n<ol>\n<li>\n<p>使用 splash 解析，要在配置文件中设置 splash 服务器地址：</p>\n<pre><code class=\"language-python\">SPLASH_URL = 'http://localhost:8050/'\n</code></pre>\n</li>\n<li>\n<p>将 splash middleware 添加到 <code>DOWNLOADER_MIDDLEWARE</code> 中</p>\n<pre><code class=\"language-python\">DOWNLOADER_MIDDLEWARES = {\n    'scrapy_splash.SplashCookiesMiddleware': 723,\n    'scrapy_splash.SplashMiddleware': 725,\n    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,\n}\n</code></pre>\n</li>\n<li>\n<p>Enable <code>SplashDeduplicateArgsMiddleware</code></p>\n<pre><code class=\"language-python\">SPIDER_MIDDLEWARES = {\n    'scrapy_splash.SplashDeduplicateArgsMiddleware': 100\n}\n</code></pre>\n<p>这个中间件需要支持 <code>cache_args</code> 功能; 它允许通过不在磁盘请求队列中多次存储重复的 Splash 参数来节省磁盘空间。如果使用 Splash 2.1+，则中间件也可以通过不将这些重复的参数多次发送到 Splash 服务器来节省网络流量</p>\n</li>\n<li>\n<p>配置消息队列所使用的过滤类</p>\n<pre><code class=\"language-python\">DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'\n</code></pre>\n</li>\n<li>\n<p>配置消息队列需要使用的类</p>\n<pre><code class=\"language-python\">HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'\n</code></pre>\n</li>\n</ol>\n</li>\n</ol>\n<h3 id=\"%E6%A0%B7%E4%BE%8B\">样例 <a class=\"heading-anchor-permalink\" href=\"#%E6%A0%B7%E4%BE%8B\">#</a></h3>\n<pre><code class=\"language-python\">import scrapy\nfrom scrapy_splash import SplashRequest\n\n\nclass DoubanSpider(scrapy.Spider):\n    name = 'douban'\n\n    allowed_domains = ['douban.com']\n\n\ndef start_requests(self):\n    yield SplashRequest('https://movie.douban.com/typerank?type_name=剧情&amp;type=11&amp;interval_id=100:90', args={'wait': 0.5})\n\n\ndef parse(self, response):\n    print(response.text)\n\n</code></pre>\n",
      "id": 5
    },
    {
      "path": "Scrapy框架高级与动态页面的爬取/Splash 的使用.md",
      "url": "Scrapy框架高级与动态页面的爬取/Splash 的使用.html",
      "content": "### 1. Splash 介绍\n\n> Splash 是一个 JavaScript 渲染服务，是一个带有 HTTP API 的轻量级浏览器，同时它对接了 Python 中的 `Twisted` 和 `QT` 库。利用它，我们同样可以实现动态渲染页面的抓取\n\n### 2. 安装\n\n#### 2.1 安装 docker\n\n#### 2.2 拉取镜像\n\n```sh\ndocker pull scrapinghub/splash\n```\n\n#### 2.3 用 docker 运行 scrapinghub/splash\n\n```sh\ndocker run -p 8050:8050 scrapinghub/splash\n```\n\n#### 2.4 查看效果\n\n> 我们在 8050 端口上运行了 Splash 服务，打开`http://localhost:8050/`即可看到其 Web 页面\n> ![image](https://note.youdao.com/yws/api/personal/file/366AEA0862FF4B77B584F99F058FD0FE?method=download&shareKey=1becb4e3fd74346d3e247a6cf7d8406d)\n\n### 3 Splash 对象属性\n\n> 上图中 `main()` 方法的第一个参数是 `splash`，这个对象非常重要，它类似于 `Selenium` 中的 `WebDriver` 对象\n\n#### 3.1 images_enabled\n\n> 设置图片是否加载，默认情况下是加载的。禁用该属性后，可以节省网络流量并提高网页加载速度\n> 注意的是，禁用图片加载可能会影响 JavaScript 渲染。因为禁用图片之后，它的外层 DOM 节点的高度会受影响，进而影响 DOM 节点的位置\n> 因此，如果 JavaScript 对图片节点有操作的话，其执行就会受到影响\n\n```javascript\nfunction main(splash, args)\n  splash.images_enabled = false\n  splash:go('https://www.baidu.com')\n  return {html=splash:html()}\nend\n```\n\n#### 3.2 plugins_enabled\n\n> 可以控制浏览器插件（如 Flash 插件）是否开启\n> 默认情况下，此属性是 `false` ，表示不开启\n\n```javascript\nsplash.plugins_enabled = true / false;\n```\n\n#### 3.3 scroll_position\n\n> 控制页面上下或左右滚动\n\n```javascript\nsplash.scroll_position = {x=100, y=200}\n```\n\n### 4. Splash 对象的方法\n\n#### 4.1 go()\n\n> 该方法用来请求某个链接，而且它可以模拟 `GET` 和 `POST` 请求，同时支持传入请求头、表单等数据\n\n```javascript\nok, reason = splash:go{url, baseurl=nil, headers=nil, http_method=\"GET\", body=nil, formdata=nil}\n```\n\n> 返回结果是结果 ok 和原因 reason\n> 如果 ok 为空，代表网页加载出现了错误，此时 reason 变量中包含了错误的原因\n\n| 参数        | 含义                                                                                                      |\n| ----------- | --------------------------------------------------------------------------------------------------------- |\n| url         | 请求的 URL                                                                                                |\n| baseurl     | 可选参数，默认为空，表示资源加载相对路径                                                                  |\n| headers     | 可选参数，默认为空，表示请求头                                                                            |\n| http_method | 可选参数，默认为 `GET` ，同时支持 `POST`                                                                  |\n| body        | 可选参数，默认为空，发 `POST` 请求时的表单数据，使用的 `Content-type` 为 `application/json`               |\n| formdata    | 可选参数，默认为空，`POST` 的时候的表单数据，使用的 `Content-type` 为 `application/x-www-form-urlencoded` |\n\n```javascript\nsplash:go{\"http://www.sxt.cn\", http_method=\"POST\", body=\"name=17703181473\"}\n```\n\n#### 4.2 wait()\n\n> 控制页面的等待时间\n\n```javascript\nsplash:wait{time, cancel_on_redirect=false, cancel_on_error=true}\n```\n\n| 参数               | 含义                                                                        |\n| ------------------ | --------------------------------------------------------------------------- |\n| time               | 等待的秒数                                                                  |\n| cancel_on_redirect | 可选参数，默认为 `false` ，表示如果发生了重定向就停止等待，并返回重定向结果 |\n| cancel_on_error    | 可选参数，默认为 `false` ，表示如果发生了加载错误，就停止等待               |\n\n```javascript\nfunction main(splash)\n    splash:go(\"https://www.taobao.com\")\n    splash:wait(2)\n    return {html=splash:html()}\nend\n```\n\n#### 4.3 jsfunc()\n\n> 直接调用 JavaScript 定义的方法，但是所调用的方法需要用双中括号包围，这相当于实现了 JavaScript 方法到 Lua 脚本的转换\n\n```javascript\nfunction main(splash, args)\n  splash:go(\"http://www.sxt.cn\")\n  local scroll_to = splash:jsfunc(\"window.scrollTo\")\n  scroll_to(0, 300)\n  return {png=splash:png()}\nend\n```\n\n#### 4.4 evaljs()与 runjs()\n\n- `evaljs()` 以执行 JavaScript 代码并返回最后一条 JavaScript 语句的返回结果\n- `runjs()` 以执行 JavaScript 代码，它与 `evaljs()` 的功能类似，但是更偏向于执行某些动作或声明某些方法\n\n```javascript\nfunction main(splash, args)\n  splash:go(\"https://www.baidu.com\")\n  splash:runjs(\"foo = function() { return 'sxt' }\")\n  local result = splash:evaljs(\"foo()\")\n  return result\nend\n```\n\n#### 4.5 html()\n\n> 获取网页的源代码\n\n```javascript\nfunction main(splash, args)\n  splash:go(\"https://www.bjsxt.com\")\n  return splash:html()\nend\n```\n\n#### 4.6 png()\n\n> 获取 PNG 格式的网页截图\n\n```javascript\nfunction main(splash, args)\n  splash:go(\"https://www.bjsxt.com\")\n  return splash:png()\nend\n```\n\n#### 4.7 har()\n\n> 获取页面加载过程描述\n\n```javascript\nfunction main(splash, args)\n  splash:go(\"https://www.bjsxt.com\")\n  return splash:har()\nend\n```\n\n#### 4.8 url()\n\n> 获取当前正在访问的 URL\n\n```javascript\nfunction main(splash, args)\n  splash:go(\"https://www.bjsxt.com\")\n  return splash:url()\nend\n```\n\n#### 4.9 get_cookies()\n\n> 获取当前页面的 `Cookies`\n\n```javascript\nfunction main(splash, args)\n  splash:go(\"https://www.bjsxt.com\")\n  return splash:get_cookies()\nend\n```\n\n#### 4.10 add_cookie()\n\n> 当前页面添加 `Cookies`\n\n```javascript\ncookies = splash:add_cookie{name, value, path=nil, domain=nil, expires=nil, httpOnly=nil, secure=nil}\n```\n\n```javascript\nfunction main(splash)\n    splash:add_cookie{\"sessionid\", \"123456abcdef\", \"/\", domain=\"http://bjsxt.com\"}\n    splash:go(\"http://bjsxt.com/\")\n    return splash:html()\nend\n```\n\n#### 4.11 clear_cookies()\n\n> 可以清除所有的 `Cookies`\n\n```javascript\nfunction main(splash)\n    splash:go(\"https://www.bjsxt.com/\")\n    splash:clear_cookies()\n    return splash:get_cookies()\nend\n```\n\n#### 4.12 set_user_agent()\n\n> 设置浏览器的 `User-Agent`\n\n```javascript\nfunction main(splash)\n  splash:set_user_agent('Splash')\n  splash:go(\"http://httpbin.org/get\")\n  return splash:html()\nend\n```\n\n#### 4.13 set_custom_headers()\n\n> 设置请求头\n\n```javascript\nfunction main(splash)\n  splash:set_custom_headers({\n     [\"User-Agent\"] = \"Splash\",\n     [\"Site\"] = \"Splash\",\n  })\n  splash:go(\"http://httpbin.org/get\")\n  return splash:html()\nend\n```\n\n#### 4.14 select()\n\n> 选中符合条件的第一个节点\n> 如果有多个节点符合条件，则只会返回一个\n> 其参数是 CSS 选择器\n\n```javascript\nfunction main(splash)\n  splash:go(\"https://www.baidu.com/\")\n  input = splash:select(\"#kw\")\n  splash:wait(3)\n  return splash:png()\nend\n```\n\n#### 4.15 send_text()\n\n> 填写文本\n\n```javascript\nfunction main(splash)\n  splash:go(\"https://www.baidu.com/\")\n  input = splash:select(\"#kw\")\n  input:send_text('Splash')\n  splash:wait(3)\n  return splash:png()\nend\n```\n\n#### 4.16 mouse_click()\n\n> 模拟鼠标点击操作\n\n```javascript\nfunction main(splash)\n  splash:go(\"https://www.baidu.com/\")\n  input = splash:select(\"#kw\")\n  input:send_text('Splash')\n  submit = splash:select('#su')\n  submit:mouse_click()\n  splash:wait(3)\n  return splash:png()\nend\n```\n\n#### 4.17 代理 Ip\n\n```javascript\nfunction main(splash)\n    splash:on_request(function(request)\n        request:set_proxy{\n            'host':'61.138.33.20',\n            'port':808,\n            'username':'uanme',\n            'password':'passwrod'\n        }\n\n     end)\n\n    -- 设置请求头\n    splash:set_user_agent(\"Mozilla/5.0\")\n\n    splash:go(\"https://httpbin.org/get\")\n    return splash:html()\nend\n```\n\n### 5 Splash 与 Python 结合\n\n#### 5.1 render.html\n\n> 此接口用于获取 JavaScript 渲染的页面的 HTML 代码，接口地址就是 Splash 的运行地址加此接口名称，例如`http://localhost:8050/render.html`\n\n```python\nimport requests\nurl = 'http://localhost:8050/render.html?url=https://www.bjsxt.com&wait=3'\nresponse = requests.get(url)\nprint(response.text)\n```\n\n#### 5.2 render.png\n\n> 此接口可以获取网页截图\n\n```python\nimport requests\n\nurl = 'http://localhost:8050/render.png?url=https://www.jd.com&wait=5&width=1000&height=700'\nresponse = requests.get(url)\nwith open('taobao.png', 'wb') as f:\n    f.write(response.content)\n```\n\n#### 5.3 execute\n\n> 最为强大的接口。前面说了很多 Splash Lua 脚本的操作，用此接口便可实现与 Lua 脚本的对接\n\n```python\nimport requests\nfrom urllib.parse import quote\n\nlua = '''\nfunction main(splash)\n    return 'hello'\nend\n'''\n\nurl = 'http://localhost:8050/execute?lua_source=' + quote(lua)\nresponse = requests.get(url)\nprint(response.text)\n```\n",
      "html": "<h3 id=\"1.-splash-%E4%BB%8B%E7%BB%8D\">1. Splash 介绍 <a class=\"heading-anchor-permalink\" href=\"#1.-splash-%E4%BB%8B%E7%BB%8D\">#</a></h3>\n<blockquote>\n<p>Splash 是一个 JavaScript 渲染服务，是一个带有 HTTP API 的轻量级浏览器，同时它对接了 Python 中的 <code>Twisted</code> 和 <code>QT</code> 库。利用它，我们同样可以实现动态渲染页面的抓取</p>\n</blockquote>\n<h3 id=\"2.-%E5%AE%89%E8%A3%85\">2. 安装 <a class=\"heading-anchor-permalink\" href=\"#2.-%E5%AE%89%E8%A3%85\">#</a></h3>\n<h4 id=\"2.1-%E5%AE%89%E8%A3%85-docker\">2.1 安装 docker <a class=\"heading-anchor-permalink\" href=\"#2.1-%E5%AE%89%E8%A3%85-docker\">#</a></h4>\n<h4 id=\"2.2-%E6%8B%89%E5%8F%96%E9%95%9C%E5%83%8F\">2.2 拉取镜像 <a class=\"heading-anchor-permalink\" href=\"#2.2-%E6%8B%89%E5%8F%96%E9%95%9C%E5%83%8F\">#</a></h4>\n<pre><code class=\"language-sh\">docker pull scrapinghub/splash\n</code></pre>\n<h4 id=\"2.3-%E7%94%A8-docker-%E8%BF%90%E8%A1%8C-scrapinghub%2Fsplash\">2.3 用 docker 运行 scrapinghub/splash <a class=\"heading-anchor-permalink\" href=\"#2.3-%E7%94%A8-docker-%E8%BF%90%E8%A1%8C-scrapinghub%2Fsplash\">#</a></h4>\n<pre><code class=\"language-sh\">docker run -p 8050:8050 scrapinghub/splash\n</code></pre>\n<h4 id=\"2.4-%E6%9F%A5%E7%9C%8B%E6%95%88%E6%9E%9C\">2.4 查看效果 <a class=\"heading-anchor-permalink\" href=\"#2.4-%E6%9F%A5%E7%9C%8B%E6%95%88%E6%9E%9C\">#</a></h4>\n<blockquote>\n<p>我们在 8050 端口上运行了 Splash 服务，打开<code>http://localhost:8050/</code>即可看到其 Web 页面\n<img src=\"https://note.youdao.com/yws/api/personal/file/366AEA0862FF4B77B584F99F058FD0FE?method=download&amp;shareKey=1becb4e3fd74346d3e247a6cf7d8406d\" alt=\"image\"></p>\n</blockquote>\n<h3 id=\"3-splash-%E5%AF%B9%E8%B1%A1%E5%B1%9E%E6%80%A7\">3 Splash 对象属性 <a class=\"heading-anchor-permalink\" href=\"#3-splash-%E5%AF%B9%E8%B1%A1%E5%B1%9E%E6%80%A7\">#</a></h3>\n<blockquote>\n<p>上图中 <code>main()</code> 方法的第一个参数是 <code>splash</code>，这个对象非常重要，它类似于 <code>Selenium</code> 中的 <code>WebDriver</code> 对象</p>\n</blockquote>\n<h4 id=\"3.1-images_enabled\">3.1 images_enabled <a class=\"heading-anchor-permalink\" href=\"#3.1-images_enabled\">#</a></h4>\n<blockquote>\n<p>设置图片是否加载，默认情况下是加载的。禁用该属性后，可以节省网络流量并提高网页加载速度\n注意的是，禁用图片加载可能会影响 JavaScript 渲染。因为禁用图片之后，它的外层 DOM 节点的高度会受影响，进而影响 DOM 节点的位置\n因此，如果 JavaScript 对图片节点有操作的话，其执行就会受到影响</p>\n</blockquote>\n<pre><code class=\"language-javascript\">function main(splash, args)\n  splash.images_enabled = false\n  splash:go('https://www.baidu.com')\n  return {html=splash:html()}\nend\n</code></pre>\n<h4 id=\"3.2-plugins_enabled\">3.2 plugins_enabled <a class=\"heading-anchor-permalink\" href=\"#3.2-plugins_enabled\">#</a></h4>\n<blockquote>\n<p>可以控制浏览器插件（如 Flash 插件）是否开启\n默认情况下，此属性是 <code>false</code> ，表示不开启</p>\n</blockquote>\n<pre><code class=\"language-javascript\">splash.plugins_enabled = true / false;\n</code></pre>\n<h4 id=\"3.3-scroll_position\">3.3 scroll_position <a class=\"heading-anchor-permalink\" href=\"#3.3-scroll_position\">#</a></h4>\n<blockquote>\n<p>控制页面上下或左右滚动</p>\n</blockquote>\n<pre><code class=\"language-javascript\">splash.scroll_position = {x=100, y=200}\n</code></pre>\n<h3 id=\"4.-splash-%E5%AF%B9%E8%B1%A1%E7%9A%84%E6%96%B9%E6%B3%95\">4. Splash 对象的方法 <a class=\"heading-anchor-permalink\" href=\"#4.-splash-%E5%AF%B9%E8%B1%A1%E7%9A%84%E6%96%B9%E6%B3%95\">#</a></h3>\n<h4 id=\"4.1-go()\">4.1 go() <a class=\"heading-anchor-permalink\" href=\"#4.1-go()\">#</a></h4>\n<blockquote>\n<p>该方法用来请求某个链接，而且它可以模拟 <code>GET</code> 和 <code>POST</code> 请求，同时支持传入请求头、表单等数据</p>\n</blockquote>\n<pre><code class=\"language-javascript\">ok, reason = splash:go{url, baseurl=nil, headers=nil, http_method=&quot;GET&quot;, body=nil, formdata=nil}\n</code></pre>\n<blockquote>\n<p>返回结果是结果 ok 和原因 reason\n如果 ok 为空，代表网页加载出现了错误，此时 reason 变量中包含了错误的原因</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>含义</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>url</td>\n<td>请求的 URL</td>\n</tr>\n<tr>\n<td>baseurl</td>\n<td>可选参数，默认为空，表示资源加载相对路径</td>\n</tr>\n<tr>\n<td>headers</td>\n<td>可选参数，默认为空，表示请求头</td>\n</tr>\n<tr>\n<td>http_method</td>\n<td>可选参数，默认为 <code>GET</code> ，同时支持 <code>POST</code></td>\n</tr>\n<tr>\n<td>body</td>\n<td>可选参数，默认为空，发 <code>POST</code> 请求时的表单数据，使用的 <code>Content-type</code> 为 <code>application/json</code></td>\n</tr>\n<tr>\n<td>formdata</td>\n<td>可选参数，默认为空，<code>POST</code> 的时候的表单数据，使用的 <code>Content-type</code> 为 <code>application/x-www-form-urlencoded</code></td>\n</tr>\n</tbody>\n</table>\n<pre><code class=\"language-javascript\">splash:go{&quot;http://www.sxt.cn&quot;, http_method=&quot;POST&quot;, body=&quot;name=17703181473&quot;}\n</code></pre>\n<h4 id=\"4.2-wait()\">4.2 wait() <a class=\"heading-anchor-permalink\" href=\"#4.2-wait()\">#</a></h4>\n<blockquote>\n<p>控制页面的等待时间</p>\n</blockquote>\n<pre><code class=\"language-javascript\">splash:wait{time, cancel_on_redirect=false, cancel_on_error=true}\n</code></pre>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>含义</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>time</td>\n<td>等待的秒数</td>\n</tr>\n<tr>\n<td>cancel_on_redirect</td>\n<td>可选参数，默认为 <code>false</code> ，表示如果发生了重定向就停止等待，并返回重定向结果</td>\n</tr>\n<tr>\n<td>cancel_on_error</td>\n<td>可选参数，默认为 <code>false</code> ，表示如果发生了加载错误，就停止等待</td>\n</tr>\n</tbody>\n</table>\n<pre><code class=\"language-javascript\">function main(splash)\n    splash:go(&quot;https://www.taobao.com&quot;)\n    splash:wait(2)\n    return {html=splash:html()}\nend\n</code></pre>\n<h4 id=\"4.3-jsfunc()\">4.3 jsfunc() <a class=\"heading-anchor-permalink\" href=\"#4.3-jsfunc()\">#</a></h4>\n<blockquote>\n<p>直接调用 JavaScript 定义的方法，但是所调用的方法需要用双中括号包围，这相当于实现了 JavaScript 方法到 Lua 脚本的转换</p>\n</blockquote>\n<pre><code class=\"language-javascript\">function main(splash, args)\n  splash:go(&quot;http://www.sxt.cn&quot;)\n  local scroll_to = splash:jsfunc(&quot;window.scrollTo&quot;)\n  scroll_to(0, 300)\n  return {png=splash:png()}\nend\n</code></pre>\n<h4 id=\"4.4-evaljs()%E4%B8%8E-runjs()\">4.4 evaljs()与 runjs() <a class=\"heading-anchor-permalink\" href=\"#4.4-evaljs()%E4%B8%8E-runjs()\">#</a></h4>\n<ul>\n<li><code>evaljs()</code> 以执行 JavaScript 代码并返回最后一条 JavaScript 语句的返回结果</li>\n<li><code>runjs()</code> 以执行 JavaScript 代码，它与 <code>evaljs()</code> 的功能类似，但是更偏向于执行某些动作或声明某些方法</li>\n</ul>\n<pre><code class=\"language-javascript\">function main(splash, args)\n  splash:go(&quot;https://www.baidu.com&quot;)\n  splash:runjs(&quot;foo = function() { return 'sxt' }&quot;)\n  local result = splash:evaljs(&quot;foo()&quot;)\n  return result\nend\n</code></pre>\n<h4 id=\"4.5-html()\">4.5 html() <a class=\"heading-anchor-permalink\" href=\"#4.5-html()\">#</a></h4>\n<blockquote>\n<p>获取网页的源代码</p>\n</blockquote>\n<pre><code class=\"language-javascript\">function main(splash, args)\n  splash:go(&quot;https://www.bjsxt.com&quot;)\n  return splash:html()\nend\n</code></pre>\n<h4 id=\"4.6-png()\">4.6 png() <a class=\"heading-anchor-permalink\" href=\"#4.6-png()\">#</a></h4>\n<blockquote>\n<p>获取 PNG 格式的网页截图</p>\n</blockquote>\n<pre><code class=\"language-javascript\">function main(splash, args)\n  splash:go(&quot;https://www.bjsxt.com&quot;)\n  return splash:png()\nend\n</code></pre>\n<h4 id=\"4.7-har()\">4.7 har() <a class=\"heading-anchor-permalink\" href=\"#4.7-har()\">#</a></h4>\n<blockquote>\n<p>获取页面加载过程描述</p>\n</blockquote>\n<pre><code class=\"language-javascript\">function main(splash, args)\n  splash:go(&quot;https://www.bjsxt.com&quot;)\n  return splash:har()\nend\n</code></pre>\n<h4 id=\"4.8-url()\">4.8 url() <a class=\"heading-anchor-permalink\" href=\"#4.8-url()\">#</a></h4>\n<blockquote>\n<p>获取当前正在访问的 URL</p>\n</blockquote>\n<pre><code class=\"language-javascript\">function main(splash, args)\n  splash:go(&quot;https://www.bjsxt.com&quot;)\n  return splash:url()\nend\n</code></pre>\n<h4 id=\"4.9-get_cookies()\">4.9 get_cookies() <a class=\"heading-anchor-permalink\" href=\"#4.9-get_cookies()\">#</a></h4>\n<blockquote>\n<p>获取当前页面的 <code>Cookies</code></p>\n</blockquote>\n<pre><code class=\"language-javascript\">function main(splash, args)\n  splash:go(&quot;https://www.bjsxt.com&quot;)\n  return splash:get_cookies()\nend\n</code></pre>\n<h4 id=\"4.10-add_cookie()\">4.10 add_cookie() <a class=\"heading-anchor-permalink\" href=\"#4.10-add_cookie()\">#</a></h4>\n<blockquote>\n<p>当前页面添加 <code>Cookies</code></p>\n</blockquote>\n<pre><code class=\"language-javascript\">cookies = splash:add_cookie{name, value, path=nil, domain=nil, expires=nil, httpOnly=nil, secure=nil}\n</code></pre>\n<pre><code class=\"language-javascript\">function main(splash)\n    splash:add_cookie{&quot;sessionid&quot;, &quot;123456abcdef&quot;, &quot;/&quot;, domain=&quot;http://bjsxt.com&quot;}\n    splash:go(&quot;http://bjsxt.com/&quot;)\n    return splash:html()\nend\n</code></pre>\n<h4 id=\"4.11-clear_cookies()\">4.11 clear_cookies() <a class=\"heading-anchor-permalink\" href=\"#4.11-clear_cookies()\">#</a></h4>\n<blockquote>\n<p>可以清除所有的 <code>Cookies</code></p>\n</blockquote>\n<pre><code class=\"language-javascript\">function main(splash)\n    splash:go(&quot;https://www.bjsxt.com/&quot;)\n    splash:clear_cookies()\n    return splash:get_cookies()\nend\n</code></pre>\n<h4 id=\"4.12-set_user_agent()\">4.12 set_user_agent() <a class=\"heading-anchor-permalink\" href=\"#4.12-set_user_agent()\">#</a></h4>\n<blockquote>\n<p>设置浏览器的 <code>User-Agent</code></p>\n</blockquote>\n<pre><code class=\"language-javascript\">function main(splash)\n  splash:set_user_agent('Splash')\n  splash:go(&quot;http://httpbin.org/get&quot;)\n  return splash:html()\nend\n</code></pre>\n<h4 id=\"4.13-set_custom_headers()\">4.13 set_custom_headers() <a class=\"heading-anchor-permalink\" href=\"#4.13-set_custom_headers()\">#</a></h4>\n<blockquote>\n<p>设置请求头</p>\n</blockquote>\n<pre><code class=\"language-javascript\">function main(splash)\n  splash:set_custom_headers({\n     [&quot;User-Agent&quot;] = &quot;Splash&quot;,\n     [&quot;Site&quot;] = &quot;Splash&quot;,\n  })\n  splash:go(&quot;http://httpbin.org/get&quot;)\n  return splash:html()\nend\n</code></pre>\n<h4 id=\"4.14-select()\">4.14 select() <a class=\"heading-anchor-permalink\" href=\"#4.14-select()\">#</a></h4>\n<blockquote>\n<p>选中符合条件的第一个节点\n如果有多个节点符合条件，则只会返回一个\n其参数是 CSS 选择器</p>\n</blockquote>\n<pre><code class=\"language-javascript\">function main(splash)\n  splash:go(&quot;https://www.baidu.com/&quot;)\n  input = splash:select(&quot;#kw&quot;)\n  splash:wait(3)\n  return splash:png()\nend\n</code></pre>\n<h4 id=\"4.15-send_text()\">4.15 send_text() <a class=\"heading-anchor-permalink\" href=\"#4.15-send_text()\">#</a></h4>\n<blockquote>\n<p>填写文本</p>\n</blockquote>\n<pre><code class=\"language-javascript\">function main(splash)\n  splash:go(&quot;https://www.baidu.com/&quot;)\n  input = splash:select(&quot;#kw&quot;)\n  input:send_text('Splash')\n  splash:wait(3)\n  return splash:png()\nend\n</code></pre>\n<h4 id=\"4.16-mouse_click()\">4.16 mouse_click() <a class=\"heading-anchor-permalink\" href=\"#4.16-mouse_click()\">#</a></h4>\n<blockquote>\n<p>模拟鼠标点击操作</p>\n</blockquote>\n<pre><code class=\"language-javascript\">function main(splash)\n  splash:go(&quot;https://www.baidu.com/&quot;)\n  input = splash:select(&quot;#kw&quot;)\n  input:send_text('Splash')\n  submit = splash:select('#su')\n  submit:mouse_click()\n  splash:wait(3)\n  return splash:png()\nend\n</code></pre>\n<h4 id=\"4.17-%E4%BB%A3%E7%90%86-ip\">4.17 代理 Ip <a class=\"heading-anchor-permalink\" href=\"#4.17-%E4%BB%A3%E7%90%86-ip\">#</a></h4>\n<pre><code class=\"language-javascript\">function main(splash)\n    splash:on_request(function(request)\n        request:set_proxy{\n            'host':'61.138.33.20',\n            'port':808,\n            'username':'uanme',\n            'password':'passwrod'\n        }\n\n     end)\n\n    -- 设置请求头\n    splash:set_user_agent(&quot;Mozilla/5.0&quot;)\n\n    splash:go(&quot;https://httpbin.org/get&quot;)\n    return splash:html()\nend\n</code></pre>\n<h3 id=\"5-splash-%E4%B8%8E-python-%E7%BB%93%E5%90%88\">5 Splash 与 Python 结合 <a class=\"heading-anchor-permalink\" href=\"#5-splash-%E4%B8%8E-python-%E7%BB%93%E5%90%88\">#</a></h3>\n<h4 id=\"5.1-render.html\">5.1 render.html <a class=\"heading-anchor-permalink\" href=\"#5.1-render.html\">#</a></h4>\n<blockquote>\n<p>此接口用于获取 JavaScript 渲染的页面的 HTML 代码，接口地址就是 Splash 的运行地址加此接口名称，例如<code>http://localhost:8050/render.html</code></p>\n</blockquote>\n<pre><code class=\"language-python\">import requests\nurl = 'http://localhost:8050/render.html?url=https://www.bjsxt.com&amp;wait=3'\nresponse = requests.get(url)\nprint(response.text)\n</code></pre>\n<h4 id=\"5.2-render.png\">5.2 render.png <a class=\"heading-anchor-permalink\" href=\"#5.2-render.png\">#</a></h4>\n<blockquote>\n<p>此接口可以获取网页截图</p>\n</blockquote>\n<pre><code class=\"language-python\">import requests\n\nurl = 'http://localhost:8050/render.png?url=https://www.jd.com&amp;wait=5&amp;width=1000&amp;height=700'\nresponse = requests.get(url)\nwith open('taobao.png', 'wb') as f:\n    f.write(response.content)\n</code></pre>\n<h4 id=\"5.3-execute\">5.3 execute <a class=\"heading-anchor-permalink\" href=\"#5.3-execute\">#</a></h4>\n<blockquote>\n<p>最为强大的接口。前面说了很多 Splash Lua 脚本的操作，用此接口便可实现与 Lua 脚本的对接</p>\n</blockquote>\n<pre><code class=\"language-python\">import requests\nfrom urllib.parse import quote\n\nlua = '''\nfunction main(splash)\n    return 'hello'\nend\n'''\n\nurl = 'http://localhost:8050/execute?lua_source=' + quote(lua)\nresponse = requests.get(url)\nprint(response.text)\n</code></pre>\n",
      "id": 6
    },
    {
      "path": "数据提取与验证码的识别（上）/Beautiful Soup.md",
      "url": "数据提取与验证码的识别（上）/Beautiful Soup.html",
      "content": "### 1. Beautiful Soup 的简介\n\n> Beautiful Soup 提供一些简单的、python 式的函数用来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。\n> Beautiful Soup 自动将输入文档转换为 Unicode 编码，输出文档转换为 utf-8 编码。你不需要考虑编码方式，除非文档没有指定一个编码方式，这时，Beautiful Soup 就不能自动识别编码方式了。然后，你仅仅需要说明一下原始编码方式就可以了。\n> Beautiful Soup 已成为和 lxml、html6lib 一样出色的 python 解释器，为用户灵活地提供不同的解析策略或强劲的速度\n\n[官网](https://beautifulsoup.readthedocs.io/zh_CN/latest/)\n\n### 2. Beautiful Soup 安装\n\n> Beautiful Soup 3 目前已经停止开发，推荐在现在的项目中使用 Beautiful Soup 4，不过它已经被移植到 BS4 了,也就是说导入时我们需要 import bs4\n\n```sh\npip install beautifulsoup4\n```\n\n> Beautiful Soup 支持 Python 标准库中的 HTML 解析器,还支持一些第三方的解析器，如果我们不安装它，则 Python 会使用 Python 默认的解析器，lxml 解析器更加强大，速度更快，推荐安装\n\n| 解析器           | 使用方法                                                            | 优势                                                                      | 劣势                                            |\n| ---------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------------- | ----------------------------------------------- |\n| Python 标准库    | BeautifulSoup(markup, “html.parser”)                                | 1. Python 的内置标准库 2. 执行速度适中 3.文档容错能力强                   | Python 2.7.3 or 3.2.2)前 的版本中文档容错能力差 |\n| lxml HTML 解析器 | BeautifulSoup(markup, “lxml”)                                       | 1. 速度快 2.文档容错能力强                                                | 需要安装 C 语言库                               |\n| lxml XML 解析器  | BeautifulSoup(markup, [“lxml”, “xml”]) BeautifulSoup(markup, “xml”) | 1. 速度快 2.唯一支持 XML 的解析器 3.需要安装 C 语言库                     |\n| html5lib         | BeautifulSoup(markup, “html5lib”)                                   | 1. 最好的容错性 2.以浏览器的方式解析文档 3.生成 HTML5 格式的文档 4.速度慢 | 不依赖外部扩展                                  |\n\n### 3. 创建 Beautiful Soup 对象\n\n```python\nfrom bs4 import BeautifulSoup\n\nbs = BeautifulSoup(html,\"lxml\")\n```\n\n### 4. 四大对象种类\n\n> Beautiful Soup 将复杂 HTML 文档转换成一个复杂的树形结构,每个节点都是 Python 对象,所有对象可以归纳为 4 种:\n\n- Tag\n- NavigableString\n- BeautifulSoup\n- Comment\n\n#### 4.1 Tag 是什么？通俗点讲就是 HTML 中的一个个标签\n\n例如：`<div>` `<title>`\n\n使用方式：\n\n```html\n#以以下代码为例子\n<title>知乎</title>\n<div class=\"info\" float=\"left\">Welcome to Zhihu</div>\n<div class=\"info\" float=\"right\">\n  <span>Good Good Study</span>\n  <a href=\"www.zhihu.com\"></a>\n  <strong><!--没用--></strong>\n</div>\n```\n\n##### 4.1.1 获取标签\n\n```python\n#以lxml方式解析\nsoup = BeautifulSoup(info, 'lxml')\nprint(soup.title)\n# <title>bilibili</title>\n```\n\n**注意**\n\n> 相同的标签只能获取第一个符合要求的标签\n\n##### 4.1.2 获取属性\n\n```python\n#获取所有属性\nprint(soup.title.attrs)\n#class='info' float='left'\n\n#获取单个属性的值\nprint(soup.div.get('class'))\nprint(soup.div['class'])\nprint(soup.a['href'])\n#info\n```\n\n#### 4.2 NavigableString 获取内容\n\n```python\nprint(soup.title.string)\nprint(soup.title.text)\n#bilibili\n```\n\n#### 4.3 BeautifulSoup\n\n> BeautifulSoup 对象表示的是一个文档的全部内容.大部分时候,可以把它当作 Tag 对象,它支持 遍历文档树 和 搜索文档树 中描述的大部分的方法.\n> 因为 BeautifulSoup 对象并不是真正的 HTML 或 XML 的 tag,所以它没有 `name` 和 `attribute` 属性.但有时查看它的 `.name` 属性是很方便的,所以 BeautifulSoup 对象包含了一个值为 `[document]` 的特殊属性 `.name`\n\n```python\nprint(soup.name)\nprint(soup.head.name)\n# [document]\n# head\n```\n\n#### 4.4 Comment\n\n> Comment 对象是一个特殊类型的 NavigableString 对象，其实输出的内容仍然不包括注释符号，但是如果不好好处理它，可能会对我们的文本处理造成意想不到的麻烦\n\n```python\nif type(soup.strong.string)==Comment:\n    print(soup.strong.prettify())\nelse:\n    print(soup.strong.string)\n```\n\n### 5 搜索文档树\n\n> Beautiful Soup 定义了很多搜索方法,这里着重介绍 2 个: find() 和 find_all() .其它方法的参数和用法类似,请同学们举一反三\n\n#### 5.1 过滤器\n\n> 介绍 find_all() 方法前,先介绍一下过滤器的类型 ,这些过滤器贯穿整个搜索的 API.过滤器可以被用在 tag 的 name 中,节点的属性中,字符串中或他们的混合中\n\n##### 5.1.1 字符串\n\n> 最简单的过滤器是字符串.在搜索方法中传入一个字符串参数,Beautiful Soup 会查找与字符串完整匹配的内容,下面的例子用于查找文档中所有的`<div>`标签\n\n```python\n#返回所有的div标签\nprint(soup.find_all('div'))\n```\n\n> 如果传入字节码参数,Beautiful Soup 会当作 UTF-8 编码,可以传入一段 Unicode 编码来避免 Beautiful Soup 解析编码出错\n\n##### 5.1.2 正则表达式\n\n如果传入正则表达式作为参数,Beautiful Soup 会通过正则表达式的 match() 来匹配内容\n\n```python\n#返回所有的div标签\nprint (soup.find_all(re.compile(\"^div\")))\n```\n\n##### 5.1.3 列表\n\n> 如果传入列表参数,Beautiful Soup 会将与列表中任一元素匹配的内容返回\n\n```python\n#返回所有匹配到的span a标签\nprint(soup.find_all(['span','a']))\n```\n\n##### 5.1.4 keyword\n\n> 如果一个指定名字的参数不是搜索内置的参数名,搜索时会把该参数当作指定名字 tag 的属性来搜索,如果包含一个名字为 id 的参数,Beautiful Soup 会搜索每个 tag 的”id”属性\n\n```python\n#返回id为welcom的标签\nprint(soup.find_all(id='welcom'))\n```\n\n##### 5.1.4 True\n\n> True 可以匹配任何值,下面代码查找到所有的 tag,但是不会返回字符串节点\n\n##### 5.1.5 按 CSS 搜索\n\n> 按照 CSS 类名搜索 tag 的功能非常实用,但标识 CSS 类名的关键字 class 在 Python 中是保留字,使用 class 做参数会导致语法错误.从 Beautiful Soup 的 4.1.1 版本开始,可以通过 class\\_ 参数搜索有指定 CSS 类名的 tag\n\n```python\n# 返回class等于info的div\nprint(soup.find_all('div',class_='info'))\n```\n\n#### 5.1.6 按属性的搜索\n\n```python\nsoup.find_all(\"div\", attrs={\"class\": \"info\"})\n```\n\n### 6. CSS 选择器（扩展）\n\nsoup.select(参数)\n\n| 表达式                      | 说明                                          |\n| --------------------------- | --------------------------------------------- |\n| tag                         | 选择指定标签                                  |\n| \\*                          | 选择所有节点                                  |\n| #id                         | 选择 id 为 container 的节点                   |\n| .class                      | 选取所有 class 包含 container 的节点          |\n| li a                        | 选取所有 li 下的所有 a 节点                   |\n| ul + p                      | (兄弟)选择 ul 后面的第一个 p 元素             |\n| div#id > ul                 | (父子)选取 id 为 id 的 div 的第一个 ul 子元素 |\n| table ~ div                 | 选取与 table 相邻的所有 div 元素              |\n| a[title]                    | 选取所有有 title 属性的 a 元素                |\n| a[class=”title”]            | 选取所有 class 属性为 title 值的 a            |\n| a[href*=”sxt”]              | 选取所有 href 属性包含 sxt 的 a 元素          |\n| a[href^=”http”]             | 选取所有 href 属性值以 http 开头的 a 元素     |\n| a[href$=”.png”]             | 选取所有 href 属性值以.png 结尾的 a 元素      |\n| input[type=\"redio\"]:checked | 选取选中的 hobby 的元素                       |\n",
      "html": "<h3 id=\"1.-beautiful-soup-%E7%9A%84%E7%AE%80%E4%BB%8B\">1. Beautiful Soup 的简介 <a class=\"heading-anchor-permalink\" href=\"#1.-beautiful-soup-%E7%9A%84%E7%AE%80%E4%BB%8B\">#</a></h3>\n<blockquote>\n<p>Beautiful Soup 提供一些简单的、python 式的函数用来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。\nBeautiful Soup 自动将输入文档转换为 Unicode 编码，输出文档转换为 utf-8 编码。你不需要考虑编码方式，除非文档没有指定一个编码方式，这时，Beautiful Soup 就不能自动识别编码方式了。然后，你仅仅需要说明一下原始编码方式就可以了。\nBeautiful Soup 已成为和 lxml、html6lib 一样出色的 python 解释器，为用户灵活地提供不同的解析策略或强劲的速度</p>\n</blockquote>\n<p><a href=\"https://beautifulsoup.readthedocs.io/zh_CN/latest/\">官网</a></p>\n<h3 id=\"2.-beautiful-soup-%E5%AE%89%E8%A3%85\">2. Beautiful Soup 安装 <a class=\"heading-anchor-permalink\" href=\"#2.-beautiful-soup-%E5%AE%89%E8%A3%85\">#</a></h3>\n<blockquote>\n<p>Beautiful Soup 3 目前已经停止开发，推荐在现在的项目中使用 Beautiful Soup 4，不过它已经被移植到 BS4 了,也就是说导入时我们需要 import bs4</p>\n</blockquote>\n<pre><code class=\"language-sh\">pip install beautifulsoup4\n</code></pre>\n<blockquote>\n<p>Beautiful Soup 支持 Python 标准库中的 HTML 解析器,还支持一些第三方的解析器，如果我们不安装它，则 Python 会使用 Python 默认的解析器，lxml 解析器更加强大，速度更快，推荐安装</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>解析器</th>\n<th>使用方法</th>\n<th>优势</th>\n<th>劣势</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Python 标准库</td>\n<td>BeautifulSoup(markup, “html.parser”)</td>\n<td>1. Python 的内置标准库 2. 执行速度适中 3.文档容错能力强</td>\n<td>Python 2.7.3 or 3.2.2)前 的版本中文档容错能力差</td>\n</tr>\n<tr>\n<td>lxml HTML 解析器</td>\n<td>BeautifulSoup(markup, “lxml”)</td>\n<td>1. 速度快 2.文档容错能力强</td>\n<td>需要安装 C 语言库</td>\n</tr>\n<tr>\n<td>lxml XML 解析器</td>\n<td>BeautifulSoup(markup, [“lxml”, “xml”]) BeautifulSoup(markup, “xml”)</td>\n<td>1. 速度快 2.唯一支持 XML 的解析器 3.需要安装 C 语言库</td>\n<td></td>\n</tr>\n<tr>\n<td>html5lib</td>\n<td>BeautifulSoup(markup, “html5lib”)</td>\n<td>1. 最好的容错性 2.以浏览器的方式解析文档 3.生成 HTML5 格式的文档 4.速度慢</td>\n<td>不依赖外部扩展</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"3.-%E5%88%9B%E5%BB%BA-beautiful-soup-%E5%AF%B9%E8%B1%A1\">3. 创建 Beautiful Soup 对象 <a class=\"heading-anchor-permalink\" href=\"#3.-%E5%88%9B%E5%BB%BA-beautiful-soup-%E5%AF%B9%E8%B1%A1\">#</a></h3>\n<pre><code class=\"language-python\">from bs4 import BeautifulSoup\n\nbs = BeautifulSoup(html,&quot;lxml&quot;)\n</code></pre>\n<h3 id=\"4.-%E5%9B%9B%E5%A4%A7%E5%AF%B9%E8%B1%A1%E7%A7%8D%E7%B1%BB\">4. 四大对象种类 <a class=\"heading-anchor-permalink\" href=\"#4.-%E5%9B%9B%E5%A4%A7%E5%AF%B9%E8%B1%A1%E7%A7%8D%E7%B1%BB\">#</a></h3>\n<blockquote>\n<p>Beautiful Soup 将复杂 HTML 文档转换成一个复杂的树形结构,每个节点都是 Python 对象,所有对象可以归纳为 4 种:</p>\n</blockquote>\n<ul>\n<li>Tag</li>\n<li>NavigableString</li>\n<li>BeautifulSoup</li>\n<li>Comment</li>\n</ul>\n<h4 id=\"4.1-tag-%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%E9%80%9A%E4%BF%97%E7%82%B9%E8%AE%B2%E5%B0%B1%E6%98%AF-html-%E4%B8%AD%E7%9A%84%E4%B8%80%E4%B8%AA%E4%B8%AA%E6%A0%87%E7%AD%BE\">4.1 Tag 是什么？通俗点讲就是 HTML 中的一个个标签 <a class=\"heading-anchor-permalink\" href=\"#4.1-tag-%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%E9%80%9A%E4%BF%97%E7%82%B9%E8%AE%B2%E5%B0%B1%E6%98%AF-html-%E4%B8%AD%E7%9A%84%E4%B8%80%E4%B8%AA%E4%B8%AA%E6%A0%87%E7%AD%BE\">#</a></h4>\n<p>例如：<code>&lt;div&gt;</code> <code>&lt;title&gt;</code></p>\n<p>使用方式：</p>\n<pre><code class=\"language-html\">#以以下代码为例子\n&lt;title&gt;知乎&lt;/title&gt;\n&lt;div class=&quot;info&quot; float=&quot;left&quot;&gt;Welcome to Zhihu&lt;/div&gt;\n&lt;div class=&quot;info&quot; float=&quot;right&quot;&gt;\n  &lt;span&gt;Good Good Study&lt;/span&gt;\n  &lt;a href=&quot;www.zhihu.com&quot;&gt;&lt;/a&gt;\n  &lt;strong&gt;&lt;!--没用--&gt;&lt;/strong&gt;\n&lt;/div&gt;\n</code></pre>\n<h5 id=\"4.1.1-%E8%8E%B7%E5%8F%96%E6%A0%87%E7%AD%BE\">4.1.1 获取标签 <a class=\"heading-anchor-permalink\" href=\"#4.1.1-%E8%8E%B7%E5%8F%96%E6%A0%87%E7%AD%BE\">#</a></h5>\n<pre><code class=\"language-python\">#以lxml方式解析\nsoup = BeautifulSoup(info, 'lxml')\nprint(soup.title)\n# &lt;title&gt;bilibili&lt;/title&gt;\n</code></pre>\n<p><strong>注意</strong></p>\n<blockquote>\n<p>相同的标签只能获取第一个符合要求的标签</p>\n</blockquote>\n<h5 id=\"4.1.2-%E8%8E%B7%E5%8F%96%E5%B1%9E%E6%80%A7\">4.1.2 获取属性 <a class=\"heading-anchor-permalink\" href=\"#4.1.2-%E8%8E%B7%E5%8F%96%E5%B1%9E%E6%80%A7\">#</a></h5>\n<pre><code class=\"language-python\">#获取所有属性\nprint(soup.title.attrs)\n#class='info' float='left'\n\n#获取单个属性的值\nprint(soup.div.get('class'))\nprint(soup.div['class'])\nprint(soup.a['href'])\n#info\n</code></pre>\n<h4 id=\"4.2-navigablestring-%E8%8E%B7%E5%8F%96%E5%86%85%E5%AE%B9\">4.2 NavigableString 获取内容 <a class=\"heading-anchor-permalink\" href=\"#4.2-navigablestring-%E8%8E%B7%E5%8F%96%E5%86%85%E5%AE%B9\">#</a></h4>\n<pre><code class=\"language-python\">print(soup.title.string)\nprint(soup.title.text)\n#bilibili\n</code></pre>\n<h4 id=\"4.3-beautifulsoup\">4.3 BeautifulSoup <a class=\"heading-anchor-permalink\" href=\"#4.3-beautifulsoup\">#</a></h4>\n<blockquote>\n<p>BeautifulSoup 对象表示的是一个文档的全部内容.大部分时候,可以把它当作 Tag 对象,它支持 遍历文档树 和 搜索文档树 中描述的大部分的方法.\n因为 BeautifulSoup 对象并不是真正的 HTML 或 XML 的 tag,所以它没有 <code>name</code> 和 <code>attribute</code> 属性.但有时查看它的 <code>.name</code> 属性是很方便的,所以 BeautifulSoup 对象包含了一个值为 <code>[document]</code> 的特殊属性 <code>.name</code></p>\n</blockquote>\n<pre><code class=\"language-python\">print(soup.name)\nprint(soup.head.name)\n# [document]\n# head\n</code></pre>\n<h4 id=\"4.4-comment\">4.4 Comment <a class=\"heading-anchor-permalink\" href=\"#4.4-comment\">#</a></h4>\n<blockquote>\n<p>Comment 对象是一个特殊类型的 NavigableString 对象，其实输出的内容仍然不包括注释符号，但是如果不好好处理它，可能会对我们的文本处理造成意想不到的麻烦</p>\n</blockquote>\n<pre><code class=\"language-python\">if type(soup.strong.string)==Comment:\n    print(soup.strong.prettify())\nelse:\n    print(soup.strong.string)\n</code></pre>\n<h3 id=\"5-%E6%90%9C%E7%B4%A2%E6%96%87%E6%A1%A3%E6%A0%91\">5 搜索文档树 <a class=\"heading-anchor-permalink\" href=\"#5-%E6%90%9C%E7%B4%A2%E6%96%87%E6%A1%A3%E6%A0%91\">#</a></h3>\n<blockquote>\n<p>Beautiful Soup 定义了很多搜索方法,这里着重介绍 2 个: find() 和 find_all() .其它方法的参数和用法类似,请同学们举一反三</p>\n</blockquote>\n<h4 id=\"5.1-%E8%BF%87%E6%BB%A4%E5%99%A8\">5.1 过滤器 <a class=\"heading-anchor-permalink\" href=\"#5.1-%E8%BF%87%E6%BB%A4%E5%99%A8\">#</a></h4>\n<blockquote>\n<p>介绍 find_all() 方法前,先介绍一下过滤器的类型 ,这些过滤器贯穿整个搜索的 API.过滤器可以被用在 tag 的 name 中,节点的属性中,字符串中或他们的混合中</p>\n</blockquote>\n<h5 id=\"5.1.1-%E5%AD%97%E7%AC%A6%E4%B8%B2\">5.1.1 字符串 <a class=\"heading-anchor-permalink\" href=\"#5.1.1-%E5%AD%97%E7%AC%A6%E4%B8%B2\">#</a></h5>\n<blockquote>\n<p>最简单的过滤器是字符串.在搜索方法中传入一个字符串参数,Beautiful Soup 会查找与字符串完整匹配的内容,下面的例子用于查找文档中所有的<code>&lt;div&gt;</code>标签</p>\n</blockquote>\n<pre><code class=\"language-python\">#返回所有的div标签\nprint(soup.find_all('div'))\n</code></pre>\n<blockquote>\n<p>如果传入字节码参数,Beautiful Soup 会当作 UTF-8 编码,可以传入一段 Unicode 编码来避免 Beautiful Soup 解析编码出错</p>\n</blockquote>\n<h5 id=\"5.1.2-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F\">5.1.2 正则表达式 <a class=\"heading-anchor-permalink\" href=\"#5.1.2-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F\">#</a></h5>\n<p>如果传入正则表达式作为参数,Beautiful Soup 会通过正则表达式的 match() 来匹配内容</p>\n<pre><code class=\"language-python\">#返回所有的div标签\nprint (soup.find_all(re.compile(&quot;^div&quot;)))\n</code></pre>\n<h5 id=\"5.1.3-%E5%88%97%E8%A1%A8\">5.1.3 列表 <a class=\"heading-anchor-permalink\" href=\"#5.1.3-%E5%88%97%E8%A1%A8\">#</a></h5>\n<blockquote>\n<p>如果传入列表参数,Beautiful Soup 会将与列表中任一元素匹配的内容返回</p>\n</blockquote>\n<pre><code class=\"language-python\">#返回所有匹配到的span a标签\nprint(soup.find_all(['span','a']))\n</code></pre>\n<h5 id=\"5.1.4-keyword\">5.1.4 keyword <a class=\"heading-anchor-permalink\" href=\"#5.1.4-keyword\">#</a></h5>\n<blockquote>\n<p>如果一个指定名字的参数不是搜索内置的参数名,搜索时会把该参数当作指定名字 tag 的属性来搜索,如果包含一个名字为 id 的参数,Beautiful Soup 会搜索每个 tag 的”id”属性</p>\n</blockquote>\n<pre><code class=\"language-python\">#返回id为welcom的标签\nprint(soup.find_all(id='welcom'))\n</code></pre>\n<h5 id=\"5.1.4-true\">5.1.4 True <a class=\"heading-anchor-permalink\" href=\"#5.1.4-true\">#</a></h5>\n<blockquote>\n<p>True 可以匹配任何值,下面代码查找到所有的 tag,但是不会返回字符串节点</p>\n</blockquote>\n<h5 id=\"5.1.5-%E6%8C%89-css-%E6%90%9C%E7%B4%A2\">5.1.5 按 CSS 搜索 <a class=\"heading-anchor-permalink\" href=\"#5.1.5-%E6%8C%89-css-%E6%90%9C%E7%B4%A2\">#</a></h5>\n<blockquote>\n<p>按照 CSS 类名搜索 tag 的功能非常实用,但标识 CSS 类名的关键字 class 在 Python 中是保留字,使用 class 做参数会导致语法错误.从 Beautiful Soup 的 4.1.1 版本开始,可以通过 class_ 参数搜索有指定 CSS 类名的 tag</p>\n</blockquote>\n<pre><code class=\"language-python\"># 返回class等于info的div\nprint(soup.find_all('div',class_='info'))\n</code></pre>\n<h4 id=\"5.1.6-%E6%8C%89%E5%B1%9E%E6%80%A7%E7%9A%84%E6%90%9C%E7%B4%A2\">5.1.6 按属性的搜索 <a class=\"heading-anchor-permalink\" href=\"#5.1.6-%E6%8C%89%E5%B1%9E%E6%80%A7%E7%9A%84%E6%90%9C%E7%B4%A2\">#</a></h4>\n<pre><code class=\"language-python\">soup.find_all(&quot;div&quot;, attrs={&quot;class&quot;: &quot;info&quot;})\n</code></pre>\n<h3 id=\"6.-css-%E9%80%89%E6%8B%A9%E5%99%A8%EF%BC%88%E6%89%A9%E5%B1%95%EF%BC%89\">6. CSS 选择器（扩展） <a class=\"heading-anchor-permalink\" href=\"#6.-css-%E9%80%89%E6%8B%A9%E5%99%A8%EF%BC%88%E6%89%A9%E5%B1%95%EF%BC%89\">#</a></h3>\n<p>soup.select(参数)</p>\n<table>\n<thead>\n<tr>\n<th>表达式</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>tag</td>\n<td>选择指定标签</td>\n</tr>\n<tr>\n<td>*</td>\n<td>选择所有节点</td>\n</tr>\n<tr>\n<td>#id</td>\n<td>选择 id 为 container 的节点</td>\n</tr>\n<tr>\n<td>.class</td>\n<td>选取所有 class 包含 container 的节点</td>\n</tr>\n<tr>\n<td>li a</td>\n<td>选取所有 li 下的所有 a 节点</td>\n</tr>\n<tr>\n<td>ul + p</td>\n<td>(兄弟)选择 ul 后面的第一个 p 元素</td>\n</tr>\n<tr>\n<td>div#id &gt; ul</td>\n<td>(父子)选取 id 为 id 的 div 的第一个 ul 子元素</td>\n</tr>\n<tr>\n<td>table ~ div</td>\n<td>选取与 table 相邻的所有 div 元素</td>\n</tr>\n<tr>\n<td>a[title]</td>\n<td>选取所有有 title 属性的 a 元素</td>\n</tr>\n<tr>\n<td>a[class=”title”]</td>\n<td>选取所有 class 属性为 title 值的 a</td>\n</tr>\n<tr>\n<td>a[href*=”sxt”]</td>\n<td>选取所有 href 属性包含 sxt 的 a 元素</td>\n</tr>\n<tr>\n<td>a[href^=”http”]</td>\n<td>选取所有 href 属性值以 http 开头的 a 元素</td>\n</tr>\n<tr>\n<td>a[href$=”.png”]</td>\n<td>选取所有 href 属性值以.png 结尾的 a 元素</td>\n</tr>\n<tr>\n<td>input[type=“redio”]:checked</td>\n<td>选取选中的 hobby 的元素</td>\n</tr>\n</tbody>\n</table>\n",
      "id": 7
    },
    {
      "path": "数据提取与验证码的识别（上）/JsonPath.md",
      "url": "数据提取与验证码的识别（上）/JsonPath.html",
      "content": "### 1. JSON 与 JsonPATH\n\nJSON(JavaScript Object Notation) 是一种轻量级的数据交换格式，它使得人们很容易的进行阅读和编写。同时也方便了机器进行解析和生成。适用于进行数据交互的场景，比如网站前台与后台之间的数据交互。\n\nJSON 和 XML 的比较可谓不相上下。\n\nPython 中自带了 JSON 模块，直接 import json 就可以使用了。\n\n[官方文档](http://docs.python.org/library/json.html)\n\n[Json 在线解析网站](http://www.json.cn/)\n\n### 2. JSON\n\njson 简单说就是 javascript 中的对象和数组，所以这两种结构就是对象和数组两种结构，通过这两种结构可以表示各种复杂的结构\n\n1. 对象：对象在 js 中表示为{ }括起来的内容，数据结构为 { key：value, key：value, ... }的键值对的结构，在面向对象的语言中，key 为对象的属性，value 为对应的属性值，所以很容易理解，取值方法为 对象.key 获取属性值，这个属性值的类型可以是数字、字符串、数组、对象这几种\n\n2. 数组：数组在 js 中是中括号[ ]括起来的内容，数据结构为 [\"Python\", \"javascript\", \"C++\", ...]，取值方式和所有语言中一样，使用索引获取，字段值的类型可以是 数字、字符串、数组、对象几种\n\n### 3. Python 中的 json 模块\n\n> json 模块提供了四个功能：dumps、dump、loads、load，用于字符串 和 python 数据类型间进行转换\n\n#### 3.1 json.loads()\n\n> 把 Json 格式字符串解码转换成 Python 对象 从 json 到 python 的类型转化对照如下：\n\n```python\nimport json\n\nstrList = '[1, 2, 3, 4]'\nstrDict = '{\"city\": \"上海\", \"name\": \"施想\"}'\njson.loads(strList)\n# [1, 2, 3, 4]\njson.loads(strDict) # json数据自动按Unicode存储\n# {u'city': u'\\u5317\\u4eac', u'name': u'\\u5927\\u732b'}\n```\n\n#### 3.2 json.dumps()\n\n> 实现 python 类型转化为 json 字符串，返回一个 str 对象 把一个 Python 对象编码转换成 Json 字符串\n\n从 python 原始类型向 json 类型的转化对照如下：\n\n```python\n# json_dumps.py\n\nimport json\n\n\nlistStr = [1, 2, 3, 4]\ntupleStr = (1, 2, 3, 4)\ndictStr = {\"city\": \"上海\", \"name\": \"施想\"}\n\njson.dumps(listStr)\n# '[1, 2, 3, 4]'\njson.dumps(tupleStr)\n# '[1, 2, 3, 4]'\n\n# 注意：json.dumps() 序列化时默认使用的ascii编码\n# 添加参数 ensure_ascii=False 禁用ascii编码，按utf-8编码\n\njson.dumps(dictStr)\n# '{\"city\": \"\\\\u5317\\\\u4eac\", \"name\": \"\\\\u5927\\\\u5218\"}'\n\nprint(json.dumps(dictStr, ensure_ascii=False))\n# {\"city\": \"上海\", \"name\": \"施想\"}\n\n```\n\n#### 3.3 json.dump()\n\n> 将 Python 内置类型序列化为 json 对象后写入文件\n\n```python\nimport json\n\nlistStr = [{\"city\": \"上海\"}, {\"name\": \"施想\"}]\njson.dump(listStr, open(\"listStr.json\",\"w\"), ensure_ascii=False)\n\ndictStr = {\"city\": \"上海\", \"name\": \"施想\"}\njson.dump(dictStr, open(\"dictStr.json\",\"w\"), ensure_ascii=False)\n```\n\n#### 3.4 json.load()\n\n> 读取文件中 json 形式的字符串元素 转化成 python 类型\n\n```python\nimport json\n\nstrList = json.load(open(\"listStr.json\"))\nprint(strList)\n\n# [{u'city': u'\\u5317\\u4eac'}, {u'name': u'\\u5927\\u5218'}]\n\nstrDict = json.load(open(\"dictStr.json\"))\nprint(strDict)\n# {u'city': u'\\u5317\\u4eac', u'name': u'\\u5927\\u5218'}\n```\n\n### 4 JsonPath\n\nJsonPath 是一种信息抽取类库，是从 JSON 文档中抽取指定信息的工具，提供多种语言实现版本，包括：Javascript, Python， PHP 和 Java。\n\nJsonPath 对于 JSON 来说，相当于 XPATH 对于 XML。\n\n安装方法：`pip install jsonpath`\n\n[官方文档](http://goessner.net/articles/JsonPath)\n\n### 5 JsonPath 与 XPath 语法对比\n\nJson 结构清晰，可读性高，复杂度低，非常容易匹配，下表中对应了 XPath 的用法\n\n| XPath  | JSONPath | 描述                                                                   |\n| ------ | -------- | ---------------------------------------------------------------------- |\n| `/`    | `$`      | 根节点                                                                 |\n| `.`    | `@`      | 现行节点                                                               |\n| `/`    | `.or[]`  | 取子节点                                                               |\n| `..`   | `n/a`    | 取父节点，Jsonpath 未支持                                              |\n| `//`   | `..`     | 就是不管位置，选择所有符合条件的条件                                   |\n| `*`    | `*`      | 匹配所有元素节点                                                       |\n| `@`    | `n/a`    | 根据属性访问，Json 不支持，因为 Json 是个 Key-value 递归结构，不需要。 |\n| `[]`   | `[]`     | 迭代器标示（可以在里边做简单的迭代操作，如数组下标，根据内容选值等）   |\n| &#124; | `[,]`    | 支持迭代器中做多选。                                                   |\n| `[]`   | `?()`    | 支持过滤操作.                                                          |\n| `n/a`  | `()`     | 支持表达式计算                                                         |\n| `()`   | `n/a`    | 分组，JsonPath 不支持                                                  |\n\n#### 6. 示例\n\n我们以[拉勾网城市 JSON 文件](http://www.lagou.com/lbs/getAllCitySearchLabels.json) 为例，获取所有城市\n\n```python\nfrom urllib.request import urlopen\nfrom urllib.request import Request\nimport jsonpath\nimport json\n\nurl = 'http://www.lagou.com/lbs/getAllCitySearchLabels.json'\nrequest =Request(url)\nresponse = urlopen(request)\nhtml = response.read()\n# 把json格式字符串转换成python对象\njsonobj = json.loads(html)\n# 从根节点开始，匹配name节点\ncitylist = jsonpath.jsonpath(jsonobj,'$..name')\nprint(citylist)\nprint(type(citylist))\nfp = open('city.json','w')\ncontent = json.dumps(citylist, ensure_ascii=False)\nprint(content)\nfp.write(content)\nfp.close()\n```\n\n#### 7. 注意事项\n\n- json.loads() 是把 Json 格式字符串解码转换成 Python 对象，如果在 json.loads 的时候出错，要注意被解码的 Json 字符的编码。\n  如果传入的字符串的编码不是 UTF-8 的话，需要指定字符编码的参数 encoding\n  `dataDict = json.loads(jsonStrGBK);`\n- dataJsonStr 是 JSON 字符串，假设其编码本身是非 UTF-8 的话而是 GBK 的，那么上述代码会导致出错，改为对应的：\n\n  ```python\n  dataDict = json.loads(jsonStrGBK, encoding=\"GBK\");\n  ```\n\n- 如果 dataJsonStr 通过 encoding 指定了合适的编码，但是其中又包含了其他编码的字符，则需要先去将 dataJsonStr 转换为 Unicode，然后再指定编码格式调用 json.loads()\n\n  ```python\n  dataJsonStrUni = dataJsonStr.decode(\"GB2312\");\n  dataDict = json.loads(dataJsonStrUni, encoding=\"GB2312\");\n  ```\n\n##### 7.1 字符串编码转换\n\n这是中国程序员最苦逼的地方，什么乱码之类的几乎都是由汉字引起的\n\n其实编码问题很好搞定，只要记住一点：\n\n**任何平台的任何编码 都能和 Unicode 互相转换**\n\nUTF-8 与 GBK 互相转换，那就先把 UTF-8 转换成 Unicode，再从 Unicode 转换成 GBK，反之同理。\n\n```python\n# 这是一个 UTF-8 编码的字符串\nutf8Str = \"你好地球\"\n\n# 1. 将 UTF-8 编码的字符串 转换成 Unicode 编码\nunicodeStr = utf8Str.decode(\"UTF-8\")\n\n# 2. 再将 Unicode 编码格式字符串 转换成 GBK 编码\ngbkData = unicodeStr.encode(\"GBK\")\n\n# 1. 再将 GBK 编码格式字符串 转化成 Unicode\nunicodeStr = gbkData.decode(\"gbk\")\n\n# 2. 再将 Unicode 编码格式字符串转换成 UTF-8\nutf8Str = unicodeStr.encode(\"UTF-8\")\n```\n\ndecode 的作用是将其他编码的字符串转换成 Unicode 编码\n\nencode 的作用是将 Unicode 编码转换成其他编码的字符串\n\n一句话：UTF-8 是对 Unicode 字符集进行编码的一种编码方式\n",
      "html": "<h3 id=\"1.-json-%E4%B8%8E-jsonpath\">1. JSON 与 JsonPATH <a class=\"heading-anchor-permalink\" href=\"#1.-json-%E4%B8%8E-jsonpath\">#</a></h3>\n<p>JSON(JavaScript Object Notation) 是一种轻量级的数据交换格式，它使得人们很容易的进行阅读和编写。同时也方便了机器进行解析和生成。适用于进行数据交互的场景，比如网站前台与后台之间的数据交互。</p>\n<p>JSON 和 XML 的比较可谓不相上下。</p>\n<p>Python 中自带了 JSON 模块，直接 import json 就可以使用了。</p>\n<p><a href=\"http://docs.python.org/library/json.html\">官方文档</a></p>\n<p><a href=\"http://www.json.cn/\">Json 在线解析网站</a></p>\n<h3 id=\"2.-json\">2. JSON <a class=\"heading-anchor-permalink\" href=\"#2.-json\">#</a></h3>\n<p>json 简单说就是 javascript 中的对象和数组，所以这两种结构就是对象和数组两种结构，通过这两种结构可以表示各种复杂的结构</p>\n<ol>\n<li>\n<p>对象：对象在 js 中表示为{ }括起来的内容，数据结构为 { key：value, key：value, … }的键值对的结构，在面向对象的语言中，key 为对象的属性，value 为对应的属性值，所以很容易理解，取值方法为 对象.key 获取属性值，这个属性值的类型可以是数字、字符串、数组、对象这几种</p>\n</li>\n<li>\n<p>数组：数组在 js 中是中括号[ ]括起来的内容，数据结构为 [“Python”, “javascript”, “C++”, …]，取值方式和所有语言中一样，使用索引获取，字段值的类型可以是 数字、字符串、数组、对象几种</p>\n</li>\n</ol>\n<h3 id=\"3.-python-%E4%B8%AD%E7%9A%84-json-%E6%A8%A1%E5%9D%97\">3. Python 中的 json 模块 <a class=\"heading-anchor-permalink\" href=\"#3.-python-%E4%B8%AD%E7%9A%84-json-%E6%A8%A1%E5%9D%97\">#</a></h3>\n<blockquote>\n<p>json 模块提供了四个功能：dumps、dump、loads、load，用于字符串 和 python 数据类型间进行转换</p>\n</blockquote>\n<h4 id=\"3.1-json.loads()\">3.1 json.loads() <a class=\"heading-anchor-permalink\" href=\"#3.1-json.loads()\">#</a></h4>\n<blockquote>\n<p>把 Json 格式字符串解码转换成 Python 对象 从 json 到 python 的类型转化对照如下：</p>\n</blockquote>\n<pre><code class=\"language-python\">import json\n\nstrList = '[1, 2, 3, 4]'\nstrDict = '{&quot;city&quot;: &quot;上海&quot;, &quot;name&quot;: &quot;施想&quot;}'\njson.loads(strList)\n# [1, 2, 3, 4]\njson.loads(strDict) # json数据自动按Unicode存储\n# {u'city': u'\\u5317\\u4eac', u'name': u'\\u5927\\u732b'}\n</code></pre>\n<h4 id=\"3.2-json.dumps()\">3.2 json.dumps() <a class=\"heading-anchor-permalink\" href=\"#3.2-json.dumps()\">#</a></h4>\n<blockquote>\n<p>实现 python 类型转化为 json 字符串，返回一个 str 对象 把一个 Python 对象编码转换成 Json 字符串</p>\n</blockquote>\n<p>从 python 原始类型向 json 类型的转化对照如下：</p>\n<pre><code class=\"language-python\"># json_dumps.py\n\nimport json\n\n\nlistStr = [1, 2, 3, 4]\ntupleStr = (1, 2, 3, 4)\ndictStr = {&quot;city&quot;: &quot;上海&quot;, &quot;name&quot;: &quot;施想&quot;}\n\njson.dumps(listStr)\n# '[1, 2, 3, 4]'\njson.dumps(tupleStr)\n# '[1, 2, 3, 4]'\n\n# 注意：json.dumps() 序列化时默认使用的ascii编码\n# 添加参数 ensure_ascii=False 禁用ascii编码，按utf-8编码\n\njson.dumps(dictStr)\n# '{&quot;city&quot;: &quot;\\\\u5317\\\\u4eac&quot;, &quot;name&quot;: &quot;\\\\u5927\\\\u5218&quot;}'\n\nprint(json.dumps(dictStr, ensure_ascii=False))\n# {&quot;city&quot;: &quot;上海&quot;, &quot;name&quot;: &quot;施想&quot;}\n\n</code></pre>\n<h4 id=\"3.3-json.dump()\">3.3 json.dump() <a class=\"heading-anchor-permalink\" href=\"#3.3-json.dump()\">#</a></h4>\n<blockquote>\n<p>将 Python 内置类型序列化为 json 对象后写入文件</p>\n</blockquote>\n<pre><code class=\"language-python\">import json\n\nlistStr = [{&quot;city&quot;: &quot;上海&quot;}, {&quot;name&quot;: &quot;施想&quot;}]\njson.dump(listStr, open(&quot;listStr.json&quot;,&quot;w&quot;), ensure_ascii=False)\n\ndictStr = {&quot;city&quot;: &quot;上海&quot;, &quot;name&quot;: &quot;施想&quot;}\njson.dump(dictStr, open(&quot;dictStr.json&quot;,&quot;w&quot;), ensure_ascii=False)\n</code></pre>\n<h4 id=\"3.4-json.load()\">3.4 json.load() <a class=\"heading-anchor-permalink\" href=\"#3.4-json.load()\">#</a></h4>\n<blockquote>\n<p>读取文件中 json 形式的字符串元素 转化成 python 类型</p>\n</blockquote>\n<pre><code class=\"language-python\">import json\n\nstrList = json.load(open(&quot;listStr.json&quot;))\nprint(strList)\n\n# [{u'city': u'\\u5317\\u4eac'}, {u'name': u'\\u5927\\u5218'}]\n\nstrDict = json.load(open(&quot;dictStr.json&quot;))\nprint(strDict)\n# {u'city': u'\\u5317\\u4eac', u'name': u'\\u5927\\u5218'}\n</code></pre>\n<h3 id=\"4-jsonpath\">4 JsonPath <a class=\"heading-anchor-permalink\" href=\"#4-jsonpath\">#</a></h3>\n<p>JsonPath 是一种信息抽取类库，是从 JSON 文档中抽取指定信息的工具，提供多种语言实现版本，包括：Javascript, Python， PHP 和 Java。</p>\n<p>JsonPath 对于 JSON 来说，相当于 XPATH 对于 XML。</p>\n<p>安装方法：<code>pip install jsonpath</code></p>\n<p><a href=\"http://goessner.net/articles/JsonPath\">官方文档</a></p>\n<h3 id=\"5-jsonpath-%E4%B8%8E-xpath-%E8%AF%AD%E6%B3%95%E5%AF%B9%E6%AF%94\">5 JsonPath 与 XPath 语法对比 <a class=\"heading-anchor-permalink\" href=\"#5-jsonpath-%E4%B8%8E-xpath-%E8%AF%AD%E6%B3%95%E5%AF%B9%E6%AF%94\">#</a></h3>\n<p>Json 结构清晰，可读性高，复杂度低，非常容易匹配，下表中对应了 XPath 的用法</p>\n<table>\n<thead>\n<tr>\n<th>XPath</th>\n<th>JSONPath</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>/</code></td>\n<td><code>$</code></td>\n<td>根节点</td>\n</tr>\n<tr>\n<td><code>.</code></td>\n<td><code>@</code></td>\n<td>现行节点</td>\n</tr>\n<tr>\n<td><code>/</code></td>\n<td><code>.or[]</code></td>\n<td>取子节点</td>\n</tr>\n<tr>\n<td><code>..</code></td>\n<td><code>n/a</code></td>\n<td>取父节点，Jsonpath 未支持</td>\n</tr>\n<tr>\n<td><code>//</code></td>\n<td><code>..</code></td>\n<td>就是不管位置，选择所有符合条件的条件</td>\n</tr>\n<tr>\n<td><code>*</code></td>\n<td><code>*</code></td>\n<td>匹配所有元素节点</td>\n</tr>\n<tr>\n<td><code>@</code></td>\n<td><code>n/a</code></td>\n<td>根据属性访问，Json 不支持，因为 Json 是个 Key-value 递归结构，不需要。</td>\n</tr>\n<tr>\n<td><code>[]</code></td>\n<td><code>[]</code></td>\n<td>迭代器标示（可以在里边做简单的迭代操作，如数组下标，根据内容选值等）</td>\n</tr>\n<tr>\n<td>|</td>\n<td><code>[,]</code></td>\n<td>支持迭代器中做多选。</td>\n</tr>\n<tr>\n<td><code>[]</code></td>\n<td><code>?()</code></td>\n<td>支持过滤操作.</td>\n</tr>\n<tr>\n<td><code>n/a</code></td>\n<td><code>()</code></td>\n<td>支持表达式计算</td>\n</tr>\n<tr>\n<td><code>()</code></td>\n<td><code>n/a</code></td>\n<td>分组，JsonPath 不支持</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"6.-%E7%A4%BA%E4%BE%8B\">6. 示例 <a class=\"heading-anchor-permalink\" href=\"#6.-%E7%A4%BA%E4%BE%8B\">#</a></h4>\n<p>我们以<a href=\"http://www.lagou.com/lbs/getAllCitySearchLabels.json\">拉勾网城市 JSON 文件</a> 为例，获取所有城市</p>\n<pre><code class=\"language-python\">from urllib.request import urlopen\nfrom urllib.request import Request\nimport jsonpath\nimport json\n\nurl = 'http://www.lagou.com/lbs/getAllCitySearchLabels.json'\nrequest =Request(url)\nresponse = urlopen(request)\nhtml = response.read()\n# 把json格式字符串转换成python对象\njsonobj = json.loads(html)\n# 从根节点开始，匹配name节点\ncitylist = jsonpath.jsonpath(jsonobj,'$..name')\nprint(citylist)\nprint(type(citylist))\nfp = open('city.json','w')\ncontent = json.dumps(citylist, ensure_ascii=False)\nprint(content)\nfp.write(content)\nfp.close()\n</code></pre>\n<h4 id=\"7.-%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9\">7. 注意事项 <a class=\"heading-anchor-permalink\" href=\"#7.-%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9\">#</a></h4>\n<ul>\n<li>\n<p>json.loads() 是把 Json 格式字符串解码转换成 Python 对象，如果在 json.loads 的时候出错，要注意被解码的 Json 字符的编码。\n如果传入的字符串的编码不是 UTF-8 的话，需要指定字符编码的参数 encoding\n<code>dataDict = json.loads(jsonStrGBK);</code></p>\n</li>\n<li>\n<p>dataJsonStr 是 JSON 字符串，假设其编码本身是非 UTF-8 的话而是 GBK 的，那么上述代码会导致出错，改为对应的：</p>\n<pre><code class=\"language-python\">dataDict = json.loads(jsonStrGBK, encoding=&quot;GBK&quot;);\n</code></pre>\n</li>\n<li>\n<p>如果 dataJsonStr 通过 encoding 指定了合适的编码，但是其中又包含了其他编码的字符，则需要先去将 dataJsonStr 转换为 Unicode，然后再指定编码格式调用 json.loads()</p>\n<pre><code class=\"language-python\">dataJsonStrUni = dataJsonStr.decode(&quot;GB2312&quot;);\ndataDict = json.loads(dataJsonStrUni, encoding=&quot;GB2312&quot;);\n</code></pre>\n</li>\n</ul>\n<h5 id=\"7.1-%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%BC%96%E7%A0%81%E8%BD%AC%E6%8D%A2\">7.1 字符串编码转换 <a class=\"heading-anchor-permalink\" href=\"#7.1-%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%BC%96%E7%A0%81%E8%BD%AC%E6%8D%A2\">#</a></h5>\n<p>这是中国程序员最苦逼的地方，什么乱码之类的几乎都是由汉字引起的</p>\n<p>其实编码问题很好搞定，只要记住一点：</p>\n<p><strong>任何平台的任何编码 都能和 Unicode 互相转换</strong></p>\n<p>UTF-8 与 GBK 互相转换，那就先把 UTF-8 转换成 Unicode，再从 Unicode 转换成 GBK，反之同理。</p>\n<pre><code class=\"language-python\"># 这是一个 UTF-8 编码的字符串\nutf8Str = &quot;你好地球&quot;\n\n# 1. 将 UTF-8 编码的字符串 转换成 Unicode 编码\nunicodeStr = utf8Str.decode(&quot;UTF-8&quot;)\n\n# 2. 再将 Unicode 编码格式字符串 转换成 GBK 编码\ngbkData = unicodeStr.encode(&quot;GBK&quot;)\n\n# 1. 再将 GBK 编码格式字符串 转化成 Unicode\nunicodeStr = gbkData.decode(&quot;gbk&quot;)\n\n# 2. 再将 Unicode 编码格式字符串转换成 UTF-8\nutf8Str = unicodeStr.encode(&quot;UTF-8&quot;)\n</code></pre>\n<p>decode 的作用是将其他编码的字符串转换成 Unicode 编码</p>\n<p>encode 的作用是将 Unicode 编码转换成其他编码的字符串</p>\n<p>一句话：UTF-8 是对 Unicode 字符集进行编码的一种编码方式</p>\n",
      "id": 8
    },
    {
      "path": "数据提取与验证码的识别（上）/PyQuery.md",
      "url": "数据提取与验证码的识别（上）/PyQuery.html",
      "content": "### 1. pyquery\n\n#### 1.1 介绍\n\n> 如果你对 CSS 选择器与 Jquery 有有所了解，那么还有个解析库可以适合你--Jquery\n> [官网](https://pythonhosted.org/pyquery/)\n\n#### 1.2 安装\n\n```sh\npip install pyquery\n```\n\n#### 1.3 使用方式\n\n##### 1.3.1 初始化方式\n\n- 字符串\n\n```python\nfrom pyquery import PyQuery as pq\ndoc = pq(str)\nprint(doc(tagname))\n```\n\n- url\n\n```python\nfrom pyquery import PyQuery as pq\ndoc = pq(url='http://www.baidu.com')\nprint(doc('title'))\n```\n\n- 文件\n\n```python\nfrom pyquery import PyQuery as pq\ndoc = pq(filename='demo.html')\nprint(doc(tagname))\n```\n\n##### 1.3.2 选择节点\n\n- 获取当前节点\n\n```python\nfrom pyquery import PyQuery as pq\ndoc = pq(filename='demo.html')\ndoc('#main #top')\n```\n\n- 获取子节点\n  - 在 doc 中一层层写出来\n  - 获取到父标签后使用 children 方法\n\n```python\nfrom pyquery import PyQuery as pq\ndoc = pq(filename='demo.html')\ndoc('#main #top').children()\n```\n\n- 获取父节点\n  - 获取到当前节点后使用 parent 方法\n- 获取兄弟节点\n  - 获取到当前节点后使用 siblings 方法\n\n##### 1.3.3 获取属性\n\n```python\nfrom pyquery import PyQuery as pq\ndoc = pq(filename='demo.html')\na = doc('#main #top')\nprint(a.attrib['href'])\nprint(a.attr('href'))\n```\n\n##### 1.3.4 获取内容\n\n```python\nfrom pyquery import PyQuery as pq\ndoc = pq(filename='demo.html')\ndiv = doc('#main #top')\nprint(a.html())\nprint(a.text())\n```\n\n##### 1.3.5 样例\n\n```python\nfrom pyquery import PyQuery as pq\n# 1.可加载一段HTML字符串，或一个HTML文件，或是一个url地址，\nd=pq(\"<html><title>hello</title></html>\")\nd=pq(filename=path_to_html_file)\nd=pq(url='http://www.baidu.com')注意：此处url似乎必须写全\n\n# 2.html()和text() ——获取相应的HTML块或文本块，\np=pq(\"<head><title>hello</title></head>\")\np('head').html()#返回<title>hello</title>\np('head').text()#返回hello\n\n# 3.根据HTML标签来获取元素，\nd=pq('<div><p>test 1</p><p>test 2</p></div>')\nd('p')#返回[<p>,<p>]\nprint d('p')#返回<p>test 1</p><p>test 2</p>\nprint d('p').html()#返回test 1\n# 注意：当获取到的元素不只一个时，html()方法只返回首个元素的相应内容块\n\n# 4.eq(index) ——根据给定的索引号得到指定元素。接上例，若想得到第二个p标签内的内容，则可以：\nprint d('p').eq(1).html() #返回test 2\n\n# 5.filter() ——根据类名、id名得到指定元素，例：\nd=pq(\"<div><p id='1'>test 1</p><p class='2'>test 2</p></div>\")\nd('p').filter('#1') #返回[<p#1>]\nd('p').filter('.2') #返回[<p.2>]\n\n# 6.find() ——查找嵌套元素，例：\nd=pq(\"<div><p id='1'>test 1</p><p class='2'>test 2</p></div>\")\nd('div').find('p')#返回[<p#1>, <p.2>]\nd('div').find('p').eq(0)#返回[<p#1>]\n\n#7.直接根据类名、id名获取元素，例：\nd=pq(\"<div><p id='1'>test 1</p><p class='2'>test 2</p></div>\")\nd('#1').html()#返回test 1\nd('.2').html()#返回test 2\n\n# 8.获取属性值，例：\nd=pq(\"<p id='my_id'><a href='http://hello.com'>hello</a></p>\")\nd('a').attr('href')#返回http://hello.com\nd('p').attr('id')#返回my_id\n\n# 9.修改属性值，例：\nd('a').attr('href', 'http://baidu.com')把href属性修改为了baidu\n\n# 10.addClass(value) ——为元素添加类，例：\nd=pq('<div></div>')\nd.addClass('my_class')#返回[<div.my_class>]\n\n# 11.hasClass(name) #返回判断元素是否包含给定的类，例：\nd=pq(\"<div class='my_class'></div>\")\nd.hasClass('my_class')#返回True\n\n# 12.children(selector=None) ——获取子元素，例：\nd=pq(\"<span><p id='1'>hello</p><p id='2'>world</p></span>\")\nd.children()#返回[<p#1>, <p#2>]\nd.children('#2')#返回[<p#2>]\n\n# 13.parents(selector=None)——获取父元素，例：\nd=pq(\"<span><p id='1'>hello</p><p id='2'>world</p></span>\")\nd('p').parents()#返回[<span>]\nd('#1').parents('span')#返回[<span>]\nd('#1').parents('p')#返回[]\n\n# 14.clone() ——返回一个节点的拷贝\n\n# 15.empty() ——移除节点内容\n\n# 16.nextAll(selector=None) ——返回后面全部的元素块，例：\nd=pq(\"<p id='1'>hello</p><p id='2'>world</p><img scr='' />\")\nd('p:first').nextAll()#返回[<p#2>, <img>]\nd('p:last').nextAll()#返回[<img>]\n\n# 17.not_(selector) ——返回不匹配选择器的元素，例：\nd=pq(\"<p id='1'>test 1</p><p id='2'>test 2</p>\")\nd('p').not_('#2')#返回[<p#1>]\n```\n",
      "html": "<h3 id=\"1.-pyquery\">1. pyquery <a class=\"heading-anchor-permalink\" href=\"#1.-pyquery\">#</a></h3>\n<h4 id=\"1.1-%E4%BB%8B%E7%BB%8D\">1.1 介绍 <a class=\"heading-anchor-permalink\" href=\"#1.1-%E4%BB%8B%E7%BB%8D\">#</a></h4>\n<blockquote>\n<p>如果你对 CSS 选择器与 Jquery 有有所了解，那么还有个解析库可以适合你–Jquery\n<a href=\"https://pythonhosted.org/pyquery/\">官网</a></p>\n</blockquote>\n<h4 id=\"1.2-%E5%AE%89%E8%A3%85\">1.2 安装 <a class=\"heading-anchor-permalink\" href=\"#1.2-%E5%AE%89%E8%A3%85\">#</a></h4>\n<pre><code class=\"language-sh\">pip install pyquery\n</code></pre>\n<h4 id=\"1.3-%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F\">1.3 使用方式 <a class=\"heading-anchor-permalink\" href=\"#1.3-%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F\">#</a></h4>\n<h5 id=\"1.3.1-%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E5%BC%8F\">1.3.1 初始化方式 <a class=\"heading-anchor-permalink\" href=\"#1.3.1-%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E5%BC%8F\">#</a></h5>\n<ul>\n<li>字符串</li>\n</ul>\n<pre><code class=\"language-python\">from pyquery import PyQuery as pq\ndoc = pq(str)\nprint(doc(tagname))\n</code></pre>\n<ul>\n<li>url</li>\n</ul>\n<pre><code class=\"language-python\">from pyquery import PyQuery as pq\ndoc = pq(url='http://www.baidu.com')\nprint(doc('title'))\n</code></pre>\n<ul>\n<li>文件</li>\n</ul>\n<pre><code class=\"language-python\">from pyquery import PyQuery as pq\ndoc = pq(filename='demo.html')\nprint(doc(tagname))\n</code></pre>\n<h5 id=\"1.3.2-%E9%80%89%E6%8B%A9%E8%8A%82%E7%82%B9\">1.3.2 选择节点 <a class=\"heading-anchor-permalink\" href=\"#1.3.2-%E9%80%89%E6%8B%A9%E8%8A%82%E7%82%B9\">#</a></h5>\n<ul>\n<li>获取当前节点</li>\n</ul>\n<pre><code class=\"language-python\">from pyquery import PyQuery as pq\ndoc = pq(filename='demo.html')\ndoc('#main #top')\n</code></pre>\n<ul>\n<li>获取子节点\n<ul>\n<li>在 doc 中一层层写出来</li>\n<li>获取到父标签后使用 children 方法</li>\n</ul>\n</li>\n</ul>\n<pre><code class=\"language-python\">from pyquery import PyQuery as pq\ndoc = pq(filename='demo.html')\ndoc('#main #top').children()\n</code></pre>\n<ul>\n<li>获取父节点\n<ul>\n<li>获取到当前节点后使用 parent 方法</li>\n</ul>\n</li>\n<li>获取兄弟节点\n<ul>\n<li>获取到当前节点后使用 siblings 方法</li>\n</ul>\n</li>\n</ul>\n<h5 id=\"1.3.3-%E8%8E%B7%E5%8F%96%E5%B1%9E%E6%80%A7\">1.3.3 获取属性 <a class=\"heading-anchor-permalink\" href=\"#1.3.3-%E8%8E%B7%E5%8F%96%E5%B1%9E%E6%80%A7\">#</a></h5>\n<pre><code class=\"language-python\">from pyquery import PyQuery as pq\ndoc = pq(filename='demo.html')\na = doc('#main #top')\nprint(a.attrib['href'])\nprint(a.attr('href'))\n</code></pre>\n<h5 id=\"1.3.4-%E8%8E%B7%E5%8F%96%E5%86%85%E5%AE%B9\">1.3.4 获取内容 <a class=\"heading-anchor-permalink\" href=\"#1.3.4-%E8%8E%B7%E5%8F%96%E5%86%85%E5%AE%B9\">#</a></h5>\n<pre><code class=\"language-python\">from pyquery import PyQuery as pq\ndoc = pq(filename='demo.html')\ndiv = doc('#main #top')\nprint(a.html())\nprint(a.text())\n</code></pre>\n<h5 id=\"1.3.5-%E6%A0%B7%E4%BE%8B\">1.3.5 样例 <a class=\"heading-anchor-permalink\" href=\"#1.3.5-%E6%A0%B7%E4%BE%8B\">#</a></h5>\n<pre><code class=\"language-python\">from pyquery import PyQuery as pq\n# 1.可加载一段HTML字符串，或一个HTML文件，或是一个url地址，\nd=pq(&quot;&lt;html&gt;&lt;title&gt;hello&lt;/title&gt;&lt;/html&gt;&quot;)\nd=pq(filename=path_to_html_file)\nd=pq(url='http://www.baidu.com')注意：此处url似乎必须写全\n\n# 2.html()和text() ——获取相应的HTML块或文本块，\np=pq(&quot;&lt;head&gt;&lt;title&gt;hello&lt;/title&gt;&lt;/head&gt;&quot;)\np('head').html()#返回&lt;title&gt;hello&lt;/title&gt;\np('head').text()#返回hello\n\n# 3.根据HTML标签来获取元素，\nd=pq('&lt;div&gt;&lt;p&gt;test 1&lt;/p&gt;&lt;p&gt;test 2&lt;/p&gt;&lt;/div&gt;')\nd('p')#返回[&lt;p&gt;,&lt;p&gt;]\nprint d('p')#返回&lt;p&gt;test 1&lt;/p&gt;&lt;p&gt;test 2&lt;/p&gt;\nprint d('p').html()#返回test 1\n# 注意：当获取到的元素不只一个时，html()方法只返回首个元素的相应内容块\n\n# 4.eq(index) ——根据给定的索引号得到指定元素。接上例，若想得到第二个p标签内的内容，则可以：\nprint d('p').eq(1).html() #返回test 2\n\n# 5.filter() ——根据类名、id名得到指定元素，例：\nd=pq(&quot;&lt;div&gt;&lt;p id='1'&gt;test 1&lt;/p&gt;&lt;p class='2'&gt;test 2&lt;/p&gt;&lt;/div&gt;&quot;)\nd('p').filter('#1') #返回[&lt;p#1&gt;]\nd('p').filter('.2') #返回[&lt;p.2&gt;]\n\n# 6.find() ——查找嵌套元素，例：\nd=pq(&quot;&lt;div&gt;&lt;p id='1'&gt;test 1&lt;/p&gt;&lt;p class='2'&gt;test 2&lt;/p&gt;&lt;/div&gt;&quot;)\nd('div').find('p')#返回[&lt;p#1&gt;, &lt;p.2&gt;]\nd('div').find('p').eq(0)#返回[&lt;p#1&gt;]\n\n#7.直接根据类名、id名获取元素，例：\nd=pq(&quot;&lt;div&gt;&lt;p id='1'&gt;test 1&lt;/p&gt;&lt;p class='2'&gt;test 2&lt;/p&gt;&lt;/div&gt;&quot;)\nd('#1').html()#返回test 1\nd('.2').html()#返回test 2\n\n# 8.获取属性值，例：\nd=pq(&quot;&lt;p id='my_id'&gt;&lt;a href='http://hello.com'&gt;hello&lt;/a&gt;&lt;/p&gt;&quot;)\nd('a').attr('href')#返回http://hello.com\nd('p').attr('id')#返回my_id\n\n# 9.修改属性值，例：\nd('a').attr('href', 'http://baidu.com')把href属性修改为了baidu\n\n# 10.addClass(value) ——为元素添加类，例：\nd=pq('&lt;div&gt;&lt;/div&gt;')\nd.addClass('my_class')#返回[&lt;div.my_class&gt;]\n\n# 11.hasClass(name) #返回判断元素是否包含给定的类，例：\nd=pq(&quot;&lt;div class='my_class'&gt;&lt;/div&gt;&quot;)\nd.hasClass('my_class')#返回True\n\n# 12.children(selector=None) ——获取子元素，例：\nd=pq(&quot;&lt;span&gt;&lt;p id='1'&gt;hello&lt;/p&gt;&lt;p id='2'&gt;world&lt;/p&gt;&lt;/span&gt;&quot;)\nd.children()#返回[&lt;p#1&gt;, &lt;p#2&gt;]\nd.children('#2')#返回[&lt;p#2&gt;]\n\n# 13.parents(selector=None)——获取父元素，例：\nd=pq(&quot;&lt;span&gt;&lt;p id='1'&gt;hello&lt;/p&gt;&lt;p id='2'&gt;world&lt;/p&gt;&lt;/span&gt;&quot;)\nd('p').parents()#返回[&lt;span&gt;]\nd('#1').parents('span')#返回[&lt;span&gt;]\nd('#1').parents('p')#返回[]\n\n# 14.clone() ——返回一个节点的拷贝\n\n# 15.empty() ——移除节点内容\n\n# 16.nextAll(selector=None) ——返回后面全部的元素块，例：\nd=pq(&quot;&lt;p id='1'&gt;hello&lt;/p&gt;&lt;p id='2'&gt;world&lt;/p&gt;&lt;img scr='' /&gt;&quot;)\nd('p:first').nextAll()#返回[&lt;p#2&gt;, &lt;img&gt;]\nd('p:last').nextAll()#返回[&lt;img&gt;]\n\n# 17.not_(selector) ——返回不匹配选择器的元素，例：\nd=pq(&quot;&lt;p id='1'&gt;test 1&lt;/p&gt;&lt;p id='2'&gt;test 2&lt;/p&gt;&quot;)\nd('p').not_('#2')#返回[&lt;p#1&gt;]\n</code></pre>\n",
      "id": 9
    },
    {
      "path": "数据提取与验证码的识别（上）/Re.md",
      "url": "数据提取与验证码的识别（上）/Re.html",
      "content": "### 1. 提取数据\n\n在前面我们已经搞定了怎样获取页面的内容，不过还差一步，这么多杂乱的代码夹杂文字我们怎样把它提取出来整理呢？下面就开始介绍一个十分强大的工具，正则表达式！\n\n> 正则表达式是对字符串操作的一种逻辑公式，就是用事先定义好的一些特定字符、及这些特定字符的组合，组成一个“规则字符串”，这个“规则字符串”用来表达对字符串的一种过滤逻辑。\n\n正则表达式是用来匹配字符串非常强大的工具，在其他编程语言中同样有正则表达式的概念，Python 同样不例外，利用了正则表达式，我们想要从返回的页面内容提取出我们想要的内容就易如反掌了\n\n**规则**：\n\n| 模式            | 描述                                                                                                                                                                      |\n| --------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------- |\n| ^               | 匹配字符串的开头                                                                                                                                                          |\n| $               | 匹配字符串的末尾                                                                                                                                                          |\n| .               | 匹配任意字符，除了换行符，当 re.DOTALL 标记被指定时，则可以匹配包括换行符的任意字符                                                                                       |\n| [...]           | 用来表示一组字符,单独列出：[amk] 匹配 'a'，'m'或'k'                                                                                                                       |\n| [^...]          | 不在[]中的字符：[^abc] 匹配除了 a,b,c 之外的字符                                                                                                                          |\n| re\\*            | 匹配 0 个或多个的表达式                                                                                                                                                   |\n| re+             | 匹配 1 个或多个的表达式                                                                                                                                                   |\n| re?             | 匹配 0 个或 1 个由前面的正则表达式定义的片段，非贪婪方式                                                                                                                  |\n| re{ n}          |\n| re{ n,}         | 精确匹配 n 个前面表达式                                                                                                                                                   |\n| re{ n, m}       | 匹配 n 到 m 次由前面的正则表达式定义的片段，贪婪方式                                                                                                                      |\n| a               | b                                                                                                                                                                         | 匹配 a 或 b |\n| (re)            | G 匹配括号内的表达式，也表示一个组                                                                                                                                        |\n| (?imx)          | 正则表达式包含三种可选标志：i, m, 或 x 。只影响括号中的区域                                                                                                               |\n| (?-imx)         | 正则表达式关闭 i, m, 或 x 可选标志。只影响括号中的区域                                                                                                                    |\n| (?: re)         | 类似 (...), 但是不表示一个组                                                                                                                                              |\n| (?imx: re)      | 在括号中使用 i, m, 或 x 可选标志                                                                                                                                          |\n| (?-imx: re)     | 在括号中不使用 i, m, 或 x 可选标志                                                                                                                                        |\n| (?#...)         | 注释                                                                                                                                                                      |\n| (?= re)         | 前向肯定界定符。如果所含正则表达式，以 ... 表示，在当前位置成功匹配时成功，否则失败。但一旦所含表达式已经尝试，匹配引擎根本没有提高；模式的剩余部分还要尝试界定符的右边。 |\n| (?! re)         | 前向否定界定符。与肯定界定符相反；当所含表达式不能在字符串当前位置匹配时成功                                                                                              |\n| (?> re)         | 匹配的独立模式，省去回溯                                                                                                                                                  |\n| \\w              | 匹配字母数字及下划线                                                                                                                                                      |\n| \\W              | 匹配非字母数字及下划线                                                                                                                                                    |\n| \\s              | 匹配任意空白字符，等价于 [\\t\\n\\r\\f].                                                                                                                                      |\n| \\S              | 匹配任意非空字符                                                                                                                                                          |\n| \\d              | 匹配任意数字，等价于 [0-9]                                                                                                                                                |\n| \\D              | 匹配任意非数字                                                                                                                                                            |\n| \\A              | 匹配字符串开始                                                                                                                                                            |\n| \\Z              | 匹配字符串结束，如果是存在换行，只匹配到换行前的结束字符串。c                                                                                                             |\n| \\z              | 匹配字符串结束                                                                                                                                                            |\n| \\G              | 匹配最后匹配完成的位置                                                                                                                                                    |\n| \\b              | 匹配一个单词边界，也就是指单词和空格间的位置。例如， 'er\\b' 可以匹配\"never\" 中的 'er'，但不能匹配 \"verb\" 中的 'er'                                                        |\n| \\B              | 匹配非单词边界。'er\\B' 能匹配 \"verb\" 中的 'er'，但不能匹配 \"never\" 中的 'er'                                                                                              |\n| \\n, \\t, 等.     | 匹配一个换行符。匹配一个制表符。等                                                                                                                                        |\n| \\1...\\9         | 匹配第 n 个分组的内容                                                                                                                                                     |\n| \\10             | 匹配第 n 个分组的内容，如果它经匹配。否则指的是八进制字符码的表达式                                                                                                       |\n| [\\u4e00-\\u9fa5] | 中文                                                                                                                                                                      |\n\n### 2. 正则表达式相关注解\n\n#### 2.1 数量词的贪婪模式与非贪婪模式\n\n正则表达式通常用于在文本中查找匹配的字符串\nPython 里数量词默认是贪婪的（在少数语言里也可能是默认非贪婪），总是尝试匹配尽可能多的字符；非贪婪的则相反，总是尝试匹配尽可能少的字符\n\n例如：正则表达式”ab*”如果用于查找”abbbc”，将找到”abbb”。而如果使用非贪婪的数量词”ab*?”，将找到”a”\n\n#### 2.2 常用方法\n\n- re.match\n  - re.match 尝试从字符串的起始位置匹配一个模式，如果不是起始位置匹配成功的话，match()就返回 none\n  - 函数语法：\n    re.match(pattern, string, flags=0)\n- re.search\n  - re.search 扫描整个字符串并返回第一个成功的匹配。\n  - 函数语法：\n    re.search(pattern, string, flags=0)\n- re.sub\n  - re.sub 替换字符串\n    re.sub(pattern,replace,string)\n- re.findall\n  - re.findall 查找全部\n    re.findall(pattern,string,flags=0)\n\n### 3. 正则表达式修饰符 - 可选标志\n\n> 正则表达式可以包含一些可选标志修饰符来控制匹配的模式。修饰符被指定为一个可选的标志。多个标志可以通过按位 OR(|) 它们来指定。如 re.I | re.M 被设置成 I 和 M 标志：\n\n| 修饰符 | 描述                                                         |\n| ------ | ------------------------------------------------------------ | ------------------- |\n| re.I   | 使匹配对大小写不敏感                                         |\n| re.L   | 做本地化识别（locale-aware）匹配                             |\n| re.M   |                                                              | 多行匹配，影响 ^ 和 |\n| re.S   | 使 . 匹配包括换行在内的所有字符                              |\n| re.U   | 根据 Unicode 字符集解析字符。这个标志影响 \\w, \\W, \\b, \\B     |\n| re.X   | 该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解 |\n",
      "html": "<h3 id=\"1.-%E6%8F%90%E5%8F%96%E6%95%B0%E6%8D%AE\">1. 提取数据 <a class=\"heading-anchor-permalink\" href=\"#1.-%E6%8F%90%E5%8F%96%E6%95%B0%E6%8D%AE\">#</a></h3>\n<p>在前面我们已经搞定了怎样获取页面的内容，不过还差一步，这么多杂乱的代码夹杂文字我们怎样把它提取出来整理呢？下面就开始介绍一个十分强大的工具，正则表达式！</p>\n<blockquote>\n<p>正则表达式是对字符串操作的一种逻辑公式，就是用事先定义好的一些特定字符、及这些特定字符的组合，组成一个“规则字符串”，这个“规则字符串”用来表达对字符串的一种过滤逻辑。</p>\n</blockquote>\n<p>正则表达式是用来匹配字符串非常强大的工具，在其他编程语言中同样有正则表达式的概念，Python 同样不例外，利用了正则表达式，我们想要从返回的页面内容提取出我们想要的内容就易如反掌了</p>\n<p><strong>规则</strong>：</p>\n<table>\n<thead>\n<tr>\n<th>模式</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>^</td>\n<td>匹配字符串的开头</td>\n</tr>\n<tr>\n<td>$</td>\n<td>匹配字符串的末尾</td>\n</tr>\n<tr>\n<td>.</td>\n<td>匹配任意字符，除了换行符，当 re.DOTALL 标记被指定时，则可以匹配包括换行符的任意字符</td>\n</tr>\n<tr>\n<td>[…]</td>\n<td>用来表示一组字符,单独列出：[amk] 匹配 ‘a’，‘m’或’k’</td>\n</tr>\n<tr>\n<td>[^…]</td>\n<td>不在[]中的字符：[^abc] 匹配除了 a,b,c 之外的字符</td>\n</tr>\n<tr>\n<td>re*</td>\n<td>匹配 0 个或多个的表达式</td>\n</tr>\n<tr>\n<td>re+</td>\n<td>匹配 1 个或多个的表达式</td>\n</tr>\n<tr>\n<td>re?</td>\n<td>匹配 0 个或 1 个由前面的正则表达式定义的片段，非贪婪方式</td>\n</tr>\n<tr>\n<td>re{ n}</td>\n<td></td>\n</tr>\n<tr>\n<td>re{ n,}</td>\n<td>精确匹配 n 个前面表达式</td>\n</tr>\n<tr>\n<td>re{ n, m}</td>\n<td>匹配 n 到 m 次由前面的正则表达式定义的片段，贪婪方式</td>\n</tr>\n<tr>\n<td>a</td>\n<td>b</td>\n</tr>\n<tr>\n<td>(re)</td>\n<td>G 匹配括号内的表达式，也表示一个组</td>\n</tr>\n<tr>\n<td>(?imx)</td>\n<td>正则表达式包含三种可选标志：i, m, 或 x 。只影响括号中的区域</td>\n</tr>\n<tr>\n<td>(?-imx)</td>\n<td>正则表达式关闭 i, m, 或 x 可选标志。只影响括号中的区域</td>\n</tr>\n<tr>\n<td>(?: re)</td>\n<td>类似 (…), 但是不表示一个组</td>\n</tr>\n<tr>\n<td>(?imx: re)</td>\n<td>在括号中使用 i, m, 或 x 可选标志</td>\n</tr>\n<tr>\n<td>(?-imx: re)</td>\n<td>在括号中不使用 i, m, 或 x 可选标志</td>\n</tr>\n<tr>\n<td>(?#…)</td>\n<td>注释</td>\n</tr>\n<tr>\n<td>(?= re)</td>\n<td>前向肯定界定符。如果所含正则表达式，以 … 表示，在当前位置成功匹配时成功，否则失败。但一旦所含表达式已经尝试，匹配引擎根本没有提高；模式的剩余部分还要尝试界定符的右边。</td>\n</tr>\n<tr>\n<td>(?! re)</td>\n<td>前向否定界定符。与肯定界定符相反；当所含表达式不能在字符串当前位置匹配时成功</td>\n</tr>\n<tr>\n<td>(?&gt; re)</td>\n<td>匹配的独立模式，省去回溯</td>\n</tr>\n<tr>\n<td>\\w</td>\n<td>匹配字母数字及下划线</td>\n</tr>\n<tr>\n<td>\\W</td>\n<td>匹配非字母数字及下划线</td>\n</tr>\n<tr>\n<td>\\s</td>\n<td>匹配任意空白字符，等价于 [\\t\\n\\r\\f].</td>\n</tr>\n<tr>\n<td>\\S</td>\n<td>匹配任意非空字符</td>\n</tr>\n<tr>\n<td>\\d</td>\n<td>匹配任意数字，等价于 [0-9]</td>\n</tr>\n<tr>\n<td>\\D</td>\n<td>匹配任意非数字</td>\n</tr>\n<tr>\n<td>\\A</td>\n<td>匹配字符串开始</td>\n</tr>\n<tr>\n<td>\\Z</td>\n<td>匹配字符串结束，如果是存在换行，只匹配到换行前的结束字符串。c</td>\n</tr>\n<tr>\n<td>\\z</td>\n<td>匹配字符串结束</td>\n</tr>\n<tr>\n<td>\\G</td>\n<td>匹配最后匹配完成的位置</td>\n</tr>\n<tr>\n<td>\\b</td>\n<td>匹配一个单词边界，也就是指单词和空格间的位置。例如， ‘er\\b’ 可以匹配&quot;never&quot; 中的 ‘er’，但不能匹配 “verb” 中的 ‘er’</td>\n</tr>\n<tr>\n<td>\\B</td>\n<td>匹配非单词边界。‘er\\B’ 能匹配 “verb” 中的 ‘er’，但不能匹配 “never” 中的 ‘er’</td>\n</tr>\n<tr>\n<td>\\n, \\t, 等.</td>\n<td>匹配一个换行符。匹配一个制表符。等</td>\n</tr>\n<tr>\n<td>\\1…\\9</td>\n<td>匹配第 n 个分组的内容</td>\n</tr>\n<tr>\n<td>\\10</td>\n<td>匹配第 n 个分组的内容，如果它经匹配。否则指的是八进制字符码的表达式</td>\n</tr>\n<tr>\n<td>[\\u4e00-\\u9fa5]</td>\n<td>中文</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"2.-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E7%9B%B8%E5%85%B3%E6%B3%A8%E8%A7%A3\">2. 正则表达式相关注解 <a class=\"heading-anchor-permalink\" href=\"#2.-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E7%9B%B8%E5%85%B3%E6%B3%A8%E8%A7%A3\">#</a></h3>\n<h4 id=\"2.1-%E6%95%B0%E9%87%8F%E8%AF%8D%E7%9A%84%E8%B4%AA%E5%A9%AA%E6%A8%A1%E5%BC%8F%E4%B8%8E%E9%9D%9E%E8%B4%AA%E5%A9%AA%E6%A8%A1%E5%BC%8F\">2.1 数量词的贪婪模式与非贪婪模式 <a class=\"heading-anchor-permalink\" href=\"#2.1-%E6%95%B0%E9%87%8F%E8%AF%8D%E7%9A%84%E8%B4%AA%E5%A9%AA%E6%A8%A1%E5%BC%8F%E4%B8%8E%E9%9D%9E%E8%B4%AA%E5%A9%AA%E6%A8%A1%E5%BC%8F\">#</a></h4>\n<p>正则表达式通常用于在文本中查找匹配的字符串\nPython 里数量词默认是贪婪的（在少数语言里也可能是默认非贪婪），总是尝试匹配尽可能多的字符；非贪婪的则相反，总是尝试匹配尽可能少的字符</p>\n<p>例如：正则表达式”ab*”如果用于查找”abbbc”，将找到”abbb”。而如果使用非贪婪的数量词”ab*?”，将找到”a”</p>\n<h4 id=\"2.2-%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95\">2.2 常用方法 <a class=\"heading-anchor-permalink\" href=\"#2.2-%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95\">#</a></h4>\n<ul>\n<li>re.match\n<ul>\n<li>re.match 尝试从字符串的起始位置匹配一个模式，如果不是起始位置匹配成功的话，match()就返回 none</li>\n<li>函数语法：\nre.match(pattern, string, flags=0)</li>\n</ul>\n</li>\n<li>re.search\n<ul>\n<li>re.search 扫描整个字符串并返回第一个成功的匹配。</li>\n<li>函数语法：\nre.search(pattern, string, flags=0)</li>\n</ul>\n</li>\n<li>re.sub\n<ul>\n<li>re.sub 替换字符串\nre.sub(pattern,replace,string)</li>\n</ul>\n</li>\n<li>re.findall\n<ul>\n<li>re.findall 查找全部\nre.findall(pattern,string,flags=0)</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3.-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E4%BF%AE%E9%A5%B0%E7%AC%A6---%E5%8F%AF%E9%80%89%E6%A0%87%E5%BF%97\">3. 正则表达式修饰符 - 可选标志 <a class=\"heading-anchor-permalink\" href=\"#3.-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E4%BF%AE%E9%A5%B0%E7%AC%A6---%E5%8F%AF%E9%80%89%E6%A0%87%E5%BF%97\">#</a></h3>\n<blockquote>\n<p>正则表达式可以包含一些可选标志修饰符来控制匹配的模式。修饰符被指定为一个可选的标志。多个标志可以通过按位 OR(|) 它们来指定。如 re.I | re.M 被设置成 I 和 M 标志：</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>修饰符</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>re.I</td>\n<td>使匹配对大小写不敏感</td>\n</tr>\n<tr>\n<td>re.L</td>\n<td>做本地化识别（locale-aware）匹配</td>\n</tr>\n<tr>\n<td>re.M</td>\n<td></td>\n</tr>\n<tr>\n<td>re.S</td>\n<td>使 . 匹配包括换行在内的所有字符</td>\n</tr>\n<tr>\n<td>re.U</td>\n<td>根据 Unicode 字符集解析字符。这个标志影响 \\w, \\W, \\b, \\B</td>\n</tr>\n<tr>\n<td>re.X</td>\n<td>该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解</td>\n</tr>\n</tbody>\n</table>\n",
      "id": 10
    },
    {
      "path": "数据提取与验证码的识别（上）/XPath.md",
      "url": "数据提取与验证码的识别（上）/XPath.html",
      "content": "### 1. 介绍\n\n> 之前 BeautifulSoup 的用法，这个已经是非常强大的库了，不过还有一些比较流行的解析库，例如 lxml，使用的是 Xpath 语法，同样是效率比较高的解析方法。如果大家对 BeautifulSoup 使用不太习惯的话，可以尝试下 Xpath\n\n[官网](http://lxml.de/index.html)\n\n[w3c](http://www.w3school.com.cn/xpath/index.asp)\n\n### 2. 安装\n\n```sh\npip install lxml\n```\n\n### 3. XPath 语法\n\n> XPath 是一门在 XML 文档中查找信息的语言。XPath 可用来在 XML 文档中对元素和属性进行遍历。XPath 是 W3C XSLT 标准的主要元素，并且 XQuery 和 XPointer 都构建于 XPath 表达之上\n\n#### 3.1 节点的关系\n\n- 父（Parent）\n- 子（Children）\n- 同胞（Sibling）\n- 先辈（Ancestor）\n- 后代（Descendant）\n\n#### 3.2 选取节点\n\n##### 3.2.1 常用的路径表达式\n\n| 表达式     | 描述                                                     |\n| ---------- | -------------------------------------------------------- |\n| `nodename` | 选取此节点的所有子节点                                   |\n| `/`        | 从根节点选取                                             |\n| `//`       | 从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置 |\n| `.`        | 选取当前节点                                             |\n| `..`       | 选取当前节点的父节点                                     |\n| `@`        | 选取属性                                                 |\n\n##### 3.2.2 通配符\n\nXPath 通配符可用来选取未知的 XML 元素。\n\n| 通配符   | 描述               | 举例               | 结果                      |\n| -------- | ------------------ | ------------------ | ------------------------- |\n| `_`      | 匹配任何元素节点   | `xpath('div/\\_')`  | 获取 div 下的所有子节点   |\n| `@_`     | 匹配任何属性节点   | `xpath('div[@_]')` | 选取所有带属性的 div 节点 |\n| `node()` | 匹配任何类型的节点 |                    |                           |\n\n##### 3.2.3 选取若干路径\n\n通过在路径表达式中使用“|”运算符，您可以选取若干个路径\n\n| 表达式                            | 结果                             |\n| --------------------------------- | -------------------------------- |\n| xpath('//div &#124; //table') | 获取所有的 `div` 与 `table` 节点 |\n\n##### 3.2.4 谓语\n\n谓语被嵌在方括号内，用来查找某个特定的节点或包含某个制定的值的节点\n\n| 表达式                              | 结果                                               |\n| ----------------------------------- | -------------------------------------------------- |\n| `xpath('/body/div[1]')`             | 选取 `body` 下的第一个 `div` 节点                  |\n| `xpath('/body/div[last()]')`        | 选取 `body` 下最后一个 `div` 节点                  |\n| `xpath('/body/div[last()-1]')`      | 选取 `body` 下倒数第二个节点                       |\n| `xpath('/body/div[positon()<3]')`   | 选取 `body` 下前丙个 `div` 节点                    |\n| `xpath('/body/div[@class]')`        | 选取 `body` 下带有 `class` 属性的 `div` 节点       |\n| `xpath('/body/div[@class=\"main\"]')` | 选取 `body` 下 `class` 属性为 `main` 的 `div` 节点 |\n| `xpath('/body/div[price>35.00]')`   | 选取 `body` 下 `price` 元素大于 `35` 的 `div` 节点 |\n\n##### 3.2.5 XPath 运算符\n\n| 运算符 | 描述           | 实例                      | 返回值                                                              |\n| ------ | -------------- | ------------------------- | ------------------------------------------------------------------- |\n|        | 计算两个节点集 | //book &#124; //cd        | 返回所有拥有 book 和 cd 元素的节点集                                |\n| +      | 加法           | 6 + 4                     | 10                                                                  |\n| –      | 减法           | 6 – 4                     | 2                                                                   |\n| \\*     | 乘法           | 6 \\* 4                    | 24                                                                  |\n| div    | 除法           | 8 div 4                   | 2                                                                   |\n| =      | 等于           | price=9.80                | 如果 price 是 9.80，则返回 true。如果 price 是 9.90，则返回 false。 |\n| !=     | 不等于         | price!=9.80               | 如果 price 是 9.90，则返回 true。如果 price 是 9.80，则返回 false。 |\n| <      | 小于           | price<9.80                | 如果 price 是 9.00，则返回 true。如果 price 是 9.90，则返回 false。 |\n| <=     | 小于或等于     | price<=9.80               | 如果 price 是 9.00，则返回 true。如果 price 是 9.90，则返回 false。 |\n| >      | 大于           | price>9.80                | 如果 price 是 9.90，则返回 true。如果 price 是 9.80，则返回 false。 |\n| >=     | 大于或等于     | price>=9.80               | 如果 price 是 9.90，则返回 true。如果 price 是 9.70，则返回 false。 |\n| or     | 或             | price=9.80 or price=9.70  | 如果 price 是 9.80，则返回 true。如果 price 是 9.50，则返回 false。 |\n| and    | 与             | price>9.00 and price<9.90 | 如果 price 是 9.80，则返回 true。如果 price 是 8.50，则返回 false。 |\n| mod    | 计算除法的余数 | 5 mod 2                   | 1                                                                   |\n\n#### 3.3 使用\n\n##### 3.3.1 小例子\n\n```python\nfrom lxml import etree\ntext = '''\n<div>\n    <ul>\n         <li class=\"item-0\"><a href=\"link1.html\">first item</a></li>\n         <li class=\"item-1\"><a href=\"link2.html\">second item</a></li>\n         <li class=\"item-inactive\"><a href=\"link3.html\">third item</a></li>\n         <li class=\"item-1\"><a href=\"link4.html\">fourth item</a></li>\n         <li class=\"item-0\"><a href=\"link5.html\">fifth item</a>\n     </ul>\n </div>\n'''\nhtml = etree.HTML(text)\nresult = etree.tostring(html)\nprint(result)\n```\n\n首先我们使用 lxml 的 etree 库，然后利用 etree.HTML 初始化，然后我们将其打印出来。\n\n其中，这里体现了 lxml 的一个非常实用的功能就是自动修正 html 代码，大家应该注意到了，最后一个 li 标签，其实我把尾标签删掉了，是不闭合的。不过，lxml 因为继承了 libxml2 的特性，具有自动修正 HTML 代码的功能。\n\n所以输出结果是这样的\n\n```html\n<html>\n  <body>\n    <div>\n      <ul>\n        <li class=\"item-0\"><a href=\"link1.html\">first item</a></li>\n        <li class=\"item-1\"><a href=\"link2.html\">second item</a></li>\n        <li class=\"item-inactive\"><a href=\"link3.html\">third item</a></li>\n        <li class=\"item-1\"><a href=\"link4.html\">fourth item</a></li>\n        <li class=\"item-0\"><a href=\"link5.html\">fifth item</a></li>\n      </ul>\n    </div>\n  </body>\n</html>\n```\n\n不仅补全了 li 标签，还添加了 body，html 标签。\n文件读取\n\n除了直接读取字符串，还支持从文件读取内容。比如我们新建一个文件叫做 hello.html，内容为\n\n```html\n<div>\n  <ul>\n    <li class=\"item-0\"><a href=\"link1.html\">first item</a></li>\n    <li class=\"item-1\"><a href=\"link2.html\">second item</a></li>\n    <li class=\"item-inactive\">\n      <a href=\"link3.html\"><span class=\"bold\">third item</span></a>\n    </li>\n    <li class=\"item-1\"><a href=\"link4.html\">fourth item</a></li>\n    <li class=\"item-0\"><a href=\"link5.html\">fifth item</a></li>\n  </ul>\n</div>\n```\n\n利用 parse 方法来读取文件\n\n```python\nfrom lxml import etree\nhtml = etree.parse('hello.html')\nresult = etree.tostring(html, pretty_print=True)\nprint(result)\n```\n\n同样可以得到相同的结果\n\n##### 3.3.2 XPath 具体使用\n\n依然以上一段程序为例\n\n1. 获取所有的 `<li>` 标签\n\n   ```python\n   from lxml import etree\n   html = etree.parse('hello.html')\n   print (type(html))\n   result = html.xpath('//li')\n   print (result)\n   print (len(result))\n   print (type(result))\n   print (type(result[0]))\n   ```\n\n   运行结果\n\n   ```text\n   <type 'lxml.etree._ElementTree'>\n   [<Element li at 0x1014e0e18>, <Element li at 0x1014e0ef0>, <Element li at 0x1014e0f38>, <Element li at 0x1014e0f80>, <Element li at 0x1014e0fc8>]\n\n   <type 'list'>\n   <type 'lxml.etree._Element'>\n   ```\n\n   可见，etree.parse 的类型是 ElementTree，通过调用 xpath 以后，得到了一个列表，包含了 5 个 `<li>` 元素，每个元素都是 Element 类型\n\n2. 获取`<li>`标签的所有 class\n\n   ```python\n   result = html.xpath('//li/@class')\n   print (result)\n   ```\n\n   运行结果\n\n   ```text\n   ['item-0', 'item-1', 'item-inactive', 'item-1', 'item-0']\n   ```\n\n3. 获取 `<li>` 标签下 href 为 link1.html 的 `<a>` 标签\n\n   ```python\n   result = html.xpath('//li/a[@href=\"link1.html\"]')\n   print (result)\n   ```\n\n   运行结果\n\n   ```text\n   [<Element a at 0x10ffaae18>]\n   ```\n\n4. 获取`<li>`标签下的所有 `<span>` 标签\n\n   **注意**: 这么写是不对的\n\n   ```python\n   result = html.xpath('//li/span')\n\n   #因为 / 是用来获取子元素的，而 <span> 并不是 <li> 的子元素，所以，要用双斜杠\n   result = html.xpath('//li//span')\n   print(result)\n   ```\n\n   运行结果\n\n   ```text\n   [<Element span at 0x10d698e18>]\n   ```\n\n5. 获取 `<li>` 标签下的所有 class，不包括`<li>`\n\n   ```python\n   result = html.xpath('//li/a//@class')\n   print (resul)t\n   #运行结果\n   ['blod']\n   ```\n\n6. 获取最后一个 `<li>` 的 `<a>` 的 href\n\n   ```python\n   result = html.xpath('//li[last()]/a/@href')\n   print (result)\n   ```\n\n   运行结果\n\n   ```text\n   ['link5.html']\n   ```\n\n7. 获取倒数第二个元素的内容\n\n   ```python\n   result = html.xpath('//li[last()-1]/a')\n   print (result[0].text)\n   ```\n\n   运行结果\n\n   ```text\n   fourth item\n   ```\n\n8. 获取 class 为 bold 的标签名\n\n   ```python\n   result = html.xpath('//*[@class=\"bold\"]')\n   print (result[0].tag)\n   ```\n\n   运行结果\n\n   ```text\n   span\n   ```\n\n#### 选择 XML 文件中节点\n\n- element（元素节点）\n- attribute（属性节点）\n- text （文本节点）\n- concat(元素节点,元素节点)\n- comment （注释节点）\n- root （根节点）\n",
      "html": "<h3 id=\"1.-%E4%BB%8B%E7%BB%8D\">1. 介绍 <a class=\"heading-anchor-permalink\" href=\"#1.-%E4%BB%8B%E7%BB%8D\">#</a></h3>\n<blockquote>\n<p>之前 BeautifulSoup 的用法，这个已经是非常强大的库了，不过还有一些比较流行的解析库，例如 lxml，使用的是 Xpath 语法，同样是效率比较高的解析方法。如果大家对 BeautifulSoup 使用不太习惯的话，可以尝试下 Xpath</p>\n</blockquote>\n<p><a href=\"http://lxml.de/index.html\">官网</a></p>\n<p><a href=\"http://www.w3school.com.cn/xpath/index.asp\">w3c</a></p>\n<h3 id=\"2.-%E5%AE%89%E8%A3%85\">2. 安装 <a class=\"heading-anchor-permalink\" href=\"#2.-%E5%AE%89%E8%A3%85\">#</a></h3>\n<pre><code class=\"language-sh\">pip install lxml\n</code></pre>\n<h3 id=\"3.-xpath-%E8%AF%AD%E6%B3%95\">3. XPath 语法 <a class=\"heading-anchor-permalink\" href=\"#3.-xpath-%E8%AF%AD%E6%B3%95\">#</a></h3>\n<blockquote>\n<p>XPath 是一门在 XML 文档中查找信息的语言。XPath 可用来在 XML 文档中对元素和属性进行遍历。XPath 是 W3C XSLT 标准的主要元素，并且 XQuery 和 XPointer 都构建于 XPath 表达之上</p>\n</blockquote>\n<h4 id=\"3.1-%E8%8A%82%E7%82%B9%E7%9A%84%E5%85%B3%E7%B3%BB\">3.1 节点的关系 <a class=\"heading-anchor-permalink\" href=\"#3.1-%E8%8A%82%E7%82%B9%E7%9A%84%E5%85%B3%E7%B3%BB\">#</a></h4>\n<ul>\n<li>父（Parent）</li>\n<li>子（Children）</li>\n<li>同胞（Sibling）</li>\n<li>先辈（Ancestor）</li>\n<li>后代（Descendant）</li>\n</ul>\n<h4 id=\"3.2-%E9%80%89%E5%8F%96%E8%8A%82%E7%82%B9\">3.2 选取节点 <a class=\"heading-anchor-permalink\" href=\"#3.2-%E9%80%89%E5%8F%96%E8%8A%82%E7%82%B9\">#</a></h4>\n<h5 id=\"3.2.1-%E5%B8%B8%E7%94%A8%E7%9A%84%E8%B7%AF%E5%BE%84%E8%A1%A8%E8%BE%BE%E5%BC%8F\">3.2.1 常用的路径表达式 <a class=\"heading-anchor-permalink\" href=\"#3.2.1-%E5%B8%B8%E7%94%A8%E7%9A%84%E8%B7%AF%E5%BE%84%E8%A1%A8%E8%BE%BE%E5%BC%8F\">#</a></h5>\n<table>\n<thead>\n<tr>\n<th>表达式</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>nodename</code></td>\n<td>选取此节点的所有子节点</td>\n</tr>\n<tr>\n<td><code>/</code></td>\n<td>从根节点选取</td>\n</tr>\n<tr>\n<td><code>//</code></td>\n<td>从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置</td>\n</tr>\n<tr>\n<td><code>.</code></td>\n<td>选取当前节点</td>\n</tr>\n<tr>\n<td><code>..</code></td>\n<td>选取当前节点的父节点</td>\n</tr>\n<tr>\n<td><code>@</code></td>\n<td>选取属性</td>\n</tr>\n</tbody>\n</table>\n<h5 id=\"3.2.2-%E9%80%9A%E9%85%8D%E7%AC%A6\">3.2.2 通配符 <a class=\"heading-anchor-permalink\" href=\"#3.2.2-%E9%80%9A%E9%85%8D%E7%AC%A6\">#</a></h5>\n<p>XPath 通配符可用来选取未知的 XML 元素。</p>\n<table>\n<thead>\n<tr>\n<th>通配符</th>\n<th>描述</th>\n<th>举例</th>\n<th>结果</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>_</code></td>\n<td>匹配任何元素节点</td>\n<td><code>xpath('div/\\_')</code></td>\n<td>获取 div 下的所有子节点</td>\n</tr>\n<tr>\n<td><code>@_</code></td>\n<td>匹配任何属性节点</td>\n<td><code>xpath('div[@_]')</code></td>\n<td>选取所有带属性的 div 节点</td>\n</tr>\n<tr>\n<td><code>node()</code></td>\n<td>匹配任何类型的节点</td>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<h5 id=\"3.2.3-%E9%80%89%E5%8F%96%E8%8B%A5%E5%B9%B2%E8%B7%AF%E5%BE%84\">3.2.3 选取若干路径 <a class=\"heading-anchor-permalink\" href=\"#3.2.3-%E9%80%89%E5%8F%96%E8%8B%A5%E5%B9%B2%E8%B7%AF%E5%BE%84\">#</a></h5>\n<p>通过在路径表达式中使用“|”运算符，您可以选取若干个路径</p>\n<table>\n<thead>\n<tr>\n<th>表达式</th>\n<th>结果</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>xpath(’//div | //table’)</td>\n<td>获取所有的 <code>div</code> 与 <code>table</code> 节点</td>\n</tr>\n</tbody>\n</table>\n<h5 id=\"3.2.4-%E8%B0%93%E8%AF%AD\">3.2.4 谓语 <a class=\"heading-anchor-permalink\" href=\"#3.2.4-%E8%B0%93%E8%AF%AD\">#</a></h5>\n<p>谓语被嵌在方括号内，用来查找某个特定的节点或包含某个制定的值的节点</p>\n<table>\n<thead>\n<tr>\n<th>表达式</th>\n<th>结果</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>xpath('/body/div[1]')</code></td>\n<td>选取 <code>body</code> 下的第一个 <code>div</code> 节点</td>\n</tr>\n<tr>\n<td><code>xpath('/body/div[last()]')</code></td>\n<td>选取 <code>body</code> 下最后一个 <code>div</code> 节点</td>\n</tr>\n<tr>\n<td><code>xpath('/body/div[last()-1]')</code></td>\n<td>选取 <code>body</code> 下倒数第二个节点</td>\n</tr>\n<tr>\n<td><code>xpath('/body/div[positon()&lt;3]')</code></td>\n<td>选取 <code>body</code> 下前丙个 <code>div</code> 节点</td>\n</tr>\n<tr>\n<td><code>xpath('/body/div[@class]')</code></td>\n<td>选取 <code>body</code> 下带有 <code>class</code> 属性的 <code>div</code> 节点</td>\n</tr>\n<tr>\n<td><code>xpath('/body/div[@class=&quot;main&quot;]')</code></td>\n<td>选取 <code>body</code> 下 <code>class</code> 属性为 <code>main</code> 的 <code>div</code> 节点</td>\n</tr>\n<tr>\n<td><code>xpath('/body/div[price&gt;35.00]')</code></td>\n<td>选取 <code>body</code> 下 <code>price</code> 元素大于 <code>35</code> 的 <code>div</code> 节点</td>\n</tr>\n</tbody>\n</table>\n<h5 id=\"3.2.5-xpath-%E8%BF%90%E7%AE%97%E7%AC%A6\">3.2.5 XPath 运算符 <a class=\"heading-anchor-permalink\" href=\"#3.2.5-xpath-%E8%BF%90%E7%AE%97%E7%AC%A6\">#</a></h5>\n<table>\n<thead>\n<tr>\n<th>运算符</th>\n<th>描述</th>\n<th>实例</th>\n<th>返回值</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>计算两个节点集</td>\n<td>//book | //cd</td>\n<td>返回所有拥有 book 和 cd 元素的节点集</td>\n</tr>\n<tr>\n<td>+</td>\n<td>加法</td>\n<td>6 + 4</td>\n<td>10</td>\n</tr>\n<tr>\n<td>–</td>\n<td>减法</td>\n<td>6 – 4</td>\n<td>2</td>\n</tr>\n<tr>\n<td>*</td>\n<td>乘法</td>\n<td>6 * 4</td>\n<td>24</td>\n</tr>\n<tr>\n<td>div</td>\n<td>除法</td>\n<td>8 div 4</td>\n<td>2</td>\n</tr>\n<tr>\n<td>=</td>\n<td>等于</td>\n<td>price=9.80</td>\n<td>如果 price 是 9.80，则返回 true。如果 price 是 9.90，则返回 false。</td>\n</tr>\n<tr>\n<td>!=</td>\n<td>不等于</td>\n<td>price!=9.80</td>\n<td>如果 price 是 9.90，则返回 true。如果 price 是 9.80，则返回 false。</td>\n</tr>\n<tr>\n<td>&lt;</td>\n<td>小于</td>\n<td>price&lt;9.80</td>\n<td>如果 price 是 9.00，则返回 true。如果 price 是 9.90，则返回 false。</td>\n</tr>\n<tr>\n<td>&lt;=</td>\n<td>小于或等于</td>\n<td>price&lt;=9.80</td>\n<td>如果 price 是 9.00，则返回 true。如果 price 是 9.90，则返回 false。</td>\n</tr>\n<tr>\n<td>&gt;</td>\n<td>大于</td>\n<td>price&gt;9.80</td>\n<td>如果 price 是 9.90，则返回 true。如果 price 是 9.80，则返回 false。</td>\n</tr>\n<tr>\n<td>&gt;=</td>\n<td>大于或等于</td>\n<td>price&gt;=9.80</td>\n<td>如果 price 是 9.90，则返回 true。如果 price 是 9.70，则返回 false。</td>\n</tr>\n<tr>\n<td>or</td>\n<td>或</td>\n<td>price=9.80 or price=9.70</td>\n<td>如果 price 是 9.80，则返回 true。如果 price 是 9.50，则返回 false。</td>\n</tr>\n<tr>\n<td>and</td>\n<td>与</td>\n<td>price&gt;9.00 and price&lt;9.90</td>\n<td>如果 price 是 9.80，则返回 true。如果 price 是 8.50，则返回 false。</td>\n</tr>\n<tr>\n<td>mod</td>\n<td>计算除法的余数</td>\n<td>5 mod 2</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"3.3-%E4%BD%BF%E7%94%A8\">3.3 使用 <a class=\"heading-anchor-permalink\" href=\"#3.3-%E4%BD%BF%E7%94%A8\">#</a></h4>\n<h5 id=\"3.3.1-%E5%B0%8F%E4%BE%8B%E5%AD%90\">3.3.1 小例子 <a class=\"heading-anchor-permalink\" href=\"#3.3.1-%E5%B0%8F%E4%BE%8B%E5%AD%90\">#</a></h5>\n<pre><code class=\"language-python\">from lxml import etree\ntext = '''\n&lt;div&gt;\n    &lt;ul&gt;\n         &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link1.html&quot;&gt;first item&lt;/a&gt;&lt;/li&gt;\n         &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;\n         &lt;li class=&quot;item-inactive&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;third item&lt;/a&gt;&lt;/li&gt;\n         &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;\n         &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;\n     &lt;/ul&gt;\n &lt;/div&gt;\n'''\nhtml = etree.HTML(text)\nresult = etree.tostring(html)\nprint(result)\n</code></pre>\n<p>首先我们使用 lxml 的 etree 库，然后利用 etree.HTML 初始化，然后我们将其打印出来。</p>\n<p>其中，这里体现了 lxml 的一个非常实用的功能就是自动修正 html 代码，大家应该注意到了，最后一个 li 标签，其实我把尾标签删掉了，是不闭合的。不过，lxml 因为继承了 libxml2 的特性，具有自动修正 HTML 代码的功能。</p>\n<p>所以输出结果是这样的</p>\n<pre><code class=\"language-html\">&lt;html&gt;\n  &lt;body&gt;\n    &lt;div&gt;\n      &lt;ul&gt;\n        &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link1.html&quot;&gt;first item&lt;/a&gt;&lt;/li&gt;\n        &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;\n        &lt;li class=&quot;item-inactive&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;third item&lt;/a&gt;&lt;/li&gt;\n        &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;\n        &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;\n      &lt;/ul&gt;\n    &lt;/div&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n<p>不仅补全了 li 标签，还添加了 body，html 标签。\n文件读取</p>\n<p>除了直接读取字符串，还支持从文件读取内容。比如我们新建一个文件叫做 hello.html，内容为</p>\n<pre><code class=\"language-html\">&lt;div&gt;\n  &lt;ul&gt;\n    &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link1.html&quot;&gt;first item&lt;/a&gt;&lt;/li&gt;\n    &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;\n    &lt;li class=&quot;item-inactive&quot;&gt;\n      &lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;\n    &lt;/li&gt;\n    &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;\n    &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/div&gt;\n</code></pre>\n<p>利用 parse 方法来读取文件</p>\n<pre><code class=\"language-python\">from lxml import etree\nhtml = etree.parse('hello.html')\nresult = etree.tostring(html, pretty_print=True)\nprint(result)\n</code></pre>\n<p>同样可以得到相同的结果</p>\n<h5 id=\"3.3.2-xpath-%E5%85%B7%E4%BD%93%E4%BD%BF%E7%94%A8\">3.3.2 XPath 具体使用 <a class=\"heading-anchor-permalink\" href=\"#3.3.2-xpath-%E5%85%B7%E4%BD%93%E4%BD%BF%E7%94%A8\">#</a></h5>\n<p>依然以上一段程序为例</p>\n<ol>\n<li>\n<p>获取所有的 <code>&lt;li&gt;</code> 标签</p>\n<pre><code class=\"language-python\">from lxml import etree\nhtml = etree.parse('hello.html')\nprint (type(html))\nresult = html.xpath('//li')\nprint (result)\nprint (len(result))\nprint (type(result))\nprint (type(result[0]))\n</code></pre>\n<p>运行结果</p>\n<pre><code class=\"language-text\">&lt;type 'lxml.etree._ElementTree'&gt;\n[&lt;Element li at 0x1014e0e18&gt;, &lt;Element li at 0x1014e0ef0&gt;, &lt;Element li at 0x1014e0f38&gt;, &lt;Element li at 0x1014e0f80&gt;, &lt;Element li at 0x1014e0fc8&gt;]\n\n&lt;type 'list'&gt;\n&lt;type 'lxml.etree._Element'&gt;\n</code></pre>\n<p>可见，etree.parse 的类型是 ElementTree，通过调用 xpath 以后，得到了一个列表，包含了 5 个 <code>&lt;li&gt;</code> 元素，每个元素都是 Element 类型</p>\n</li>\n<li>\n<p>获取<code>&lt;li&gt;</code>标签的所有 class</p>\n<pre><code class=\"language-python\">result = html.xpath('//li/@class')\nprint (result)\n</code></pre>\n<p>运行结果</p>\n<pre><code class=\"language-text\">['item-0', 'item-1', 'item-inactive', 'item-1', 'item-0']\n</code></pre>\n</li>\n<li>\n<p>获取 <code>&lt;li&gt;</code> 标签下 href 为 link1.html 的 <code>&lt;a&gt;</code> 标签</p>\n<pre><code class=\"language-python\">result = html.xpath('//li/a[@href=&quot;link1.html&quot;]')\nprint (result)\n</code></pre>\n<p>运行结果</p>\n<pre><code class=\"language-text\">[&lt;Element a at 0x10ffaae18&gt;]\n</code></pre>\n</li>\n<li>\n<p>获取<code>&lt;li&gt;</code>标签下的所有 <code>&lt;span&gt;</code> 标签</p>\n<p><strong>注意</strong>: 这么写是不对的</p>\n<pre><code class=\"language-python\">result = html.xpath('//li/span')\n\n#因为 / 是用来获取子元素的，而 &lt;span&gt; 并不是 &lt;li&gt; 的子元素，所以，要用双斜杠\nresult = html.xpath('//li//span')\nprint(result)\n</code></pre>\n<p>运行结果</p>\n<pre><code class=\"language-text\">[&lt;Element span at 0x10d698e18&gt;]\n</code></pre>\n</li>\n<li>\n<p>获取 <code>&lt;li&gt;</code> 标签下的所有 class，不包括<code>&lt;li&gt;</code></p>\n<pre><code class=\"language-python\">result = html.xpath('//li/a//@class')\nprint (resul)t\n#运行结果\n['blod']\n</code></pre>\n</li>\n<li>\n<p>获取最后一个 <code>&lt;li&gt;</code> 的 <code>&lt;a&gt;</code> 的 href</p>\n<pre><code class=\"language-python\">result = html.xpath('//li[last()]/a/@href')\nprint (result)\n</code></pre>\n<p>运行结果</p>\n<pre><code class=\"language-text\">['link5.html']\n</code></pre>\n</li>\n<li>\n<p>获取倒数第二个元素的内容</p>\n<pre><code class=\"language-python\">result = html.xpath('//li[last()-1]/a')\nprint (result[0].text)\n</code></pre>\n<p>运行结果</p>\n<pre><code class=\"language-text\">fourth item\n</code></pre>\n</li>\n<li>\n<p>获取 class 为 bold 的标签名</p>\n<pre><code class=\"language-python\">result = html.xpath('//*[@class=&quot;bold&quot;]')\nprint (result[0].tag)\n</code></pre>\n<p>运行结果</p>\n<pre><code class=\"language-text\">span\n</code></pre>\n</li>\n</ol>\n<h4 id=\"%E9%80%89%E6%8B%A9-xml-%E6%96%87%E4%BB%B6%E4%B8%AD%E8%8A%82%E7%82%B9\">选择 XML 文件中节点 <a class=\"heading-anchor-permalink\" href=\"#%E9%80%89%E6%8B%A9-xml-%E6%96%87%E4%BB%B6%E4%B8%AD%E8%8A%82%E7%82%B9\">#</a></h4>\n<ul>\n<li>element（元素节点）</li>\n<li>attribute（属性节点）</li>\n<li>text （文本节点）</li>\n<li>concat(元素节点,元素节点)</li>\n<li>comment （注释节点）</li>\n<li>root （根节点）</li>\n</ul>\n",
      "id": 11
    },
    {
      "path": "数据提取与验证码的识别（下）/Python下Tesseract Ocr引擎及安装介绍.md",
      "url": "数据提取与验证码的识别（下）/Python下Tesseract Ocr引擎及安装介绍.html",
      "content": "### 1. Tesseract 介绍\n\ntesseract 是一个 google 支持的开源 ocr 项目\n\n> [GitHub 项目](https://github.com/tesseract-ocr/tesseract)\n\n目前最新的源码可以在这里下载\n\n### 2. Tesseract 安装包下载\n\n[Tesseract 的 release 版本下载](https://github.com/tesseract-ocr/tesseract/wiki/Downloads)，这里需要注意这一段话：\n\n> Currently, there is no official Windows installer for newer versions\n\n意思就是官方不提供最新版 windows 平台安装包，只有相对略老的`3.02.02`版本，[下载地址](https://sourceforge.net/projects/tesseract-ocr-alt/files/)\n\n最新版 `3.03` 和 `3.05` 版本，都是三方维护和管理的安装包，有好几个发行机构，分别是：\n\n- https://www.dropbox.com/s/8t54mz39i58qslh/tesseract-3.05.00dev-win32-vc19.zip?dl=1\n- https://github.com/UB-Mannheim/tesseract/wiki\n- http://domasofan.spdns.eu/tesseract/\n\n### 3. 小结\n\n1. [官方发布的 3.02 版本下载地址](http://downloads.sourceforge.net/project/tesseract-ocr-alt/tesseract-ocr-setup-3.02.02.exe?r=https%3A%2F%2Fsourceforge.net%2Fprojects%2Ftesseract-ocr-alt%2Ffiles%2F&ts=1464880498&use_mirror=jaist)\n\n2. [德国曼海姆大学发行的 3.05 版本下载地址](http://digi.bib.uni-mannheim.de/tesseract/tesseract-ocr-setup-3.05.00dev.exe)\n\n3. [imon Eigeldinger (@DomasoFan) 维护的另一个版本](http://3.onj.me/tesseract/)\n   值得称道的是，这个网址里还有一个比较详细的说明\n\n### 4. Tesseract ocr 使用\n\n安装之后，默认目录`C:\\Program Files (x86)\\Tesseract-OCR`，你需要把这个路径放到你操作系统的 path 搜索路径中，否则后面使用起来会不方便。\n\n在安装目录`C:\\Program Files (x86)\\Tesseract-OCR`下可以看到 `tesseract.exe`这个命令行执行程序\n\n```sh\ntesseract 1.png output -l eng -psm 7\n```\n\n`-psm 7` 表示用单行文本识别\n\npagesegmode 值：\n\n- 0 =定向和脚本检测（OSD）。\n- 1 =带 OSD 的自动页面分割。\n- 2 =自动页面分割，但没有 OSD 或 OCR\n- 3 =全自动页面分割，但没有 OSD。（默认）\n- 4 =假设一列可变大小的文本。\n- 5 =假设一个统一的垂直对齐文本块。\n- 6 =假设一个统一的文本块。\n- 7 =将图像作为单个文本行处理。\n- 8 =把图像当作一个单词。\n- 9 =把图像当作一个圆圈中的一个词来对待。\n- 10 =将图像作为单个字符处理\n\n`-l eng` 代表使用英语识别\n",
      "html": "<h3 id=\"1.-tesseract-%E4%BB%8B%E7%BB%8D\">1. Tesseract 介绍 <a class=\"heading-anchor-permalink\" href=\"#1.-tesseract-%E4%BB%8B%E7%BB%8D\">#</a></h3>\n<p>tesseract 是一个 google 支持的开源 ocr 项目</p>\n<blockquote>\n<p><a href=\"https://github.com/tesseract-ocr/tesseract\">GitHub 项目</a></p>\n</blockquote>\n<p>目前最新的源码可以在这里下载</p>\n<h3 id=\"2.-tesseract-%E5%AE%89%E8%A3%85%E5%8C%85%E4%B8%8B%E8%BD%BD\">2. Tesseract 安装包下载 <a class=\"heading-anchor-permalink\" href=\"#2.-tesseract-%E5%AE%89%E8%A3%85%E5%8C%85%E4%B8%8B%E8%BD%BD\">#</a></h3>\n<p><a href=\"https://github.com/tesseract-ocr/tesseract/wiki/Downloads\">Tesseract 的 release 版本下载</a>，这里需要注意这一段话：</p>\n<blockquote>\n<p>Currently, there is no official Windows installer for newer versions</p>\n</blockquote>\n<p>意思就是官方不提供最新版 windows 平台安装包，只有相对略老的<code>3.02.02</code>版本，<a href=\"https://sourceforge.net/projects/tesseract-ocr-alt/files/\">下载地址</a></p>\n<p>最新版 <code>3.03</code> 和 <code>3.05</code> 版本，都是三方维护和管理的安装包，有好几个发行机构，分别是：</p>\n<ul>\n<li><a href=\"https://www.dropbox.com/s/8t54mz39i58qslh/tesseract-3.05.00dev-win32-vc19.zip?dl=1\">https://www.dropbox.com/s/8t54mz39i58qslh/tesseract-3.05.00dev-win32-vc19.zip?dl=1</a></li>\n<li><a href=\"https://github.com/UB-Mannheim/tesseract/wiki\">https://github.com/UB-Mannheim/tesseract/wiki</a></li>\n<li><a href=\"http://domasofan.spdns.eu/tesseract/\">http://domasofan.spdns.eu/tesseract/</a></li>\n</ul>\n<h3 id=\"3.-%E5%B0%8F%E7%BB%93\">3. 小结 <a class=\"heading-anchor-permalink\" href=\"#3.-%E5%B0%8F%E7%BB%93\">#</a></h3>\n<ol>\n<li>\n<p><a href=\"http://downloads.sourceforge.net/project/tesseract-ocr-alt/tesseract-ocr-setup-3.02.02.exe?r=https%3A%2F%2Fsourceforge.net%2Fprojects%2Ftesseract-ocr-alt%2Ffiles%2F&amp;ts=1464880498&amp;use_mirror=jaist\">官方发布的 3.02 版本下载地址</a></p>\n</li>\n<li>\n<p><a href=\"http://digi.bib.uni-mannheim.de/tesseract/tesseract-ocr-setup-3.05.00dev.exe\">德国曼海姆大学发行的 3.05 版本下载地址</a></p>\n</li>\n<li>\n<p><a href=\"http://3.onj.me/tesseract/\">imon Eigeldinger (@DomasoFan) 维护的另一个版本</a>\n值得称道的是，这个网址里还有一个比较详细的说明</p>\n</li>\n</ol>\n<h3 id=\"4.-tesseract-ocr-%E4%BD%BF%E7%94%A8\">4. Tesseract ocr 使用 <a class=\"heading-anchor-permalink\" href=\"#4.-tesseract-ocr-%E4%BD%BF%E7%94%A8\">#</a></h3>\n<p>安装之后，默认目录<code>C:\\Program Files (x86)\\Tesseract-OCR</code>，你需要把这个路径放到你操作系统的 path 搜索路径中，否则后面使用起来会不方便。</p>\n<p>在安装目录<code>C:\\Program Files (x86)\\Tesseract-OCR</code>下可以看到 <code>tesseract.exe</code>这个命令行执行程序</p>\n<pre><code class=\"language-sh\">tesseract 1.png output -l eng -psm 7\n</code></pre>\n<p><code>-psm 7</code> 表示用单行文本识别</p>\n<p>pagesegmode 值：</p>\n<ul>\n<li>0 =定向和脚本检测（OSD）。</li>\n<li>1 =带 OSD 的自动页面分割。</li>\n<li>2 =自动页面分割，但没有 OSD 或 OCR</li>\n<li>3 =全自动页面分割，但没有 OSD。（默认）</li>\n<li>4 =假设一列可变大小的文本。</li>\n<li>5 =假设一个统一的垂直对齐文本块。</li>\n<li>6 =假设一个统一的文本块。</li>\n<li>7 =将图像作为单个文本行处理。</li>\n<li>8 =把图像当作一个单词。</li>\n<li>9 =把图像当作一个圆圈中的一个词来对待。</li>\n<li>10 =将图像作为单个字符处理</li>\n</ul>\n<p><code>-l eng</code> 代表使用英语识别</p>\n",
      "id": 12
    },
    {
      "path": "数据提取与验证码的识别（下）/Selenium 处理滚动条.md",
      "url": "数据提取与验证码的识别（下）/Selenium 处理滚动条.html",
      "content": "#### Selenium 处理滚动条\n\n> selenium 并不是万能的，有时候页面上操作无法实现的，这时候就需要借助 JS 来完成了\n\n当页面上的元素超过一屏后，想操作屏幕下方的元素，是不能直接定位到，会报元素不可见的。这时候需要借助滚动条来拖动屏幕，使被操作的元素显示在当前的屏幕上。滚动条是无法直接用定位工具来定位的。selenium 里面也没有直接的方法去控制滚动条，这时候只能借助 J 了，还好 selenium 提供了一个操作 js 的方法:execute_script()，可以直接执行 js 的脚本\n\n##### 一. 控制滚动条高度\n\n###### 1.1 滚动条回到顶部\n\n```python\njs=\"var q=document.getElementById('id').scrollTop=0\"\ndriver.execute_script(js)\n```\n\n###### 1.2 滚动条拉到底部\n\n```python\njs=\"var q=document.documentElement.scrollTop=10000\"\ndriver.execute_script(js)\n```\n\n可以修改 scrollTop 的值，来定位右侧滚动条的位置，0 是最上面，10000 是最底部\n\n以上方法在 Firefox 和 IE 浏览器上上是可以的，但是用 Chrome 浏览器，发现不管用。Chrome 浏览器解决办法：\n\n```python\njs = \"var q=document.body.scrollTop=0\"\ndriver.execute_script(js)\n```\n\n##### 二.横向滚动条\n\n###### 2.1 有时候浏览器页面需要左右滚动（一般屏幕最大化后，左右滚动的情况已经很少见了)\n\n###### 2.2 通过左边控制横向和纵向滚动条 scrollTo(x, y)\n\n```python\njs = \"window.scrollTo(100,400)\"\ndriver.execute_script(js)\n```\n\n##### 三.元素聚焦\n\n虽然用上面的方法可以解决拖动滚动条的位置问题，但是有时候无法确定我需要操作的元素在什么位置，有可能每次打开的页面不一样，元素所在的位置也不一样，怎么办呢？这个时候我们可以先让页面直接跳到元素出现的位置，然后就可以操作了\n\n同样需要借助 JS 去实现。 具体如下：\n\n```python\ntarget = driver.find_element_by_xxxx()\ndriver.execute_script(\"arguments[0].scrollIntoView();\", target)\n```\n\n##### 四. 参考代码\n\n```python\nfrom selenium import webdriver\nfrom lxml import etree\nimport time\n\nurl = \"https://search.jd.com/Search?keyword=%E7%AC%94%E8%AE%B0%E6%9C%AC&enc=utf-8&wq=%E7%AC%94%E8%AE%B0%E6%9C%AC&pvid=845d019c94f6476ca5c4ffc24df6865a\"\n# 加载浏览器\nwd = webdriver.Firefox()\n# 发送请求\nwd.get(url)\n# 要执行的js\njs = \"var q = document.documentElement.scrollTop=10000\"\n# 执行js\nwd.execute_script(js)\n\ntime.sleep(3)\n# 解析数据\ne = etree.HTML(wd.page_source)\n# 提取数据的xpath\nprice_xpath = '//ul[@class=\"gl-warp clearfix\"]//div[@class=\"p-price\"]/strong/i/text()'\n# 提取数据的\ninfos = e.xpath(price_xpath)\n\nprint(len(infos))\n# 关闭浏览器\nwd.quit()\n```\n",
      "html": "<h4 id=\"selenium-%E5%A4%84%E7%90%86%E6%BB%9A%E5%8A%A8%E6%9D%A1\">Selenium 处理滚动条 <a class=\"heading-anchor-permalink\" href=\"#selenium-%E5%A4%84%E7%90%86%E6%BB%9A%E5%8A%A8%E6%9D%A1\">#</a></h4>\n<blockquote>\n<p>selenium 并不是万能的，有时候页面上操作无法实现的，这时候就需要借助 JS 来完成了</p>\n</blockquote>\n<p>当页面上的元素超过一屏后，想操作屏幕下方的元素，是不能直接定位到，会报元素不可见的。这时候需要借助滚动条来拖动屏幕，使被操作的元素显示在当前的屏幕上。滚动条是无法直接用定位工具来定位的。selenium 里面也没有直接的方法去控制滚动条，这时候只能借助 J 了，还好 selenium 提供了一个操作 js 的方法:execute_script()，可以直接执行 js 的脚本</p>\n<h5 id=\"%E4%B8%80.-%E6%8E%A7%E5%88%B6%E6%BB%9A%E5%8A%A8%E6%9D%A1%E9%AB%98%E5%BA%A6\">一. 控制滚动条高度 <a class=\"heading-anchor-permalink\" href=\"#%E4%B8%80.-%E6%8E%A7%E5%88%B6%E6%BB%9A%E5%8A%A8%E6%9D%A1%E9%AB%98%E5%BA%A6\">#</a></h5>\n<h6 id=\"1.1-%E6%BB%9A%E5%8A%A8%E6%9D%A1%E5%9B%9E%E5%88%B0%E9%A1%B6%E9%83%A8\">1.1 滚动条回到顶部 <a class=\"heading-anchor-permalink\" href=\"#1.1-%E6%BB%9A%E5%8A%A8%E6%9D%A1%E5%9B%9E%E5%88%B0%E9%A1%B6%E9%83%A8\">#</a></h6>\n<pre><code class=\"language-python\">js=&quot;var q=document.getElementById('id').scrollTop=0&quot;\ndriver.execute_script(js)\n</code></pre>\n<h6 id=\"1.2-%E6%BB%9A%E5%8A%A8%E6%9D%A1%E6%8B%89%E5%88%B0%E5%BA%95%E9%83%A8\">1.2 滚动条拉到底部 <a class=\"heading-anchor-permalink\" href=\"#1.2-%E6%BB%9A%E5%8A%A8%E6%9D%A1%E6%8B%89%E5%88%B0%E5%BA%95%E9%83%A8\">#</a></h6>\n<pre><code class=\"language-python\">js=&quot;var q=document.documentElement.scrollTop=10000&quot;\ndriver.execute_script(js)\n</code></pre>\n<p>可以修改 scrollTop 的值，来定位右侧滚动条的位置，0 是最上面，10000 是最底部</p>\n<p>以上方法在 Firefox 和 IE 浏览器上上是可以的，但是用 Chrome 浏览器，发现不管用。Chrome 浏览器解决办法：</p>\n<pre><code class=\"language-python\">js = &quot;var q=document.body.scrollTop=0&quot;\ndriver.execute_script(js)\n</code></pre>\n<h5 id=\"%E4%BA%8C.%E6%A8%AA%E5%90%91%E6%BB%9A%E5%8A%A8%E6%9D%A1\">二.横向滚动条 <a class=\"heading-anchor-permalink\" href=\"#%E4%BA%8C.%E6%A8%AA%E5%90%91%E6%BB%9A%E5%8A%A8%E6%9D%A1\">#</a></h5>\n<h6 id=\"2.1-%E6%9C%89%E6%97%B6%E5%80%99%E6%B5%8F%E8%A7%88%E5%99%A8%E9%A1%B5%E9%9D%A2%E9%9C%80%E8%A6%81%E5%B7%A6%E5%8F%B3%E6%BB%9A%E5%8A%A8%EF%BC%88%E4%B8%80%E8%88%AC%E5%B1%8F%E5%B9%95%E6%9C%80%E5%A4%A7%E5%8C%96%E5%90%8E%EF%BC%8C%E5%B7%A6%E5%8F%B3%E6%BB%9A%E5%8A%A8%E7%9A%84%E6%83%85%E5%86%B5%E5%B7%B2%E7%BB%8F%E5%BE%88%E5%B0%91%E8%A7%81%E4%BA%86)\">2.1 有时候浏览器页面需要左右滚动（一般屏幕最大化后，左右滚动的情况已经很少见了) <a class=\"heading-anchor-permalink\" href=\"#2.1-%E6%9C%89%E6%97%B6%E5%80%99%E6%B5%8F%E8%A7%88%E5%99%A8%E9%A1%B5%E9%9D%A2%E9%9C%80%E8%A6%81%E5%B7%A6%E5%8F%B3%E6%BB%9A%E5%8A%A8%EF%BC%88%E4%B8%80%E8%88%AC%E5%B1%8F%E5%B9%95%E6%9C%80%E5%A4%A7%E5%8C%96%E5%90%8E%EF%BC%8C%E5%B7%A6%E5%8F%B3%E6%BB%9A%E5%8A%A8%E7%9A%84%E6%83%85%E5%86%B5%E5%B7%B2%E7%BB%8F%E5%BE%88%E5%B0%91%E8%A7%81%E4%BA%86)\">#</a></h6>\n<h6 id=\"2.2-%E9%80%9A%E8%BF%87%E5%B7%A6%E8%BE%B9%E6%8E%A7%E5%88%B6%E6%A8%AA%E5%90%91%E5%92%8C%E7%BA%B5%E5%90%91%E6%BB%9A%E5%8A%A8%E6%9D%A1-scrollto(x%2C-y)\">2.2 通过左边控制横向和纵向滚动条 scrollTo(x, y) <a class=\"heading-anchor-permalink\" href=\"#2.2-%E9%80%9A%E8%BF%87%E5%B7%A6%E8%BE%B9%E6%8E%A7%E5%88%B6%E6%A8%AA%E5%90%91%E5%92%8C%E7%BA%B5%E5%90%91%E6%BB%9A%E5%8A%A8%E6%9D%A1-scrollto(x%2C-y)\">#</a></h6>\n<pre><code class=\"language-python\">js = &quot;window.scrollTo(100,400)&quot;\ndriver.execute_script(js)\n</code></pre>\n<h5 id=\"%E4%B8%89.%E5%85%83%E7%B4%A0%E8%81%9A%E7%84%A6\">三.元素聚焦 <a class=\"heading-anchor-permalink\" href=\"#%E4%B8%89.%E5%85%83%E7%B4%A0%E8%81%9A%E7%84%A6\">#</a></h5>\n<p>虽然用上面的方法可以解决拖动滚动条的位置问题，但是有时候无法确定我需要操作的元素在什么位置，有可能每次打开的页面不一样，元素所在的位置也不一样，怎么办呢？这个时候我们可以先让页面直接跳到元素出现的位置，然后就可以操作了</p>\n<p>同样需要借助 JS 去实现。 具体如下：</p>\n<pre><code class=\"language-python\">target = driver.find_element_by_xxxx()\ndriver.execute_script(&quot;arguments[0].scrollIntoView();&quot;, target)\n</code></pre>\n<h5 id=\"%E5%9B%9B.-%E5%8F%82%E8%80%83%E4%BB%A3%E7%A0%81\">四. 参考代码 <a class=\"heading-anchor-permalink\" href=\"#%E5%9B%9B.-%E5%8F%82%E8%80%83%E4%BB%A3%E7%A0%81\">#</a></h5>\n<pre><code class=\"language-python\">from selenium import webdriver\nfrom lxml import etree\nimport time\n\nurl = &quot;https://search.jd.com/Search?keyword=%E7%AC%94%E8%AE%B0%E6%9C%AC&amp;enc=utf-8&amp;wq=%E7%AC%94%E8%AE%B0%E6%9C%AC&amp;pvid=845d019c94f6476ca5c4ffc24df6865a&quot;\n# 加载浏览器\nwd = webdriver.Firefox()\n# 发送请求\nwd.get(url)\n# 要执行的js\njs = &quot;var q = document.documentElement.scrollTop=10000&quot;\n# 执行js\nwd.execute_script(js)\n\ntime.sleep(3)\n# 解析数据\ne = etree.HTML(wd.page_source)\n# 提取数据的xpath\nprice_xpath = '//ul[@class=&quot;gl-warp clearfix&quot;]//div[@class=&quot;p-price&quot;]/strong/i/text()'\n# 提取数据的\ninfos = e.xpath(price_xpath)\n\nprint(len(infos))\n# 关闭浏览器\nwd.quit()\n</code></pre>\n",
      "id": 13
    },
    {
      "path": "数据提取与验证码的识别（下）/Selenium与PhantomJS.md",
      "url": "数据提取与验证码的识别（下）/Selenium与PhantomJS.html",
      "content": "### 1. Selenium\n\nSelenium 是一个 Web 的自动化测试工具，最初是为网站自动化测试而开发的，类型像我们玩游戏用的按键精灵，可以按指定的命令自动操作，不同是 Selenium 可以直接运行在浏览器上，它支持所有主流的浏览器（包括 PhantomJS 这些无界面的浏览器）。\n\nSelenium 可以根据我们的指令，让浏览器自动加载页面，获取需要的数据，甚至页面截屏，或者判断网站上某些动作是否发生。\n\nSelenium 自己不带浏览器，不支持浏览器的功能，它需要与第三方浏览器结合在一起才能使用。但是我们有时候需要让它内嵌在代码中运行，所以我们可以用一个叫 PhantomJS 的工具代替真实的浏览器。\n\n[PyPI 网站下载 Selenium 库](https://pypi.python.org/simple/selenium)，也可以用 第三方管理器\n\npip 用命令安装：\n\n```sh\npip install selenium\n```\n\n[Selenium 官方参考文档](http://selenium-python.readthedocs.io/index.html)\n\n### 2. PhantomJS\n\nPhantomJS 是一个基于 Webkit 的“无界面”(headless)浏览器，它会把网站加载到内存并执行页面上的 JavaScript，因为不会展示图形界面，所以运行起来比完整的浏览器要高效\n\n如果我们把 Selenium 和 PhantomJS 结合在一起，就可以运行一个非常强大的网络爬虫了，这个爬虫可以处理 JavaScrip、Cookie、headers，以及任何我们真实用户需要做的事情\n\n#### 2.1 注意：PhantomJS（python2）\n\n只能从它的[官方网站下载](http://phantomjs.org/download.html)。 因为 PhantomJS 是一个功能完善(虽然无界面)的浏览器而非一个 Python 库，所以它不需要像 Python 的其他库一样安装，但我们可以通过 Selenium 调用 PhantomJS 来直接使用。\n\n[PhantomJS 官方参考文档](http://phantomjs.org/documentation)\n\n#### 2.2 python3 使用的浏览器\n\n随着 Python3 的普及，Selenium3 也跟上了行程。而 Selenium3 最大的变化是去掉了 Selenium RC，另外就是 Webdriver 从各自浏览器中脱离，必须单独下载\n\n##### 2.1.1 安装 Firefox geckodriver\n\n安装 firefox 最新版本，添加 Firefox 可执行程序到系统环境变量。记得关闭 firefox 的自动更新\n\n[firefox 下载地址](https://github.com/mozilla/geckodriver/releases)\n\n将下载的`geckodriver.exe` 放到 path 路径下`D:\\Python\\Python36\\`\n\n##### 2.1.2 安装 ChromeDriver\n\n[ChromeDriver](http://chromedriver.storage.googleapis.com/index.html)\n\n> 注意版本号要对应\n> 下载下来的文件解压到`Python36\\Scripts`\n> chrome59 版本以后可以变成无头的浏览器，加以下参数\n\n```python\noptions = webdriver.ChromeOptions()\noptions.add_argument('--headless')\nchrome = webdriver.Chrome(chrome_options=options)\nchrome.get(\"http://ww.baidu.com\")\n```\n\n> 代理模式\n\n```python\nfrom selenium import webdriver\noption = webdriver.ChromeOptions()\noption.add_argument(\"--proxy-server=http://61.138.33.20:808\")\nchrome = webdriver.Chrome(chrome_options=option)\nchrome.get('http://httpbin.org/get')\ninfo = chrome.page_source\n\nprint(info)\n```\n\n### 3. 使用方式\n\nSelenium 库里有个叫 WebDriver 的 API。WebDriver 有点儿像可以加载网站的浏览器，但是它也可以像 BeautifulSoup 或者其他 Selector 对象一样用来查找页面元素，与页面上的元素进行交互 (发送文本、点击等)，以及执行其他动作来运行网络爬虫\n\n#### 3.1 简单例子\n\n```python\n# 导入 webdriver\nfrom selenium import webdriver\n\n# 要想调用键盘按键操作需要引入keys包\nfrom selenium.webdriver.common.keys import Keys\n\n# 调用环境变量指定的PhantomJS浏览器创建浏览器对象\ndriver = webdriver.PhantomJS()\n\n# 如果没有在环境变量指定PhantomJS位置\n# driver = webdriver.PhantomJS(executable_path=\"./phantomjs\"))\n\n# get方法会一直等到页面被完全加载，然后才会继续程序，通常测试会在这里选择 time.sleep(2)\ndriver.get(\"http://www.baidu.com/\")\n\n# 获取页面名为 wrapper的id标签的文本内容\ndata = driver.find_element_by_id(\"wrapper\").text\n\n# 打印数据内容\nprint(data)\n\n# 打印页面标题 \"百度一下，你就知道\"\nprint（driver.title）\n\n# 生成当前页面快照并保存\ndriver.save_screenshot(\"baidu.png\")\n\n# id=\"kw\"是百度搜索输入框，输入字符串\"小小图灵社\"\ndriver.find_element_by_id(\"kw\").send_keys(\"小小图灵社\")\n\n# id=\"su\"是百度搜索按钮，click() 是模拟点击\ndriver.find_element_by_id(\"su\").click()\n\n# 获取新的页面快照\ndriver.save_screenshot(\"screenshot.png\")\n\n# 打印网页渲染后的源代码\nprint(driver.page_source)\n\n# 获取当前页面Cookie\nprint(driver.get_cookies())\n\n# ctrl+a 全选输入框内容\ndriver.find_element_by_id(\"kw\").send_keys(Keys.CONTROL,'a')\n\n# ctrl+x 剪切输入框内容\ndriver.find_element_by_id(\"kw\").send_keys(Keys.CONTROL,'x')\n\n# 输入框重新输入内容\ndriver.find_element_by_id(\"kw\").send_keys(\"python爬虫\")\n\n# 模拟Enter回车键\ndriver.find_element_by_id(\"su\").send_keys(Keys.RETURN)\n\n# 清除输入框内容\ndriver.find_element_by_id(\"kw\").clear()\n\n# 生成新的页面快照\ndriver.save_screenshot(\"python爬虫.png\")\n\n# 获取当前url\nprint(driver.current_url)\n\n# 关闭当前页面，如果只有一个页面，会关闭浏览器\n# driver.close()\n\n# 关闭浏览器\ndriver.quit()\n```\n\n### 4 页面操作\n\n#### 4.1 页面交互\n\n> 仅仅抓取页面没有多大卵用，我们真正要做的是做到和页面交互，比如点击，输入等等。那么前提就是要找到页面中的元素。WebDriver 提供了各种方法来寻找元素。例如下面有一个表单输入框\n\n```html\n<input type=\"text\" name=\"passwd\" id=\"passwd-id\" />\n```\n\n##### 4.1.1 **获取**\n\n```python\nelement = driver.find_element_by_id(\"passwd-id\")\nelement = driver.find_element_by_name(\"passwd\")\nelement = driver.find_elements_by_tag_name(\"input\")\nelement = driver.find_element_by_xpath(\"//input[@id='passwd-id']\")\n```\n\n**注意：**\n\n- 文本必须完全匹配才可以，所以这并不是一个很好的匹配方式\n\n- 在用 xpath 的时候还需要注意的如果有多个元素匹配了 xpath，它只会返回第一个匹配的元素。如果没有找到，那么会抛出 NoSuchElementException 的异常\n\n##### 4.1.2 输入内容\n\n```python\nelement.send_keys(\"some text\")\n```\n\n##### 4.1.3 模拟点击某个按键\n\n```python\nelement.send_keys(\"and some\", Keys.ARROW_DOWN)\n```\n\n##### 4.1.4 清空文本\n\n```python\nelement.clear()\n```\n\n##### 4.1.5 元素拖拽\n\n> 要完成元素的拖拽，首先你需要指定被拖动的元素和拖动目标元素，然后利用 ActionChains 类来实现\n\n以下实现元素从 source 拖动到 target 的操作\n\n```python\nelement = driver.find_element_by_name(\"source\")\ntarget = driver.find_element_by_name(\"target\")\n\nfrom selenium.webdriver import ActionChains\naction_chains = ActionChains(driver)\naction_chains.drag_and_drop(element, target).perform()\n```\n\n##### 4.1.6 历史记录\n\n> 操作页面的前进和后退功能\n\n```python\ndriver.forward()\ndriver.back()\n```\n\n### 5 API\n\n#### 5.1 元素选取\n\n##### 5.1.1 单个元素选取\n\n- find_element_by_id\n- find_element_by_name\n- find_element_by_xpath\n- find_element_by_link_text\n- find_element_by_partial_link_text\n- find_element_by_tag_name\n- find_element_by_class_name\n- find_element_by_css_selector\n\n##### 5.1.2 多个元素选取\n\n- find_elements_by_name\n- find_elements_by_xpath\n- find_elements_by_link_text\n- find_elements_by_partial_link_text\n- find_elements_by_tag_name\n- find_elements_by_class_name\n- find_elements_by_css_selector\n\n##### 5.1.3 利用 By 类来确定哪种选择方式\n\n```python\nfrom selenium.webdriver.common.by import By\n\ndriver.find_element(By.XPATH, '//button[text()=\"Some text\"]')\ndriver.find_elements(By.XPATH, '//button')\n```\n\nBy 类的一些属性如下\n\n- ID = \"id\"\n- XPATH = \"xpath\"\n- LINK_TEXT = \"link text\"\n- PARTIAL_LINK_TEXT = \"partial link text\"\n- NAME = \"name\"\n- TAG_NAME = \"tag name\"\n- CLASS_NAME = \"class name\"\n- CSS_SELECTOR = \"css selector\"\n\n### 6 等待\n\n#### 6.1 隐式等待\n\n> 到了一定的时间发现元素还没有加载，则继续等待我们指定的时间，如果超过了我们指定的时间还没有加载就会抛出异常，如果没有需要等待的时候就已经加载完毕就会立即执行\n\n```python\nfrom selenium import webdriver\nurl = 'https://www.guazi.com/nj/buy/'\ndriver = webdriver.Chrome()\ndriver.get(url)\ndriver.implicitly_wait(100)\nprint(driver.find_element_by_class_name('next'))\nprint(driver.page_source)\n```\n\n#### 6.2 显示等待\n\n> 指定一个等待条件，并且指定一个最长等待时间，会在这个时间内进行判断是否满足等待条件，如果成立就会立即返回，如果不成立，就会一直等待，直到等待你指定的最长等待时间，如果还是不满足，就会抛出异常，如果满足了就会正常返回\n\n```python\nurl = 'https://www.guazi.com/nj/buy/'\ndriver = webdriver.Chrome()\ndriver.get(url)\nwait = WebDriverWait(driver,10)\nwait.until(EC.presence_of_element_located((By.CLASS_NAME, 'next')))\nprint(driver.page_source)\n```\n\n- presence_of_element_located\n  - 元素加载出，传入定位元组，如(By.ID, 'p')\n- presence_of_all_elements_located\n  - 所有元素加载出\n- element_to_be_clickable\n  - 元素可点击\n- element_located_to_be_selected\n  - 元素可选择，传入定位元组\n\n#### 6.3 强制等待\n\n> 使用 `time.sleep`\n",
      "html": "<h3 id=\"1.-selenium\">1. Selenium <a class=\"heading-anchor-permalink\" href=\"#1.-selenium\">#</a></h3>\n<p>Selenium 是一个 Web 的自动化测试工具，最初是为网站自动化测试而开发的，类型像我们玩游戏用的按键精灵，可以按指定的命令自动操作，不同是 Selenium 可以直接运行在浏览器上，它支持所有主流的浏览器（包括 PhantomJS 这些无界面的浏览器）。</p>\n<p>Selenium 可以根据我们的指令，让浏览器自动加载页面，获取需要的数据，甚至页面截屏，或者判断网站上某些动作是否发生。</p>\n<p>Selenium 自己不带浏览器，不支持浏览器的功能，它需要与第三方浏览器结合在一起才能使用。但是我们有时候需要让它内嵌在代码中运行，所以我们可以用一个叫 PhantomJS 的工具代替真实的浏览器。</p>\n<p><a href=\"https://pypi.python.org/simple/selenium\">PyPI 网站下载 Selenium 库</a>，也可以用 第三方管理器</p>\n<p>pip 用命令安装：</p>\n<pre><code class=\"language-sh\">pip install selenium\n</code></pre>\n<p><a href=\"http://selenium-python.readthedocs.io/index.html\">Selenium 官方参考文档</a></p>\n<h3 id=\"2.-phantomjs\">2. PhantomJS <a class=\"heading-anchor-permalink\" href=\"#2.-phantomjs\">#</a></h3>\n<p>PhantomJS 是一个基于 Webkit 的“无界面”(headless)浏览器，它会把网站加载到内存并执行页面上的 JavaScript，因为不会展示图形界面，所以运行起来比完整的浏览器要高效</p>\n<p>如果我们把 Selenium 和 PhantomJS 结合在一起，就可以运行一个非常强大的网络爬虫了，这个爬虫可以处理 JavaScrip、Cookie、headers，以及任何我们真实用户需要做的事情</p>\n<h4 id=\"2.1-%E6%B3%A8%E6%84%8F%EF%BC%9Aphantomjs%EF%BC%88python2%EF%BC%89\">2.1 注意：PhantomJS（python2） <a class=\"heading-anchor-permalink\" href=\"#2.1-%E6%B3%A8%E6%84%8F%EF%BC%9Aphantomjs%EF%BC%88python2%EF%BC%89\">#</a></h4>\n<p>只能从它的<a href=\"http://phantomjs.org/download.html\">官方网站下载</a>。 因为 PhantomJS 是一个功能完善(虽然无界面)的浏览器而非一个 Python 库，所以它不需要像 Python 的其他库一样安装，但我们可以通过 Selenium 调用 PhantomJS 来直接使用。</p>\n<p><a href=\"http://phantomjs.org/documentation\">PhantomJS 官方参考文档</a></p>\n<h4 id=\"2.2-python3-%E4%BD%BF%E7%94%A8%E7%9A%84%E6%B5%8F%E8%A7%88%E5%99%A8\">2.2 python3 使用的浏览器 <a class=\"heading-anchor-permalink\" href=\"#2.2-python3-%E4%BD%BF%E7%94%A8%E7%9A%84%E6%B5%8F%E8%A7%88%E5%99%A8\">#</a></h4>\n<p>随着 Python3 的普及，Selenium3 也跟上了行程。而 Selenium3 最大的变化是去掉了 Selenium RC，另外就是 Webdriver 从各自浏览器中脱离，必须单独下载</p>\n<h5 id=\"2.1.1-%E5%AE%89%E8%A3%85-firefox-geckodriver\">2.1.1 安装 Firefox geckodriver <a class=\"heading-anchor-permalink\" href=\"#2.1.1-%E5%AE%89%E8%A3%85-firefox-geckodriver\">#</a></h5>\n<p>安装 firefox 最新版本，添加 Firefox 可执行程序到系统环境变量。记得关闭 firefox 的自动更新</p>\n<p><a href=\"https://github.com/mozilla/geckodriver/releases\">firefox 下载地址</a></p>\n<p>将下载的<code>geckodriver.exe</code> 放到 path 路径下<code>D:\\Python\\Python36\\</code></p>\n<h5 id=\"2.1.2-%E5%AE%89%E8%A3%85-chromedriver\">2.1.2 安装 ChromeDriver <a class=\"heading-anchor-permalink\" href=\"#2.1.2-%E5%AE%89%E8%A3%85-chromedriver\">#</a></h5>\n<p><a href=\"http://chromedriver.storage.googleapis.com/index.html\">ChromeDriver</a></p>\n<blockquote>\n<p>注意版本号要对应\n下载下来的文件解压到<code>Python36\\Scripts</code>\nchrome59 版本以后可以变成无头的浏览器，加以下参数</p>\n</blockquote>\n<pre><code class=\"language-python\">options = webdriver.ChromeOptions()\noptions.add_argument('--headless')\nchrome = webdriver.Chrome(chrome_options=options)\nchrome.get(&quot;http://ww.baidu.com&quot;)\n</code></pre>\n<blockquote>\n<p>代理模式</p>\n</blockquote>\n<pre><code class=\"language-python\">from selenium import webdriver\noption = webdriver.ChromeOptions()\noption.add_argument(&quot;--proxy-server=http://61.138.33.20:808&quot;)\nchrome = webdriver.Chrome(chrome_options=option)\nchrome.get('http://httpbin.org/get')\ninfo = chrome.page_source\n\nprint(info)\n</code></pre>\n<h3 id=\"3.-%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F\">3. 使用方式 <a class=\"heading-anchor-permalink\" href=\"#3.-%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F\">#</a></h3>\n<p>Selenium 库里有个叫 WebDriver 的 API。WebDriver 有点儿像可以加载网站的浏览器，但是它也可以像 BeautifulSoup 或者其他 Selector 对象一样用来查找页面元素，与页面上的元素进行交互 (发送文本、点击等)，以及执行其他动作来运行网络爬虫</p>\n<h4 id=\"3.1-%E7%AE%80%E5%8D%95%E4%BE%8B%E5%AD%90\">3.1 简单例子 <a class=\"heading-anchor-permalink\" href=\"#3.1-%E7%AE%80%E5%8D%95%E4%BE%8B%E5%AD%90\">#</a></h4>\n<pre><code class=\"language-python\"># 导入 webdriver\nfrom selenium import webdriver\n\n# 要想调用键盘按键操作需要引入keys包\nfrom selenium.webdriver.common.keys import Keys\n\n# 调用环境变量指定的PhantomJS浏览器创建浏览器对象\ndriver = webdriver.PhantomJS()\n\n# 如果没有在环境变量指定PhantomJS位置\n# driver = webdriver.PhantomJS(executable_path=&quot;./phantomjs&quot;))\n\n# get方法会一直等到页面被完全加载，然后才会继续程序，通常测试会在这里选择 time.sleep(2)\ndriver.get(&quot;http://www.baidu.com/&quot;)\n\n# 获取页面名为 wrapper的id标签的文本内容\ndata = driver.find_element_by_id(&quot;wrapper&quot;).text\n\n# 打印数据内容\nprint(data)\n\n# 打印页面标题 &quot;百度一下，你就知道&quot;\nprint（driver.title）\n\n# 生成当前页面快照并保存\ndriver.save_screenshot(&quot;baidu.png&quot;)\n\n# id=&quot;kw&quot;是百度搜索输入框，输入字符串&quot;小小图灵社&quot;\ndriver.find_element_by_id(&quot;kw&quot;).send_keys(&quot;小小图灵社&quot;)\n\n# id=&quot;su&quot;是百度搜索按钮，click() 是模拟点击\ndriver.find_element_by_id(&quot;su&quot;).click()\n\n# 获取新的页面快照\ndriver.save_screenshot(&quot;screenshot.png&quot;)\n\n# 打印网页渲染后的源代码\nprint(driver.page_source)\n\n# 获取当前页面Cookie\nprint(driver.get_cookies())\n\n# ctrl+a 全选输入框内容\ndriver.find_element_by_id(&quot;kw&quot;).send_keys(Keys.CONTROL,'a')\n\n# ctrl+x 剪切输入框内容\ndriver.find_element_by_id(&quot;kw&quot;).send_keys(Keys.CONTROL,'x')\n\n# 输入框重新输入内容\ndriver.find_element_by_id(&quot;kw&quot;).send_keys(&quot;python爬虫&quot;)\n\n# 模拟Enter回车键\ndriver.find_element_by_id(&quot;su&quot;).send_keys(Keys.RETURN)\n\n# 清除输入框内容\ndriver.find_element_by_id(&quot;kw&quot;).clear()\n\n# 生成新的页面快照\ndriver.save_screenshot(&quot;python爬虫.png&quot;)\n\n# 获取当前url\nprint(driver.current_url)\n\n# 关闭当前页面，如果只有一个页面，会关闭浏览器\n# driver.close()\n\n# 关闭浏览器\ndriver.quit()\n</code></pre>\n<h3 id=\"4-%E9%A1%B5%E9%9D%A2%E6%93%8D%E4%BD%9C\">4 页面操作 <a class=\"heading-anchor-permalink\" href=\"#4-%E9%A1%B5%E9%9D%A2%E6%93%8D%E4%BD%9C\">#</a></h3>\n<h4 id=\"4.1-%E9%A1%B5%E9%9D%A2%E4%BA%A4%E4%BA%92\">4.1 页面交互 <a class=\"heading-anchor-permalink\" href=\"#4.1-%E9%A1%B5%E9%9D%A2%E4%BA%A4%E4%BA%92\">#</a></h4>\n<blockquote>\n<p>仅仅抓取页面没有多大卵用，我们真正要做的是做到和页面交互，比如点击，输入等等。那么前提就是要找到页面中的元素。WebDriver 提供了各种方法来寻找元素。例如下面有一个表单输入框</p>\n</blockquote>\n<pre><code class=\"language-html\">&lt;input type=&quot;text&quot; name=&quot;passwd&quot; id=&quot;passwd-id&quot; /&gt;\n</code></pre>\n<h5 id=\"4.1.1-%E8%8E%B7%E5%8F%96\">4.1.1 <strong>获取</strong> <a class=\"heading-anchor-permalink\" href=\"#4.1.1-%E8%8E%B7%E5%8F%96\">#</a></h5>\n<pre><code class=\"language-python\">element = driver.find_element_by_id(&quot;passwd-id&quot;)\nelement = driver.find_element_by_name(&quot;passwd&quot;)\nelement = driver.find_elements_by_tag_name(&quot;input&quot;)\nelement = driver.find_element_by_xpath(&quot;//input[@id='passwd-id']&quot;)\n</code></pre>\n<p><strong>注意：</strong></p>\n<ul>\n<li>\n<p>文本必须完全匹配才可以，所以这并不是一个很好的匹配方式</p>\n</li>\n<li>\n<p>在用 xpath 的时候还需要注意的如果有多个元素匹配了 xpath，它只会返回第一个匹配的元素。如果没有找到，那么会抛出 NoSuchElementException 的异常</p>\n</li>\n</ul>\n<h5 id=\"4.1.2-%E8%BE%93%E5%85%A5%E5%86%85%E5%AE%B9\">4.1.2 输入内容 <a class=\"heading-anchor-permalink\" href=\"#4.1.2-%E8%BE%93%E5%85%A5%E5%86%85%E5%AE%B9\">#</a></h5>\n<pre><code class=\"language-python\">element.send_keys(&quot;some text&quot;)\n</code></pre>\n<h5 id=\"4.1.3-%E6%A8%A1%E6%8B%9F%E7%82%B9%E5%87%BB%E6%9F%90%E4%B8%AA%E6%8C%89%E9%94%AE\">4.1.3 模拟点击某个按键 <a class=\"heading-anchor-permalink\" href=\"#4.1.3-%E6%A8%A1%E6%8B%9F%E7%82%B9%E5%87%BB%E6%9F%90%E4%B8%AA%E6%8C%89%E9%94%AE\">#</a></h5>\n<pre><code class=\"language-python\">element.send_keys(&quot;and some&quot;, Keys.ARROW_DOWN)\n</code></pre>\n<h5 id=\"4.1.4-%E6%B8%85%E7%A9%BA%E6%96%87%E6%9C%AC\">4.1.4 清空文本 <a class=\"heading-anchor-permalink\" href=\"#4.1.4-%E6%B8%85%E7%A9%BA%E6%96%87%E6%9C%AC\">#</a></h5>\n<pre><code class=\"language-python\">element.clear()\n</code></pre>\n<h5 id=\"4.1.5-%E5%85%83%E7%B4%A0%E6%8B%96%E6%8B%BD\">4.1.5 元素拖拽 <a class=\"heading-anchor-permalink\" href=\"#4.1.5-%E5%85%83%E7%B4%A0%E6%8B%96%E6%8B%BD\">#</a></h5>\n<blockquote>\n<p>要完成元素的拖拽，首先你需要指定被拖动的元素和拖动目标元素，然后利用 ActionChains 类来实现</p>\n</blockquote>\n<p>以下实现元素从 source 拖动到 target 的操作</p>\n<pre><code class=\"language-python\">element = driver.find_element_by_name(&quot;source&quot;)\ntarget = driver.find_element_by_name(&quot;target&quot;)\n\nfrom selenium.webdriver import ActionChains\naction_chains = ActionChains(driver)\naction_chains.drag_and_drop(element, target).perform()\n</code></pre>\n<h5 id=\"4.1.6-%E5%8E%86%E5%8F%B2%E8%AE%B0%E5%BD%95\">4.1.6 历史记录 <a class=\"heading-anchor-permalink\" href=\"#4.1.6-%E5%8E%86%E5%8F%B2%E8%AE%B0%E5%BD%95\">#</a></h5>\n<blockquote>\n<p>操作页面的前进和后退功能</p>\n</blockquote>\n<pre><code class=\"language-python\">driver.forward()\ndriver.back()\n</code></pre>\n<h3 id=\"5-api\">5 API <a class=\"heading-anchor-permalink\" href=\"#5-api\">#</a></h3>\n<h4 id=\"5.1-%E5%85%83%E7%B4%A0%E9%80%89%E5%8F%96\">5.1 元素选取 <a class=\"heading-anchor-permalink\" href=\"#5.1-%E5%85%83%E7%B4%A0%E9%80%89%E5%8F%96\">#</a></h4>\n<h5 id=\"5.1.1-%E5%8D%95%E4%B8%AA%E5%85%83%E7%B4%A0%E9%80%89%E5%8F%96\">5.1.1 单个元素选取 <a class=\"heading-anchor-permalink\" href=\"#5.1.1-%E5%8D%95%E4%B8%AA%E5%85%83%E7%B4%A0%E9%80%89%E5%8F%96\">#</a></h5>\n<ul>\n<li>find_element_by_id</li>\n<li>find_element_by_name</li>\n<li>find_element_by_xpath</li>\n<li>find_element_by_link_text</li>\n<li>find_element_by_partial_link_text</li>\n<li>find_element_by_tag_name</li>\n<li>find_element_by_class_name</li>\n<li>find_element_by_css_selector</li>\n</ul>\n<h5 id=\"5.1.2-%E5%A4%9A%E4%B8%AA%E5%85%83%E7%B4%A0%E9%80%89%E5%8F%96\">5.1.2 多个元素选取 <a class=\"heading-anchor-permalink\" href=\"#5.1.2-%E5%A4%9A%E4%B8%AA%E5%85%83%E7%B4%A0%E9%80%89%E5%8F%96\">#</a></h5>\n<ul>\n<li>find_elements_by_name</li>\n<li>find_elements_by_xpath</li>\n<li>find_elements_by_link_text</li>\n<li>find_elements_by_partial_link_text</li>\n<li>find_elements_by_tag_name</li>\n<li>find_elements_by_class_name</li>\n<li>find_elements_by_css_selector</li>\n</ul>\n<h5 id=\"5.1.3-%E5%88%A9%E7%94%A8-by-%E7%B1%BB%E6%9D%A5%E7%A1%AE%E5%AE%9A%E5%93%AA%E7%A7%8D%E9%80%89%E6%8B%A9%E6%96%B9%E5%BC%8F\">5.1.3 利用 By 类来确定哪种选择方式 <a class=\"heading-anchor-permalink\" href=\"#5.1.3-%E5%88%A9%E7%94%A8-by-%E7%B1%BB%E6%9D%A5%E7%A1%AE%E5%AE%9A%E5%93%AA%E7%A7%8D%E9%80%89%E6%8B%A9%E6%96%B9%E5%BC%8F\">#</a></h5>\n<pre><code class=\"language-python\">from selenium.webdriver.common.by import By\n\ndriver.find_element(By.XPATH, '//button[text()=&quot;Some text&quot;]')\ndriver.find_elements(By.XPATH, '//button')\n</code></pre>\n<p>By 类的一些属性如下</p>\n<ul>\n<li>ID = “id”</li>\n<li>XPATH = “xpath”</li>\n<li>LINK_TEXT = “link text”</li>\n<li>PARTIAL_LINK_TEXT = “partial link text”</li>\n<li>NAME = “name”</li>\n<li>TAG_NAME = “tag name”</li>\n<li>CLASS_NAME = “class name”</li>\n<li>CSS_SELECTOR = “css selector”</li>\n</ul>\n<h3 id=\"6-%E7%AD%89%E5%BE%85\">6 等待 <a class=\"heading-anchor-permalink\" href=\"#6-%E7%AD%89%E5%BE%85\">#</a></h3>\n<h4 id=\"6.1-%E9%9A%90%E5%BC%8F%E7%AD%89%E5%BE%85\">6.1 隐式等待 <a class=\"heading-anchor-permalink\" href=\"#6.1-%E9%9A%90%E5%BC%8F%E7%AD%89%E5%BE%85\">#</a></h4>\n<blockquote>\n<p>到了一定的时间发现元素还没有加载，则继续等待我们指定的时间，如果超过了我们指定的时间还没有加载就会抛出异常，如果没有需要等待的时候就已经加载完毕就会立即执行</p>\n</blockquote>\n<pre><code class=\"language-python\">from selenium import webdriver\nurl = 'https://www.guazi.com/nj/buy/'\ndriver = webdriver.Chrome()\ndriver.get(url)\ndriver.implicitly_wait(100)\nprint(driver.find_element_by_class_name('next'))\nprint(driver.page_source)\n</code></pre>\n<h4 id=\"6.2-%E6%98%BE%E7%A4%BA%E7%AD%89%E5%BE%85\">6.2 显示等待 <a class=\"heading-anchor-permalink\" href=\"#6.2-%E6%98%BE%E7%A4%BA%E7%AD%89%E5%BE%85\">#</a></h4>\n<blockquote>\n<p>指定一个等待条件，并且指定一个最长等待时间，会在这个时间内进行判断是否满足等待条件，如果成立就会立即返回，如果不成立，就会一直等待，直到等待你指定的最长等待时间，如果还是不满足，就会抛出异常，如果满足了就会正常返回</p>\n</blockquote>\n<pre><code class=\"language-python\">url = 'https://www.guazi.com/nj/buy/'\ndriver = webdriver.Chrome()\ndriver.get(url)\nwait = WebDriverWait(driver,10)\nwait.until(EC.presence_of_element_located((By.CLASS_NAME, 'next')))\nprint(driver.page_source)\n</code></pre>\n<ul>\n<li>presence_of_element_located\n<ul>\n<li>元素加载出，传入定位元组，如(<a href=\"http://By.ID\">By.ID</a>, ‘p’)</li>\n</ul>\n</li>\n<li>presence_of_all_elements_located\n<ul>\n<li>所有元素加载出</li>\n</ul>\n</li>\n<li>element_to_be_clickable\n<ul>\n<li>元素可点击</li>\n</ul>\n</li>\n<li>element_located_to_be_selected\n<ul>\n<li>元素可选择，传入定位元组</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"6.3-%E5%BC%BA%E5%88%B6%E7%AD%89%E5%BE%85\">6.3 强制等待 <a class=\"heading-anchor-permalink\" href=\"#6.3-%E5%BC%BA%E5%88%B6%E7%AD%89%E5%BE%85\">#</a></h4>\n<blockquote>\n<p>使用 <code>time.sleep</code></p>\n</blockquote>\n",
      "id": 14
    },
    {
      "path": "数据提取与验证码的识别（下）/爬虫之多线程.md",
      "url": "数据提取与验证码的识别（下）/爬虫之多线程.html",
      "content": "### 1. 引入\n\n> 我们之前写的爬虫都是单个线程的？这怎么够？一旦一个地方卡到不动了，那不就永远等待下去了？为此我们可以使用多线程或者多进程来处理。\n> 不建议你用这个，不过还是介绍下了，如果想看可以看看下面，不想浪费时间直接看\n\n### 2. 如何使用\n\n> 爬虫使用多线程来处理网络请求，使用线程来处理 URL 队列中的 url，然后将 url 返回的结果保存在另一个队列中，其它线程在读取这个队列中的数据，然后写到文件中去\n\n### 3. 主要组成部分\n\n#### 3.1 URL 队列和结果队列\n\n将将要爬去的 url 放在一个队列中，这里使用标准库 Queue。访问 url 后的结果保存在结果队列中\n\n初始化一个 URL 队列\n\n```python\nfrom queue import Queue\nurls_queue = Queue()\nout_queue = Queue()\n```\n\n#### 3.2 类包装\n\n使用多个线程，不停的取 URL 队列中的 url，并进行处理：\n\n```python\nimport threading\n\nclass ThreadCrawl(threading.Thread):\n    def __init__(self, queue, out_queue):\n        threading.Thread.__init__(self)\n        self.queue = queue\n        self.out_queue = out_queue\n\n    def run(self):\n        while True:\n            item = self.queue.get()\n```\n\n如果队列为空，线程就会被阻塞，直到队列不为空。处理队列中的一条数据后，就需要通知队列已经处理完该条数据\n\n#### 3.3 函数包装\n\n```python\nfrom threading import Thread\ndef func(args)\n    pass\nif __name__ == '__main__':\n    info_html = Queue()\n    t1 = Thread(target=func,args=(info_html,)\n```\n\n#### 3.4 线程池\n\n```python\n# 简单往队列中传输线程数\nimport threading\nimport time\nimport queue\n\nclass Threadingpool():\n    def __init__(self,max_num = 10):\n        self.queue = queue.Queue(max_num)\n        for i in range(max_num):\n            self.queue.put(threading.Thread)\n\n    def getthreading(self):\n        return self.queue.get()\n\n    def addthreading(self):\n        self.queue.put(threading.Thread)\n\n\ndef func(p,i):\n    time.sleep(1)\n    print(i)\n    p.addthreading()\n\n\nif __name__ == \"__main__\":\n    p = Threadingpool()\n    for i in range(20):\n        thread = p.getthreading()\n        t = thread(target = func, args = (p,i))\n        t.start()\n```\n\n#### 4. Queue 模块中的常用方法:\n\nPython 的 Queue 模块中提供了同步的、线程安全的队列类，包括 FIFO（先入先出)队列 Queue，LIFO（后入先出）队列 LifoQueue，和优先级队列 PriorityQueue。这些队列都实现了锁原语，能够在多线程中直接使用。可以使用队列来实现线程间的同步\n\n- Queue.qsize() 返回队列的大小\n- Queue.empty() 如果队列为空，返回 True,反之 False\n- Queue.full() 如果队列满了，返回 True,反之 False\n- Queue.full 与 maxsize 大小对应\n- Queue.get([block[, timeout]])获取队列，timeout 等待时间\n- Queue.get_nowait() 相当 Queue.get(False)\n- Queue.put(item) 写入队列，timeout 等待时间\n- Queue.put_nowait(item) 相当 Queue.put(item, False)\n- Queue.task_done() 在完成一项工作之后，Queue.task_done()函数向任务已经完成的队列发送一个信号\n- Queue.join() 实际上意味着等到队列为空，再执行别的操作\n",
      "html": "<h3 id=\"1.-%E5%BC%95%E5%85%A5\">1. 引入 <a class=\"heading-anchor-permalink\" href=\"#1.-%E5%BC%95%E5%85%A5\">#</a></h3>\n<blockquote>\n<p>我们之前写的爬虫都是单个线程的？这怎么够？一旦一个地方卡到不动了，那不就永远等待下去了？为此我们可以使用多线程或者多进程来处理。\n不建议你用这个，不过还是介绍下了，如果想看可以看看下面，不想浪费时间直接看</p>\n</blockquote>\n<h3 id=\"2.-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8\">2. 如何使用 <a class=\"heading-anchor-permalink\" href=\"#2.-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8\">#</a></h3>\n<blockquote>\n<p>爬虫使用多线程来处理网络请求，使用线程来处理 URL 队列中的 url，然后将 url 返回的结果保存在另一个队列中，其它线程在读取这个队列中的数据，然后写到文件中去</p>\n</blockquote>\n<h3 id=\"3.-%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E9%83%A8%E5%88%86\">3. 主要组成部分 <a class=\"heading-anchor-permalink\" href=\"#3.-%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E9%83%A8%E5%88%86\">#</a></h3>\n<h4 id=\"3.1-url-%E9%98%9F%E5%88%97%E5%92%8C%E7%BB%93%E6%9E%9C%E9%98%9F%E5%88%97\">3.1 URL 队列和结果队列 <a class=\"heading-anchor-permalink\" href=\"#3.1-url-%E9%98%9F%E5%88%97%E5%92%8C%E7%BB%93%E6%9E%9C%E9%98%9F%E5%88%97\">#</a></h4>\n<p>将将要爬去的 url 放在一个队列中，这里使用标准库 Queue。访问 url 后的结果保存在结果队列中</p>\n<p>初始化一个 URL 队列</p>\n<pre><code class=\"language-python\">from queue import Queue\nurls_queue = Queue()\nout_queue = Queue()\n</code></pre>\n<h4 id=\"3.2-%E7%B1%BB%E5%8C%85%E8%A3%85\">3.2 类包装 <a class=\"heading-anchor-permalink\" href=\"#3.2-%E7%B1%BB%E5%8C%85%E8%A3%85\">#</a></h4>\n<p>使用多个线程，不停的取 URL 队列中的 url，并进行处理：</p>\n<pre><code class=\"language-python\">import threading\n\nclass ThreadCrawl(threading.Thread):\n    def __init__(self, queue, out_queue):\n        threading.Thread.__init__(self)\n        self.queue = queue\n        self.out_queue = out_queue\n\n    def run(self):\n        while True:\n            item = self.queue.get()\n</code></pre>\n<p>如果队列为空，线程就会被阻塞，直到队列不为空。处理队列中的一条数据后，就需要通知队列已经处理完该条数据</p>\n<h4 id=\"3.3-%E5%87%BD%E6%95%B0%E5%8C%85%E8%A3%85\">3.3 函数包装 <a class=\"heading-anchor-permalink\" href=\"#3.3-%E5%87%BD%E6%95%B0%E5%8C%85%E8%A3%85\">#</a></h4>\n<pre><code class=\"language-python\">from threading import Thread\ndef func(args)\n    pass\nif __name__ == '__main__':\n    info_html = Queue()\n    t1 = Thread(target=func,args=(info_html,)\n</code></pre>\n<h4 id=\"3.4-%E7%BA%BF%E7%A8%8B%E6%B1%A0\">3.4 线程池 <a class=\"heading-anchor-permalink\" href=\"#3.4-%E7%BA%BF%E7%A8%8B%E6%B1%A0\">#</a></h4>\n<pre><code class=\"language-python\"># 简单往队列中传输线程数\nimport threading\nimport time\nimport queue\n\nclass Threadingpool():\n    def __init__(self,max_num = 10):\n        self.queue = queue.Queue(max_num)\n        for i in range(max_num):\n            self.queue.put(threading.Thread)\n\n    def getthreading(self):\n        return self.queue.get()\n\n    def addthreading(self):\n        self.queue.put(threading.Thread)\n\n\ndef func(p,i):\n    time.sleep(1)\n    print(i)\n    p.addthreading()\n\n\nif __name__ == &quot;__main__&quot;:\n    p = Threadingpool()\n    for i in range(20):\n        thread = p.getthreading()\n        t = thread(target = func, args = (p,i))\n        t.start()\n</code></pre>\n<h4 id=\"4.-queue-%E6%A8%A1%E5%9D%97%E4%B8%AD%E7%9A%84%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%3A\">4. Queue 模块中的常用方法: <a class=\"heading-anchor-permalink\" href=\"#4.-queue-%E6%A8%A1%E5%9D%97%E4%B8%AD%E7%9A%84%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%3A\">#</a></h4>\n<p>Python 的 Queue 模块中提供了同步的、线程安全的队列类，包括 FIFO（先入先出)队列 Queue，LIFO（后入先出）队列 LifoQueue，和优先级队列 PriorityQueue。这些队列都实现了锁原语，能够在多线程中直接使用。可以使用队列来实现线程间的同步</p>\n<ul>\n<li>Queue.qsize() 返回队列的大小</li>\n<li>Queue.empty() 如果队列为空，返回 True,反之 False</li>\n<li>Queue.full() 如果队列满了，返回 True,反之 False</li>\n<li>Queue.full 与 maxsize 大小对应</li>\n<li>Queue.get([block[, timeout]])获取队列，timeout 等待时间</li>\n<li>Queue.get_nowait() 相当 Queue.get(False)</li>\n<li>Queue.put(item) 写入队列，timeout 等待时间</li>\n<li>Queue.put_nowait(item) 相当 Queue.put(item, False)</li>\n<li>Queue.task_done() 在完成一项工作之后，Queue.task_done()函数向任务已经完成的队列发送一个信号</li>\n<li>Queue.join() 实际上意味着等到队列为空，再执行别的操作</li>\n</ul>\n",
      "id": 15
    },
    {
      "path": "类级别的写法与Scrapy框架/Scrapy 数据的保存.md",
      "url": "类级别的写法与Scrapy框架/Scrapy 数据的保存.html",
      "content": "### 1. 数据的提取\n\n#### 1.1 控制台打印\n\n```python\nimport scrapy\n\n\nclass DoubanSpider(scrapy.Spider):\n    name = 'douban'\n    allwed_url = 'douban.com'\n    start_urls = [\n        'https://movie.douban.com/top250/'\n    ]\n\n    def parse(self, response):\n        movie_name = response.xpath(\"//div[@class='item']//a/span[1]/text()\").extract()\n        movie_core = response.xpath(\"//div[@class='star']/span[2]/text()\").extract()\n        yield {\n            'movie_name':movie_name,\n            'movie_core':movie_core\n        }\n```\n\n执行以上代码，我可以在控制看到：\n\n<details>\n<summary>输出</summary>\n\n<code>\n2018-01-24 15:17:14 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: spiderdemo1)\n2018-01-24 15:17:14 [scrapy.utils.log] INFO: Versions: lxml 4.1.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.3.1, w3lib 1.18.0, Twiste\nd 17.9.0, Python 3.6.3 (v3.6.3:2c5fed8, Oct  3 2017, 18:11:49) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0g  2 Nov 201\n7), cryptography 2.1.4, Platform Windows-10-10.0.10240-SP0\n2018-01-24 15:17:14 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'spiderdemo1', 'NEWSPIDER_MODULE': 'spiderdemo1.spiders',\n'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['spiderdemo1.spiders']}\n2018-01-24 15:17:14 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.logstats.LogStats']\n2018-01-24 15:17:14 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2018-01-24 15:17:14 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2018-01-24 15:17:14 [scrapy.middleware] INFO: Enabled item pipelines:\n[]\n2018-01-24 15:17:14 [scrapy.core.engine] INFO: Spider opened\n2018-01-24 15:17:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2018-01-24 15:17:14 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n2018-01-24 15:17:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://movie.douban.com/robots.txt> (referer: None)\n2018-01-24 15:17:15 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://movie.douban.com/top250> from <GET\n https://movie.douban.com/top250/>\n2018-01-24 15:17:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://movie.douban.com/top250> (referer: None)\n2018-01-24 15:17:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://movie.douban.com/top250>\n{'movie_name': ['肖申克的救赎', '霸王别姬', '这个杀手不太冷', '阿甘正传', '美丽人生', '千与千寻', '泰坦尼克号', '辛德勒的名单', '盗梦空\n间', '机器人总动员', '海上钢琴师', '三傻大闹宝莱坞', '忠犬八公的故事', '放牛班的春天', '大话西游之大圣娶亲', '教父', '龙猫', '楚门的世\n界', '乱世佳人', '熔炉', '触不可及', '天堂电影院', '当幸福来敲门', '无间道', '星际穿越'], 'movie_core': ['9.6', '9.5', '9.4', '9.4', '9\n.5', '9.2', '9.2', '9.4', '9.3', '9.3', '9.2', '9.1', '9.2', '9.2', '9.2', '9.2', '9.1', '9.1', '9.2', '9.2', '9.1', '9.1', '8.9', '9.0\n', '9.1']}\n2018-01-24 15:17:15 [scrapy.core.engine] INFO: Closing spider (finished)\n2018-01-24 15:17:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 651,\n 'downloader/request_count': 3,\n 'downloader/request_method_count/GET': 3,\n 'downloader/response_bytes': 13900,\n 'downloader/response_count': 3,\n 'downloader/response_status_count/200': 2,\n 'downloader/response_status_count/301': 1,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2018, 1, 24, 7, 17, 15, 247183),\n 'item_scraped_count': 1,\n 'log_count/DEBUG': 5,\n 'log_count/INFO': 7,\n 'response_received_count': 2,\n 'scheduler/dequeued': 2,\n 'scheduler/dequeued/memory': 2,\n 'scheduler/enqueued': 2,\n 'scheduler/enqueued/memory': 2,\n 'start_time': datetime.datetime(2018, 1, 24, 7, 17, 14, 784782)}\n2018-01-24 15:17:15 [scrapy.core.engine] INFO: Spider closed (finished)\n</code>\n</details>\n\n#### 1.2 以文件的方式输出\n\n##### 1.2.1 python 原生方式\n\n```python\nwith open(\"movie.txt\", 'wb') as f:\n    for n, c in zip(movie_name, movie_core):\n        str = n+\":\"+c+\"\\n\"\n        f.write(str.encode())\n```\n\n##### 1.2.2 以 scrapy 内置方式\n\nscrapy 内置主要有四种：JSON，JSON lines，CSV，XML\n\n我们将结果用最常用的 JSON 导出，命令如下：\n\n```sh\nscrapy crawl dmoz -o douban.json -t json\n```\n\n-o 后面是导出文件名，-t 后面是导出类型\n\n#### 2 提取内容的封装 Item\n\n> Scrapy 进程可通过使用蜘蛛提取来自网页中的数据。Scrapy 使用 Item 类生成输出对象用于收刮数据\n> Item 对象是自定义的 python 字典，可以使用标准字典语法获取某个属性的值\n\n##### 2.1 定义\n\n```python\nimport scrapy\n\n\nclass InfoItem(scrapy.Item):\n    # define the fields for your item here like:\n    movie_name = scrapy.Field()\n    movie_core = scrapy.Field()\n```\n\n##### 2.2 使用\n\n```python\ndef parse(self, response):\n    movie_name = response.xpath(\"//div[@class='item']//a/span[1]/text()\").extract()\n    movie_core = response.xpath(\"//div[@class='star']/span[2]/text()\").extract()\n\n    for n, c in zip(movie_name, movie_core):\n        movie = InfoItem()\n        movie['movie_name'] = n\n        movie['movie_core'] = c\n        yield movie\n```\n",
      "html": "<h3 id=\"1.-%E6%95%B0%E6%8D%AE%E7%9A%84%E6%8F%90%E5%8F%96\">1. 数据的提取 <a class=\"heading-anchor-permalink\" href=\"#1.-%E6%95%B0%E6%8D%AE%E7%9A%84%E6%8F%90%E5%8F%96\">#</a></h3>\n<h4 id=\"1.1-%E6%8E%A7%E5%88%B6%E5%8F%B0%E6%89%93%E5%8D%B0\">1.1 控制台打印 <a class=\"heading-anchor-permalink\" href=\"#1.1-%E6%8E%A7%E5%88%B6%E5%8F%B0%E6%89%93%E5%8D%B0\">#</a></h4>\n<pre><code class=\"language-python\">import scrapy\n\n\nclass DoubanSpider(scrapy.Spider):\n    name = 'douban'\n    allwed_url = 'douban.com'\n    start_urls = [\n        'https://movie.douban.com/top250/'\n    ]\n\n    def parse(self, response):\n        movie_name = response.xpath(&quot;//div[@class='item']//a/span[1]/text()&quot;).extract()\n        movie_core = response.xpath(&quot;//div[@class='star']/span[2]/text()&quot;).extract()\n        yield {\n            'movie_name':movie_name,\n            'movie_core':movie_core\n        }\n</code></pre>\n<p>执行以上代码，我可以在控制看到：</p>\n<details>\n<summary>输出</summary>\n<code>\n2018-01-24 15:17:14 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: spiderdemo1)\n2018-01-24 15:17:14 [scrapy.utils.log] INFO: Versions: lxml 4.1.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.3.1, w3lib 1.18.0, Twiste\nd 17.9.0, Python 3.6.3 (v3.6.3:2c5fed8, Oct  3 2017, 18:11:49) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0g  2 Nov 201\n7), cryptography 2.1.4, Platform Windows-10-10.0.10240-SP0\n2018-01-24 15:17:14 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'spiderdemo1', 'NEWSPIDER_MODULE': 'spiderdemo1.spiders',\n'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['spiderdemo1.spiders']}\n2018-01-24 15:17:14 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.logstats.LogStats']\n2018-01-24 15:17:14 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2018-01-24 15:17:14 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2018-01-24 15:17:14 [scrapy.middleware] INFO: Enabled item pipelines:\n[]\n2018-01-24 15:17:14 [scrapy.core.engine] INFO: Spider opened\n2018-01-24 15:17:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2018-01-24 15:17:14 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n2018-01-24 15:17:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://movie.douban.com/robots.txt> (referer: None)\n2018-01-24 15:17:15 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://movie.douban.com/top250> from <GET\n https://movie.douban.com/top250/>\n2018-01-24 15:17:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://movie.douban.com/top250> (referer: None)\n2018-01-24 15:17:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://movie.douban.com/top250>\n{'movie_name': ['肖申克的救赎', '霸王别姬', '这个杀手不太冷', '阿甘正传', '美丽人生', '千与千寻', '泰坦尼克号', '辛德勒的名单', '盗梦空\n间', '机器人总动员', '海上钢琴师', '三傻大闹宝莱坞', '忠犬八公的故事', '放牛班的春天', '大话西游之大圣娶亲', '教父', '龙猫', '楚门的世\n界', '乱世佳人', '熔炉', '触不可及', '天堂电影院', '当幸福来敲门', '无间道', '星际穿越'], 'movie_core': ['9.6', '9.5', '9.4', '9.4', '9\n.5', '9.2', '9.2', '9.4', '9.3', '9.3', '9.2', '9.1', '9.2', '9.2', '9.2', '9.2', '9.1', '9.1', '9.2', '9.2', '9.1', '9.1', '8.9', '9.0\n', '9.1']}\n2018-01-24 15:17:15 [scrapy.core.engine] INFO: Closing spider (finished)\n2018-01-24 15:17:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 651,\n 'downloader/request_count': 3,\n 'downloader/request_method_count/GET': 3,\n 'downloader/response_bytes': 13900,\n 'downloader/response_count': 3,\n 'downloader/response_status_count/200': 2,\n 'downloader/response_status_count/301': 1,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2018, 1, 24, 7, 17, 15, 247183),\n 'item_scraped_count': 1,\n 'log_count/DEBUG': 5,\n 'log_count/INFO': 7,\n 'response_received_count': 2,\n 'scheduler/dequeued': 2,\n 'scheduler/dequeued/memory': 2,\n 'scheduler/enqueued': 2,\n 'scheduler/enqueued/memory': 2,\n 'start_time': datetime.datetime(2018, 1, 24, 7, 17, 14, 784782)}\n2018-01-24 15:17:15 [scrapy.core.engine] INFO: Spider closed (finished)\n</code>\n</details>\n<h4 id=\"1.2-%E4%BB%A5%E6%96%87%E4%BB%B6%E7%9A%84%E6%96%B9%E5%BC%8F%E8%BE%93%E5%87%BA\">1.2 以文件的方式输出 <a class=\"heading-anchor-permalink\" href=\"#1.2-%E4%BB%A5%E6%96%87%E4%BB%B6%E7%9A%84%E6%96%B9%E5%BC%8F%E8%BE%93%E5%87%BA\">#</a></h4>\n<h5 id=\"1.2.1-python-%E5%8E%9F%E7%94%9F%E6%96%B9%E5%BC%8F\">1.2.1 python 原生方式 <a class=\"heading-anchor-permalink\" href=\"#1.2.1-python-%E5%8E%9F%E7%94%9F%E6%96%B9%E5%BC%8F\">#</a></h5>\n<pre><code class=\"language-python\">with open(&quot;movie.txt&quot;, 'wb') as f:\n    for n, c in zip(movie_name, movie_core):\n        str = n+&quot;:&quot;+c+&quot;\\n&quot;\n        f.write(str.encode())\n</code></pre>\n<h5 id=\"1.2.2-%E4%BB%A5-scrapy-%E5%86%85%E7%BD%AE%E6%96%B9%E5%BC%8F\">1.2.2 以 scrapy 内置方式 <a class=\"heading-anchor-permalink\" href=\"#1.2.2-%E4%BB%A5-scrapy-%E5%86%85%E7%BD%AE%E6%96%B9%E5%BC%8F\">#</a></h5>\n<p>scrapy 内置主要有四种：JSON，JSON lines，CSV，XML</p>\n<p>我们将结果用最常用的 JSON 导出，命令如下：</p>\n<pre><code class=\"language-sh\">scrapy crawl dmoz -o douban.json -t json\n</code></pre>\n<p>-o 后面是导出文件名，-t 后面是导出类型</p>\n<h4 id=\"2-%E6%8F%90%E5%8F%96%E5%86%85%E5%AE%B9%E7%9A%84%E5%B0%81%E8%A3%85-item\">2 提取内容的封装 Item <a class=\"heading-anchor-permalink\" href=\"#2-%E6%8F%90%E5%8F%96%E5%86%85%E5%AE%B9%E7%9A%84%E5%B0%81%E8%A3%85-item\">#</a></h4>\n<blockquote>\n<p>Scrapy 进程可通过使用蜘蛛提取来自网页中的数据。Scrapy 使用 Item 类生成输出对象用于收刮数据\nItem 对象是自定义的 python 字典，可以使用标准字典语法获取某个属性的值</p>\n</blockquote>\n<h5 id=\"2.1-%E5%AE%9A%E4%B9%89\">2.1 定义 <a class=\"heading-anchor-permalink\" href=\"#2.1-%E5%AE%9A%E4%B9%89\">#</a></h5>\n<pre><code class=\"language-python\">import scrapy\n\n\nclass InfoItem(scrapy.Item):\n    # define the fields for your item here like:\n    movie_name = scrapy.Field()\n    movie_core = scrapy.Field()\n</code></pre>\n<h5 id=\"2.2-%E4%BD%BF%E7%94%A8\">2.2 使用 <a class=\"heading-anchor-permalink\" href=\"#2.2-%E4%BD%BF%E7%94%A8\">#</a></h5>\n<pre><code class=\"language-python\">def parse(self, response):\n    movie_name = response.xpath(&quot;//div[@class='item']//a/span[1]/text()&quot;).extract()\n    movie_core = response.xpath(&quot;//div[@class='star']/span[2]/text()&quot;).extract()\n\n    for n, c in zip(movie_name, movie_core):\n        movie = InfoItem()\n        movie['movie_name'] = n\n        movie['movie_core'] = c\n        yield movie\n</code></pre>\n",
      "id": 16
    },
    {
      "path": "类级别的写法与Scrapy框架/Scrapy 数据的提取.md",
      "url": "类级别的写法与Scrapy框架/Scrapy 数据的提取.html",
      "content": "### 1 Scrapy 提取项目\n\n从网页中提取数据，Scrapy 使用基于 XPath 和 CSS 表达式的技术叫做选择器。以下是 XPath 表达式的一些例子：\n\n- 这将选择 HTML 文档中的 `<head>` 元素中的 `<title>` 元素\n\n  ```text\n  /html/head/title\n  ```\n\n- 这将选择 `<title>` 元素中的文本\n\n  ```text\n  /html/head/title/text()\n  ```\n\n- 这将选择所有的 `<td>` 元素\n\n  ```text\n  //td\n  ```\n\n- 选择 div 包含一个属性 class=”slice” 的所有元素\n\n  ```text\n  //div[@class=”slice”]\n  ```\n\n选择器有四个基本的方法，如下所示：\n\n| S.N.            | 方法 & 描述                                                   |\n| --------------- | ------------------------------------------------------------- |\n| extract()       | 它返回一个 unicode 字符串以及所选数据                         |\n| extract_first() | 它返回第一个 unicode 字符串以及所选数据                       |\n| re()            | 它返回 Unicode 字符串列表，当正则表达式被赋予作为参数时提取   |\n| xpath()         | 它返回选择器列表，它代表由指定 XPath 表达式参数选择的节点     |\n| css()           | 它返回选择器列表，它代表由指定 CSS 表达式作为参数所选择的节点 |\n\n### 2 Scrapy Shell\n\n如果使用选择器想快速的到到效果，我们可以使用 Scrapy Shell\n\n```sh\nscrapy shell \"http://www.163.com\"\n```\n\n注意 windows 系统必须使用双引号\n\n#### 2.1 举例\n\n从一个普通的 HTML 网站提取数据，查看该网站得到的 XPath 的源代码。检测后，可以看到数据将在 UL 标签，并选择 li 标签中的 元素。\n\n代码的下面行显示了不同类型的数据的提取：\n\n- 选择 li 标签内的数据：\n\n```python\nresponse.xpath('//ul/li')\n```\n\n- 对于选择描述：\n\n```python\nresponse.xpath('//ul/li/text()').extract()\n```\n\n- 对于选择网站标题：\n\n```python\nresponse.xpath('//ul/li/a/text()').extract()\n```\n\n- 对于选择网站的链接：\n\n```python\nresponse.xpath('//ul/li/a/@href').extract()\n```\n",
      "html": "<h3 id=\"1-scrapy-%E6%8F%90%E5%8F%96%E9%A1%B9%E7%9B%AE\">1 Scrapy 提取项目 <a class=\"heading-anchor-permalink\" href=\"#1-scrapy-%E6%8F%90%E5%8F%96%E9%A1%B9%E7%9B%AE\">#</a></h3>\n<p>从网页中提取数据，Scrapy 使用基于 XPath 和 CSS 表达式的技术叫做选择器。以下是 XPath 表达式的一些例子：</p>\n<ul>\n<li>\n<p>这将选择 HTML 文档中的 <code>&lt;head&gt;</code> 元素中的 <code>&lt;title&gt;</code> 元素</p>\n<pre><code class=\"language-text\">/html/head/title\n</code></pre>\n</li>\n<li>\n<p>这将选择 <code>&lt;title&gt;</code> 元素中的文本</p>\n<pre><code class=\"language-text\">/html/head/title/text()\n</code></pre>\n</li>\n<li>\n<p>这将选择所有的 <code>&lt;td&gt;</code> 元素</p>\n<pre><code class=\"language-text\">//td\n</code></pre>\n</li>\n<li>\n<p>选择 div 包含一个属性 class=”slice” 的所有元素</p>\n<pre><code class=\"language-text\">//div[@class=”slice”]\n</code></pre>\n</li>\n</ul>\n<p>选择器有四个基本的方法，如下所示：</p>\n<table>\n<thead>\n<tr>\n<th>S.N.</th>\n<th>方法 &amp; 描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>extract()</td>\n<td>它返回一个 unicode 字符串以及所选数据</td>\n</tr>\n<tr>\n<td>extract_first()</td>\n<td>它返回第一个 unicode 字符串以及所选数据</td>\n</tr>\n<tr>\n<td>re()</td>\n<td>它返回 Unicode 字符串列表，当正则表达式被赋予作为参数时提取</td>\n</tr>\n<tr>\n<td>xpath()</td>\n<td>它返回选择器列表，它代表由指定 XPath 表达式参数选择的节点</td>\n</tr>\n<tr>\n<td>css()</td>\n<td>它返回选择器列表，它代表由指定 CSS 表达式作为参数所选择的节点</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"2-scrapy-shell\">2 Scrapy Shell <a class=\"heading-anchor-permalink\" href=\"#2-scrapy-shell\">#</a></h3>\n<p>如果使用选择器想快速的到到效果，我们可以使用 Scrapy Shell</p>\n<pre><code class=\"language-sh\">scrapy shell &quot;http://www.163.com&quot;\n</code></pre>\n<p>注意 windows 系统必须使用双引号</p>\n<h4 id=\"2.1-%E4%B8%BE%E4%BE%8B\">2.1 举例 <a class=\"heading-anchor-permalink\" href=\"#2.1-%E4%B8%BE%E4%BE%8B\">#</a></h4>\n<p>从一个普通的 HTML 网站提取数据，查看该网站得到的 XPath 的源代码。检测后，可以看到数据将在 UL 标签，并选择 li 标签中的 元素。</p>\n<p>代码的下面行显示了不同类型的数据的提取：</p>\n<ul>\n<li>选择 li 标签内的数据：</li>\n</ul>\n<pre><code class=\"language-python\">response.xpath('//ul/li')\n</code></pre>\n<ul>\n<li>对于选择描述：</li>\n</ul>\n<pre><code class=\"language-python\">response.xpath('//ul/li/text()').extract()\n</code></pre>\n<ul>\n<li>对于选择网站标题：</li>\n</ul>\n<pre><code class=\"language-python\">response.xpath('//ul/li/a/text()').extract()\n</code></pre>\n<ul>\n<li>对于选择网站的链接：</li>\n</ul>\n<pre><code class=\"language-python\">response.xpath('//ul/li/a/@href').extract()\n</code></pre>\n",
      "id": 17
    },
    {
      "path": "类级别的写法与Scrapy框架/Scrapy 框架介绍与安装.md",
      "url": "类级别的写法与Scrapy框架/Scrapy 框架介绍与安装.html",
      "content": "![image](https://note.youdao.com/yws/api/personal/file/3B46CE1A83254E4ABC7CCBC6DA7F8838?method=download&shareKey=2bba5f9fd137f02bc237bfca800e603a)\n\n### 1. Scrapy 框架介绍\n\n- Scrapy 是 Python 开发的一个快速,高层次的屏幕抓取和 web 抓取框架，用于抓取 web 站点并从页面中提取结构化的数据。Scrapy = Scrach+Python\n\n- Scrapy 用途广泛，可以用于数据挖掘、监测和自动化测试、信息处理和历史档案等大量应用范围内抽取结构化数据的应用程序框架，广泛用于工业\n\n- Scrapy 使用 `Twisted` 这个异步网络库来处理网络通讯，架构清晰，并且包含了各种中间件接口，可以灵活的完成各种需求。Scrapy 是由 `Twisted` 写的一个受欢迎的 Python 事件驱动网络框架，它使用的是非堵塞的异步处理\n\n#### 1.1 为什么要使用 Scrapy？\n\n- 它更容易构建和大规模的抓取项目\n- 它内置的机制被称为选择器，用于从网站（网页）上提取数据\n- 它异步处理请求，速度十分快\n- 它可以使用自动调节机制自动调整爬行速度\n- 确保开发人员可访问性\n\n#### 1.2 Scrapy 的特点\n\n- Scrapy 是一个开源和免费使用的网络爬虫框架\n- Scrapy 生成格式导出如：JSON，CSV 和 XML\n- Scrapy 内置支持从源代码，使用 XPath 或 CSS 表达式的选择器来提取数据\n- Scrapy 基于爬虫，允许以自动方式从网页中提取数据\n\n#### 1.3 Scrapy 的优点\n\n- Scrapy 很容易扩展，快速和功能强大；\n- 这是一个跨平台应用程序框架（在 Windows，Linux，Mac OS 和 BSD）。\n- Scrapy 请求调度和异步处理；\n- Scrapy 附带了一个名为 Scrapyd 的内置服务，它允许使用 JSON Web 服务上传项目和控制蜘蛛。\n- 也能够刮削任何网站，即使该网站不具有原始数据访问 API；\n\n#### 1.4 整体架构大致如下\n\n![image](https://images2015.cnblogs.com/blog/918906/201608/918906-20160830220006980-1873919293.png)\n\n> 最简单的单个网页爬取流程是 spiders > scheduler > downloader > spiders > item pipeline\n\n#### 1.5 Scrapy 运行流程大概如下\n\n1. 引擎从调度器中取出一个链接(URL)用于接下来的抓取\n2. 引擎把 URL 封装成一个请求(Request)传给下载器\n3. 下载器把资源下载下来，并封装成应答包(Response)\n4. 爬虫解析 Response\n5. 解析出实体（Item）,则交给实体管道进行进一步的处理\n6. 解析出的是链接（URL）,则把 URL 交给调度器等待抓取\n\n#### 1.6 Scrapy 主要包括了以下组件\n\n- 引擎(Scrapy)\n  - 用来处理整个系统的数据流处理, 触发事务(框架核心)\n- 调度器(Scheduler)\n  - 用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个 URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址\n- 下载器(Downloader)\n  - 用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy 下载器是建立在 twisted 这个高效的异步模型上的)\n- 爬虫(Spiders)\n  - 爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让 Scrapy 继续抓取下一个页面\n- 项目管道(Pipeline)\n  - 负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。\n- 下载器中间件(Downloader Middlewares)\n  - 位于 Scrapy 引擎和下载器之间的框架，主要是处理 Scrapy 引擎与下载器之间的请求及响应\n- 爬虫中间件(Spider Middlewares)\n  - 介于 Scrapy 引擎和爬虫之间的框架，主要工作是处理蜘蛛的响应输入和请求输出\n- 调度中间件(Scheduler Middewares)\n  - 介于 Scrapy 引擎和调度之间的中间件，从 Scrapy 引擎发送到调度的请求和响应\n\n### 2 安装\n\n```sh\npip install Scrapy\n```\n\n注：windows 平台需要依赖 pywin32\n\n```text\nModuleNotFoundError: No module named 'win32api'\n```\n\n```sh\npip install pypiwin32\n```\n",
      "html": "<p><img src=\"https://note.youdao.com/yws/api/personal/file/3B46CE1A83254E4ABC7CCBC6DA7F8838?method=download&amp;shareKey=2bba5f9fd137f02bc237bfca800e603a\" alt=\"image\"></p>\n<h3 id=\"1.-scrapy-%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D\">1. Scrapy 框架介绍 <a class=\"heading-anchor-permalink\" href=\"#1.-scrapy-%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D\">#</a></h3>\n<ul>\n<li>\n<p>Scrapy 是 Python 开发的一个快速,高层次的屏幕抓取和 web 抓取框架，用于抓取 web 站点并从页面中提取结构化的数据。Scrapy = Scrach+Python</p>\n</li>\n<li>\n<p>Scrapy 用途广泛，可以用于数据挖掘、监测和自动化测试、信息处理和历史档案等大量应用范围内抽取结构化数据的应用程序框架，广泛用于工业</p>\n</li>\n<li>\n<p>Scrapy 使用 <code>Twisted</code> 这个异步网络库来处理网络通讯，架构清晰，并且包含了各种中间件接口，可以灵活的完成各种需求。Scrapy 是由 <code>Twisted</code> 写的一个受欢迎的 Python 事件驱动网络框架，它使用的是非堵塞的异步处理</p>\n</li>\n</ul>\n<h4 id=\"1.1-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E4%BD%BF%E7%94%A8-scrapy%EF%BC%9F\">1.1 为什么要使用 Scrapy？ <a class=\"heading-anchor-permalink\" href=\"#1.1-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E4%BD%BF%E7%94%A8-scrapy%EF%BC%9F\">#</a></h4>\n<ul>\n<li>它更容易构建和大规模的抓取项目</li>\n<li>它内置的机制被称为选择器，用于从网站（网页）上提取数据</li>\n<li>它异步处理请求，速度十分快</li>\n<li>它可以使用自动调节机制自动调整爬行速度</li>\n<li>确保开发人员可访问性</li>\n</ul>\n<h4 id=\"1.2-scrapy-%E7%9A%84%E7%89%B9%E7%82%B9\">1.2 Scrapy 的特点 <a class=\"heading-anchor-permalink\" href=\"#1.2-scrapy-%E7%9A%84%E7%89%B9%E7%82%B9\">#</a></h4>\n<ul>\n<li>Scrapy 是一个开源和免费使用的网络爬虫框架</li>\n<li>Scrapy 生成格式导出如：JSON，CSV 和 XML</li>\n<li>Scrapy 内置支持从源代码，使用 XPath 或 CSS 表达式的选择器来提取数据</li>\n<li>Scrapy 基于爬虫，允许以自动方式从网页中提取数据</li>\n</ul>\n<h4 id=\"1.3-scrapy-%E7%9A%84%E4%BC%98%E7%82%B9\">1.3 Scrapy 的优点 <a class=\"heading-anchor-permalink\" href=\"#1.3-scrapy-%E7%9A%84%E4%BC%98%E7%82%B9\">#</a></h4>\n<ul>\n<li>Scrapy 很容易扩展，快速和功能强大；</li>\n<li>这是一个跨平台应用程序框架（在 Windows，Linux，Mac OS 和 BSD）。</li>\n<li>Scrapy 请求调度和异步处理；</li>\n<li>Scrapy 附带了一个名为 Scrapyd 的内置服务，它允许使用 JSON Web 服务上传项目和控制蜘蛛。</li>\n<li>也能够刮削任何网站，即使该网站不具有原始数据访问 API；</li>\n</ul>\n<h4 id=\"1.4-%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84%E5%A4%A7%E8%87%B4%E5%A6%82%E4%B8%8B\">1.4 整体架构大致如下 <a class=\"heading-anchor-permalink\" href=\"#1.4-%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84%E5%A4%A7%E8%87%B4%E5%A6%82%E4%B8%8B\">#</a></h4>\n<p><img src=\"https://images2015.cnblogs.com/blog/918906/201608/918906-20160830220006980-1873919293.png\" alt=\"image\"></p>\n<blockquote>\n<p>最简单的单个网页爬取流程是 spiders &gt; scheduler &gt; downloader &gt; spiders &gt; item pipeline</p>\n</blockquote>\n<h4 id=\"1.5-scrapy-%E8%BF%90%E8%A1%8C%E6%B5%81%E7%A8%8B%E5%A4%A7%E6%A6%82%E5%A6%82%E4%B8%8B\">1.5 Scrapy 运行流程大概如下 <a class=\"heading-anchor-permalink\" href=\"#1.5-scrapy-%E8%BF%90%E8%A1%8C%E6%B5%81%E7%A8%8B%E5%A4%A7%E6%A6%82%E5%A6%82%E4%B8%8B\">#</a></h4>\n<ol>\n<li>引擎从调度器中取出一个链接(URL)用于接下来的抓取</li>\n<li>引擎把 URL 封装成一个请求(Request)传给下载器</li>\n<li>下载器把资源下载下来，并封装成应答包(Response)</li>\n<li>爬虫解析 Response</li>\n<li>解析出实体（Item）,则交给实体管道进行进一步的处理</li>\n<li>解析出的是链接（URL）,则把 URL 交给调度器等待抓取</li>\n</ol>\n<h4 id=\"1.6-scrapy-%E4%B8%BB%E8%A6%81%E5%8C%85%E6%8B%AC%E4%BA%86%E4%BB%A5%E4%B8%8B%E7%BB%84%E4%BB%B6\">1.6 Scrapy 主要包括了以下组件 <a class=\"heading-anchor-permalink\" href=\"#1.6-scrapy-%E4%B8%BB%E8%A6%81%E5%8C%85%E6%8B%AC%E4%BA%86%E4%BB%A5%E4%B8%8B%E7%BB%84%E4%BB%B6\">#</a></h4>\n<ul>\n<li>引擎(Scrapy)\n<ul>\n<li>用来处理整个系统的数据流处理, 触发事务(框架核心)</li>\n</ul>\n</li>\n<li>调度器(Scheduler)\n<ul>\n<li>用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个 URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址</li>\n</ul>\n</li>\n<li>下载器(Downloader)\n<ul>\n<li>用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy 下载器是建立在 twisted 这个高效的异步模型上的)</li>\n</ul>\n</li>\n<li>爬虫(Spiders)\n<ul>\n<li>爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让 Scrapy 继续抓取下一个页面</li>\n</ul>\n</li>\n<li>项目管道(Pipeline)\n<ul>\n<li>负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。</li>\n</ul>\n</li>\n<li>下载器中间件(Downloader Middlewares)\n<ul>\n<li>位于 Scrapy 引擎和下载器之间的框架，主要是处理 Scrapy 引擎与下载器之间的请求及响应</li>\n</ul>\n</li>\n<li>爬虫中间件(Spider Middlewares)\n<ul>\n<li>介于 Scrapy 引擎和爬虫之间的框架，主要工作是处理蜘蛛的响应输入和请求输出</li>\n</ul>\n</li>\n<li>调度中间件(Scheduler Middewares)\n<ul>\n<li>介于 Scrapy 引擎和调度之间的中间件，从 Scrapy 引擎发送到调度的请求和响应</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"2-%E5%AE%89%E8%A3%85\">2 安装 <a class=\"heading-anchor-permalink\" href=\"#2-%E5%AE%89%E8%A3%85\">#</a></h3>\n<pre><code class=\"language-sh\">pip install Scrapy\n</code></pre>\n<p>注：windows 平台需要依赖 pywin32</p>\n<pre><code class=\"language-text\">ModuleNotFoundError: No module named 'win32api'\n</code></pre>\n<pre><code class=\"language-sh\">pip install pypiwin32\n</code></pre>\n",
      "id": 18
    },
    {
      "path": "类级别的写法与Scrapy框架/Scrapy 框架使用.md",
      "url": "类级别的写法与Scrapy框架/Scrapy 框架使用.html",
      "content": "### 1 基本使用\n\n#### 1.1 创建项目\n\n运行命令:\n\n```sh\nscrapy startproject myfrist（your_project_name）\n```\n\n![image](https://note.youdao.com/yws/api/personal/file/2A2A0A18562A46C582E9394A5792242A?method=download&shareKey=75190e82788cd47c9c8792593be71114)\n文件说明：\n\n| 名称        | 作用                                                                                                            |\n| ----------- | --------------------------------------------------------------------------------------------------------------- |\n| scrapy.cfg  | 项目的配置信息，主要为 Scrapy 命令行工具提供一个基础的配置信息。（真正爬虫相关的配置信息在 settings.py 文件中） |\n| items.py    | 设置数据存储模板，用于结构化数据，如：Django 的 Model                                                           |\n| pipelines   | 数据处理行为，如：一般结构化的数据持久化                                                                        |\n| settings.py | 配置文件，如：递归的层数、并发数，延迟下载等                                                                    |\n| spiders     | 爬虫目录，如：创建文件，编写爬虫规则                                                                            |\n\n注意：一般创建爬虫文件时，以网站域名命名\n\n#### 2 编写 spdier\n\n在 spiders 目录中新建 daidu_spider.py 文件\n\n##### 2.1 注意\n\n1. 爬虫文件需要定义一个类，并继承 scrapy.spiders.Spider\n2. 必须定义 name，即爬虫名，如果没有 name，会报错。因为源码中是这样定义的\n\n##### 2.2 编写内容\n\n> 在这里可以告诉 scrapy 。要如何查找确切数据，这里必须要定义一些属性\n\n- name: 它定义了蜘蛛的唯一名称\n- allowed_domains: 它包含了蜘蛛抓取的基本 URL；\n- start-urls: 蜘蛛开始爬行的 URL 列表；\n- parse(): 这是提取并解析刮下数据的方法；\n\n下面的代码演示了蜘蛛代码的样子：\n\n```python\nimport scrapy\n\n\nclass DoubanSpider(scrapy.Spider):\n    name = 'douban'\n    allwed_url = 'douban.com'\n    start_urls = [\n        'https://movie.douban.com/top250/'\n    ]\n\n    def parse(self, response):\n        movie_name = response.xpath(\"//div[@class='item']//a/span[1]/text()\").extract()\n        movie_core = response.xpath(\"//div[@class='star']/span[2]/text()\").extract()\n        yield {\n            'movie_name':movie_name,\n            'movie_core':movie_core\n        }\n```\n\n### 其他命令\n\n- 创建爬虫\n\n  ```sh\n  scrapy genspider 爬虫名 爬虫的地址\n  ```\n\n- 运行爬虫\n\n  ```sh\n  scrapy crawl 爬虫名\n  ```\n",
      "html": "<h3 id=\"1-%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8\">1 基本使用 <a class=\"heading-anchor-permalink\" href=\"#1-%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8\">#</a></h3>\n<h4 id=\"1.1-%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE\">1.1 创建项目 <a class=\"heading-anchor-permalink\" href=\"#1.1-%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE\">#</a></h4>\n<p>运行命令:</p>\n<pre><code class=\"language-sh\">scrapy startproject myfrist（your_project_name）\n</code></pre>\n<p><img src=\"https://note.youdao.com/yws/api/personal/file/2A2A0A18562A46C582E9394A5792242A?method=download&amp;shareKey=75190e82788cd47c9c8792593be71114\" alt=\"image\">\n文件说明：</p>\n<table>\n<thead>\n<tr>\n<th>名称</th>\n<th>作用</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>scrapy.cfg</td>\n<td>项目的配置信息，主要为 Scrapy 命令行工具提供一个基础的配置信息。（真正爬虫相关的配置信息在 <a href=\"http://settings.py\">settings.py</a> 文件中）</td>\n</tr>\n<tr>\n<td><a href=\"http://items.py\">items.py</a></td>\n<td>设置数据存储模板，用于结构化数据，如：Django 的 Model</td>\n</tr>\n<tr>\n<td>pipelines</td>\n<td>数据处理行为，如：一般结构化的数据持久化</td>\n</tr>\n<tr>\n<td><a href=\"http://settings.py\">settings.py</a></td>\n<td>配置文件，如：递归的层数、并发数，延迟下载等</td>\n</tr>\n<tr>\n<td>spiders</td>\n<td>爬虫目录，如：创建文件，编写爬虫规则</td>\n</tr>\n</tbody>\n</table>\n<p>注意：一般创建爬虫文件时，以网站域名命名</p>\n<h4 id=\"2-%E7%BC%96%E5%86%99-spdier\">2 编写 spdier <a class=\"heading-anchor-permalink\" href=\"#2-%E7%BC%96%E5%86%99-spdier\">#</a></h4>\n<p>在 spiders 目录中新建 daidu_spider.py 文件</p>\n<h5 id=\"2.1-%E6%B3%A8%E6%84%8F\">2.1 注意 <a class=\"heading-anchor-permalink\" href=\"#2.1-%E6%B3%A8%E6%84%8F\">#</a></h5>\n<ol>\n<li>爬虫文件需要定义一个类，并继承 scrapy.spiders.Spider</li>\n<li>必须定义 name，即爬虫名，如果没有 name，会报错。因为源码中是这样定义的</li>\n</ol>\n<h5 id=\"2.2-%E7%BC%96%E5%86%99%E5%86%85%E5%AE%B9\">2.2 编写内容 <a class=\"heading-anchor-permalink\" href=\"#2.2-%E7%BC%96%E5%86%99%E5%86%85%E5%AE%B9\">#</a></h5>\n<blockquote>\n<p>在这里可以告诉 scrapy 。要如何查找确切数据，这里必须要定义一些属性</p>\n</blockquote>\n<ul>\n<li>name: 它定义了蜘蛛的唯一名称</li>\n<li>allowed_domains: 它包含了蜘蛛抓取的基本 URL；</li>\n<li>start-urls: 蜘蛛开始爬行的 URL 列表；</li>\n<li>parse(): 这是提取并解析刮下数据的方法；</li>\n</ul>\n<p>下面的代码演示了蜘蛛代码的样子：</p>\n<pre><code class=\"language-python\">import scrapy\n\n\nclass DoubanSpider(scrapy.Spider):\n    name = 'douban'\n    allwed_url = 'douban.com'\n    start_urls = [\n        'https://movie.douban.com/top250/'\n    ]\n\n    def parse(self, response):\n        movie_name = response.xpath(&quot;//div[@class='item']//a/span[1]/text()&quot;).extract()\n        movie_core = response.xpath(&quot;//div[@class='star']/span[2]/text()&quot;).extract()\n        yield {\n            'movie_name':movie_name,\n            'movie_core':movie_core\n        }\n</code></pre>\n<h3 id=\"%E5%85%B6%E4%BB%96%E5%91%BD%E4%BB%A4\">其他命令 <a class=\"heading-anchor-permalink\" href=\"#%E5%85%B6%E4%BB%96%E5%91%BD%E4%BB%A4\">#</a></h3>\n<ul>\n<li>\n<p>创建爬虫</p>\n<pre><code class=\"language-sh\">scrapy genspider 爬虫名 爬虫的地址\n</code></pre>\n</li>\n<li>\n<p>运行爬虫</p>\n<pre><code class=\"language-sh\">scrapy crawl 爬虫名\n</code></pre>\n</li>\n</ul>\n",
      "id": 19
    }
  ]
}